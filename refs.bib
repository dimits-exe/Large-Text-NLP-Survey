@article{sebastiani,
  author    = {Fabrizio Sebastiani},
  title     = {Machine Learning in Automated Text Categorization},
  journal   = {CoRR},
  volume    = {cs.IR/0110053},
  year      = {2001},
  url       = {https://arxiv.org/abs/cs/0110053},
  timestamp = {Fri, 10 Jan 2020 12:59:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/cs-IR-0110053.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{russel,
  title     = "Artificial Intelligence A Modern Approach",
  author    = "Stuart Russell and Peter Norvig",
  year      = 2016,
  publisher = "Pearson Education",
  address   = "Upper Saddle River, New Jersey 07458"
}

@article{cho,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
               Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1406.1078},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.1078},
  eprinttype = {arXiv},
  eprint    = {1406.1078},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChoMGBSB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{quinlan,
  title     = "S.L. C4.5: Programs for Machine Learning",
  author    = "J. Ross Quinlan",
  year      = 1994,
  OPTdoi = {10.1007/BF00993309},
  publisher = "Morgan Kaufmann Publishers",
  address   = "Upper Saddle River, New Jersey 07458"
}

@article{korde,
	author = {Vandana Korde and C Namrata Mahender},
	title = {TEXT CLASSIFICATION AND CLASSIFIERS: A SURVEY},
	journaltitle = {International Journal of Artificial Intelligence \& Applications (IJAIA)},
	year = {2012-03-01},
	OPTvolume = {3},
	OPTnumber = {2}
}

@inproceedings{worsham,
    title = "Genre Identification and the Compositional Effect of Genre in Literature",
    author = "Worsham, Joseph  and
      Kalita, Jugal",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1167",
    pages = "1963--1973",
    abstract = "Recent advances in Natural Language Processing are finding ways to place an emphasis on the hierarchical nature of text instead of representing language as a flat sequence or unordered collection of words or letters. A human reader must capture multiple levels of abstraction and meaning in order to formulate an understanding of a document. In this paper, we address the problem of developing approaches which are capable of working with extremely large and complex literary documents to perform Genre Identification. The task is to assign the literary classification to a full-length book belonging to a corpus of literature, where the works on average are well over 200,000 words long and genre is an abstract thematic concept. We introduce the Gutenberg Dataset for Genre Identification. Additionally, we present a study on how current deep learning models compare to traditional methods for this task. The results are presented as a baseline along with findings on how using an ensemble of chapters can significantly improve results in deep learning methods. The motivation behind the ensemble of chapters method is discussed as the compositionality of subtexts which make up a larger work and contribute to the overall genre.",
}

@book{worsham_book,
	author = {Joseph Worsham},
	title = {Towards Literary Genre Identification: Applied Neural Networks for Large Text Classification},
	year = {2014},
	OPTpublisher = {proquest}
}

@proceedings{brazil,
	editor = {Monte Serrat and Mateus Tarcinalli Machado and Evandro E S Ruiz},
	title = {A machine learning approach to literary genre classification on Portuguese texts: circumventing NLP’s standard varieties},
	year = {2021},
	OPTtranslator = {translator},
	OPTannotator = {annotator},
	OPTcommentator = {commentator},
	OPTsubtitle = {subtitle},
	OPTtitleaddon = {titleaddon},
	OPTlanguage = {english},
	OPToriglanguage = {portuguese},
	OPTdoi = {10.5753/stil.2021.17805},
	OPTeventtitle = {Simpósio Brasileiro de Tecnologia da Informação e da Linguagem Humana},
}

@misc{geroge,
	author = {Theodoridou Ifigeneia},
	title = {Development and application of language models in Greek literature},
	institution = {Aristotle University of Thessaloniki},
	year = {2020},
    doi = {10.26262/heal.auth.ir.323350},
	OPTlanguage = {greek},
}

@article{xu,
	title={An Improved Random Forest Classifier for Text Categorization},
	author={Baoxun Xu and Xiufeng Guo and Yunming Ye and Jiefeng Cheng},
	journal={J. Comput.},
	year={2012},
	volume={7},
	pages={2913-2920}
}

@misc{sicong,
	author = {Sicong Liu and Zihan Huang and Yikang Li and Zhanlin Sun and Jiahao Wu and Hongyi Zhang},
	title = {DeepGenre: Deep Neural Networks for Genre Classification in Literary Works},
	institution = {Language Technologies Institute, Carnegie Mellon University},
    year={2020}
}


@article{suleiman,
	author = {Suleiman, Dima and Awajan, Arafat},
	year = {2020},
	month = {08},
	pages = {},
	title = {Deep Learning Based Abstractive Text Summarization: Approaches, Datasets, Evaluation Measures, and Challenges},
	volume = {2020},
	journal = {Mathematical Problems in Engineering},
	doi = {10.1155/2020/9365340}
}

@article{abigail,
	author    = {Abigail See and
	Peter J. Liu and
	Christopher D. Manning},
	title     = {Get To The Point: Summarization with Pointer-Generator Networks},
	journal   = {CoRR},
	volume    = {abs/1704.04368},
	year      = {2017},
	url       = {http://arxiv.org/abs/1704.04368},
	eprinttype = {arXiv},
	eprint    = {1704.04368},
	timestamp = {Mon, 13 Aug 2018 16:46:08 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/SeeLM17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xiao,
	title = "Extractive Summarization of Long Documents by Combining Global and Local Context",
	author = "Xiao, Wen  and Carenini, Giuseppe",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-1298",
	doi = "10.18653/v1/D19-1298",
	pages = "3011--3021",
}

@article{luhn,
	author={Luhn, H. P.},
	journal={IBM Journal of Research and Development}, 
	title={The Automatic Creation of Literature Abstracts}, 
	year={1958},
	volume={2},
	number={2},
	pages={159-165},
	doi={10.1147/rd.22.0159}
}

@article{maybury,
	title = {Generating summaries from event data},
	journal = {Information Processing \& Management},
	volume = {31},
	number = {5},
	pages = {735-751},
	year = {1995},
	note = {Summarizing Text},
	issn = {0306-4573},
	doi = {https://doi.org/10.1016/0306-4573(95)00025-C},
	url = {https://www.sciencedirect.com/science/article/pii/030645739500025C},
	author = {Mark T. Maybury},
	keywords = {Automated summarization, Natural language generation, Importance, Condensation, Aggregation, Tailored summarization, Automated abstracting}
}

@inproceedings{lapata,
	title = "Neural Summarization by Extracting Sentences and Words",
	author = "Cheng, Jianpeng  and
	Lapata, Mirella",
	booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2016",
	address = "Berlin, Germany",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P16-1046",
	doi = "10.18653/v1/P16-1046",
	pages = "484--494",
}

@article{nallapati,
	author    = {Ramesh Nallapati and Feifei Zhai and Bowen Zhou},
	title     = {SummaRuNNer: {A} Recurrent Neural Network based Sequence Model for
	Extractive Summarization of Documents},
	journal   = {CoRR},
	volume    = {abs/1611.04230},
	year      = {2016},
	url       = {http://arxiv.org/abs/1611.04230},
	eprinttype = {arXiv},
	eprint    = {1611.04230},
	timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/NallapatiZZ16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{pakistan,
	doi = {10.48550/ARXIV.2207.09163},
	url = {https://arxiv.org/abs/2207.09163},
	author = {Ahmad, Waqar and Edalati, Maryam},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Urdu Speech and Text Based Sentiment Analyzer},
	publisher = {arXiv},
	year = {2022},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{kaur,
	title={Opinion mining and sentiment analysis},
	author={Rushlene Kaur Bakshi and Navneet Kaur and Ravneet Kaur and Gurpreet Kaur},
	journal={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)},
	year={2016},
	pages={452-455}
}

@misc{sergio,
	doi = {10.48550/ARXIV.2105.14373},
	url = {https://arxiv.org/abs/2105.14373},
	author = {Barreto, Sérgio and Moura, Ricardo and Carvalho, Jonnathan and Paes, Aline and Plastino, Alexandre},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Sentiment analysis in tweets: an assessment study from classical to modern text representation models},
	publisher = {arXiv},
	year = {2021},
	copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@article{shelly,
author = {Mehndiratta, Pulkit and Sachdeva, Shelly and Soni, Devpriya},
year = {2017},
month = {09},
pages = {},
title = {Detection of Sarcasm in Text Data using Deep Convolutional Neural Networks},
volume = {18},
journal = {Scalable Computing: Practice and Experience},
doi = {10.12694/scpe.v18i3.1302}
}

@inproceedings{tepperman,
	author = {Tepperman, Joseph and Traum, David and Narayanan, Shrikanth},
	year = {2006},
	month = {01},
	pages = {},
	title = {Yeah right: Sarcasm recognition for spoken dialogue systems}
}

@article{stacey,
	author = {Stacey L. Ivanko and Penny M. Pexman},
	title = {Context Incongruity and Irony Processing},
	journal = {Discourse Processes},
	volume = {35},
	number = {3},
	pages = {241-279},
	year  = {2003},
	publisher = {Routledge},
	doi = {10.1207/S15326950DP3503\_2},
	URL = {https://doi.org/10.1207/S15326950DP3503\_2},
	eprint = {https://doi.org/10.1207/S15326950DP3503\_2}
}

@inproceedings{dmitry,
	title = "Semi-Supervised Recognition of Sarcasm in {T}witter and {A}mazon",
	author = "Davidov, Dmitry  and
	Tsur, Oren  and
	Rappoport, Ari",
	booktitle = "Proceedings of the Fourteenth Conference on Computational Natural Language Learning",
	month = jul,
	year = "2010",
	address = "Uppsala, Sweden",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W10-2914",
	pages = "107--116",
}

@article{cortes,
  added-at = {2011-06-29T16:50:31.000+0200},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  biburl = {https://www.bibsonomy.org/bibtex/287a1b1cb4913bafb37c82d66b0bc088a/oliver_awm},
  ee = {http://dx.doi.org/10.1007/BF00994018},
  interhash = {c223c465141618ad63aac5a6132280f7},
  intrahash = {87a1b1cb4913bafb37c82d66b0bc088a},
  journal = {Machine Learning},
  keywords = {awm2011 svm},
  number = 3,
  pages = {273-297},
  timestamp = {2011-06-29T16:50:31.000+0200},
  title = {Support-Vector Networks.},
  url = {http://dblp.uni-trier.de/db/journals/ml/ml20.html#CortesV95},
  volume = 20,
  year = 1995
}

@incollection{rumelhart,
    added-at = {2008-02-26T11:58:58.000+0100},
    address = {Cambridge, MA},
    author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
    biburl = {https://www.bibsonomy.org/bibtex/27c3d39c519530239660d33e66493ade1/schaul},
    booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, {V}olume 1: {F}oundations},
    citeulike-article-id = {2378884},
    description = {idsia},
    editor = {Rumelhart, David E. and Mcclelland, James L.},
    interhash = {dd8485b30b80c7f35263bcb21ed81c1f},
    intrahash = {7c3d39c519530239660d33e66493ade1},
    keywords = {nn},
    pages = {318--362},
    priority = {2},
    publisher = {MIT Press},
    timestamp = {2008-02-26T12:02:43.000+0100},
    title = {Learning Internal Representations by Error Propagation},
    year = 1986
}


@article{jordan,
    title = {Serial order: a parallel distributed processing approach. Technical report, June 1985-March 1986},
    author = {Jordan, M I},
    abstractNote = {A theory of serial order is proposed that attempts to deal both with the classical problem of the temporal organization of internally generated action sequences as well as with certain of the parallel aspects of sequential behavior. The theory describes a dynamical system that is embodied as a parallel distributed processing or connectionist network. The trajectories of this dynamical system come to follow desired paths corresponding to particular action sequences as a result of a learning process during which constraints are imposed on the system. These constraints enforce sequentiality where necessary and, as they are relaxed, performance becomes more parallel. The theory is applied to the problem of coarticulation in speech production and simulation experiments are presented.},
    url = {https://www.osti.gov/biblio/6910294}, journal = {U.S Department of Energy},
    place = {United States},
    year = {1986},
    month = {5}
} 

@article{siegelman,
    title = {Turing computability with neural nets},
    journal = {Applied Mathematics Letters},
    volume = {4},
    number = {6},
    pages = {77-80},
    year = {1991},
    issn = {0893-9659},
    doi = {https://doi.org/10.1016/0893-9659(91)90080-F},
    url = {https://www.sciencedirect.com/science/article/pii/089396599190080F},
    author = {Hava T. Siegelmann and Eduardo D. Sontag},
    abstract = {This paper shows the existence of a finite neural network, made up of sigmoidal neurons, which simulates a universal Turing machine. It is composed of less than 105 synchronously evolving processors, interconnected linearly. High-order connections are not required.}
}

@misc{neuralonline,
    added-at = {2019-01-15T22:46:49.000+0100},
    author = {Nielsen, Michael A.},
    biburl = {https://www.bibsonomy.org/bibtex/274383acee84241145ff4ffede9658206/slicside},
    interhash = {04d527cadd39f888fc3babcad3343362},
    intrahash = {74383acee84241145ff4ffede9658206},
    keywords = {ba-2018-hahnrico},
    publisher = {Determination Press},
    timestamp = {2019-01-15T22:46:49.000+0100},
    title = {Neural Networks and Deep Learning},
    type = {misc},
    url = {http://neuralnetworksanddeeplearning.com/},
    year = 2018
}

@article{sepp,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    year = {1997},
    month = {12},
    pages = {1735-80},
    title = {Long Short-term Memory},
    volume = {9},
    journal = {Neural computation},
    doi = {10.1162/neco.1997.9.8.1735}
}

@inproceedings{feng,
    author = {Feng, Weijiang and Guan, Naiyang and Li, Yuan and Zhang, Xiang and Luo, Zhigang},
    year = {2017},
    month = {05},
    pages = {681-688},
    title = {Audio visual speech recognition with multimodal recurrent neural networks},
    doi = {10.1109/IJCNN.2017.7965918}
}

@book{calin,
	author = {Ovidiu Calin},
	title = {Deep Learning Architectures},
    subtitle = {A Mathematical Approach},
	year = {2014},
    doi = {https://doi.org/10.1007/978-3-030-36721-3},
	OPTpublisher = {Springer Cham}
}

@inproceedings{ilia,
     author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
     publisher = {Curran Associates, Inc.},
     title = {Attention is All you Need},
     url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
     pages={15},
     volume = {30},
     year = {2017}
}

@inproceedings{rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@inproceedings{toutanova,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{pang2,
    title = "Thumbs up? Sentiment Classification using Machine Learning Techniques",
    author = "Pang, Bo  and
      Lee, Lillian  and
      Vaithyanathan, Shivakumar",
    booktitle = "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002)",
    month = jul,
    year = "2002",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W02-1011",
    doi = "10.3115/1118693.1118704",
    pages = "79--86",
}

@inproceedings{elson,
  title={Automatic Attribution of Quoted Speech in Literary Narrative},
  author={David K. Elson and
          Kathleen R. McKeown},
    booktitle= {Automatic Attribution of Quoted Speech in Literary Narrative},
  year  = {2010},
  pages={0-7},
  url = {http://www.cs.columbia.edu/~delson/pubs/AAAI10-ElsonMcKeown.pdf}
}

@article{taylor,
  title={“Cloze Procedure”: A New Tool for Measuring Readability},
  author={Wilson L. Taylor},
  journal={Journalism \& Mass Communication Quarterly},
  year={1953},
  volume={30},
  pages={415 - 433}
}

@article{karta,
  author    = {Kevin Clark and
               Minh{-}Thang Luong and
               Quoc V. Le and
               Christopher D. Manning},
  title     = {{ELECTRA:} Pre-training Text Encoders as Discriminators Rather Than
               Generators},
  journal   = {CoRR},
  volume    = {abs/2003.10555},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.10555},
  eprinttype = {arXiv},
  eprint    = {2003.10555},
  timestamp = {Wed, 01 Apr 2020 17:39:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-10555.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{glue,
  doi = {10.48550/ARXIV.1804.07461},
  
  url = {https://arxiv.org/abs/1804.07461},
  
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{tang,
    title = "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification",
    author = "Tang, Duyu  and
      Qin, Bing  and
      Liu, Ting",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1167",
    doi = "10.18653/v1/D15-1167",
    pages = "1422--1432",
}

@misc{dauphin,
  doi = {10.48550/ARXIV.1612.08083},
  
  url = {https://arxiv.org/abs/1612.08083},
  
  author = {Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Modeling with Gated Convolutional Networks},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{vu,
  doi = {10.48550/ARXIV.1605.07333},
  
  url = {https://arxiv.org/abs/1605.07333},
  
  author = {Vu, Ngoc Thang and Adel, Heike and Gupta, Pankaj and Schütze, Hinrich},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Combining Recurrent and Convolutional Neural Networks for Relation Classification},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{scuttle,
  doi = {10.48550/ARXIV.1409.0575},
  
  url = {https://arxiv.org/abs/1409.0575},
  
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences, I.4.8; I.5.2},
  
  title = {ImageNet Large Scale Visual Recognition Challenge},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{yang,
    AUTHOR = {Yang, Yu-Xin and Wen, Chang and Xie, Kai and Wen, Fang-Qing and Sheng, Guan-Qun and Tang, Xin-Gong},
    TITLE = {Face Recognition Using the SR-CNN Model},
    JOURNAL = {Sensors},
    VOLUME = {18},
    YEAR = {2018},
    NUMBER = {12},
    ARTICLE-NUMBER = {4237},
    URL = {https://www.mdpi.com/1424-8220/18/12/4237},
    ISSN = {1424-8220},
    ABSTRACT = {In order to solve the problem of face recognition in complex environments being vulnerable to illumination change, object rotation, occlusion, and so on, which leads to the imprecision of target position, a face recognition algorithm with multi-feature fusion is proposed. This study presents a new robust face-matching method named SR-CNN, combining the rotation-invariant texture feature (RITF) vector, the scale-invariant feature transform (SIFT) vector, and the convolution neural network (CNN). Furthermore, a graphics processing unit (GPU) is used to parallelize the model for an optimal computational performance. The Labeled Faces in the Wild (LFW) database and self-collection face database were selected for experiments. It turns out that the true positive rate is improved by 10.97\&ndash;13.24\% and the acceleration ratio (the ratio between central processing unit (CPU) operation time and GPU time) is 5\&ndash;6 times for the LFW face database. For the self-collection, the true positive rate increased by 12.65\&ndash;15.31\%, and the acceleration ratio improved by a factor of 6\&ndash;7.},
    DOI = {10.3390/s18124237}
}

@misc{patrick,
  doi = {10.48550/ARXIV.1701.03056},
  
  url = {https://arxiv.org/abs/1701.03056},
  
  author = {Kayalibay, Baris and Jensen, Grady and van der Smagt, Patrick},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {CNN-based Segmentation of Medical Imaging Data},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{diao,
    author = {Diao, Qiming and Qiu, Minghui and Wu, Chao-Yuan and Smola, Alexander and Wang, Chong},
    year = {2014},
    month = {08},
    pages = {},
    title = {Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)},
    isbn = {978-1-4503-2956-9},
    journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    doi = {10.1145/2623330.2623758}
}

@article{dragomir,
    title = {Centroid-based summarization of multiple documents},
    journal = {Information Processing \& Management},
    volume = {40},
    number = {6},
    pages = {919-938},
    year = {2004},
    issn = {0306-4573},
    doi = {https://doi.org/10.1016/j.ipm.2003.10.006},
    url = {https://www.sciencedirect.com/science/article/pii/S0306457303000955},
    author = {Dragomir R. Radev and Hongyan Jing and Małgorzata Styś and Daniel Tam},
    keywords = {Multi-document summarization, Centroid-based summarization, Cluster-based relative utility, Cross-sentence informational subsumption},
    abstract = {We present a multi-document summarizer, MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We describe two new techniques, a centroid-based summarizer, and an evaluation scheme based on sentence utility and subsumption. We have applied this evaluation to both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.}
}

@article{ouyang,
title = {Applying regression models to query-focused multi-document summarization},
journal = {Information Processing \& Management},
volume = {47},
number = {2},
pages = {227-237},
year = {2011},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2010.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306457310000257},
author = {You Ouyang and Wenjie Li and Sujian Li and Qin Lu},
keywords = {Query-focused summarization, Support Vector Regression, Training data construction},
abstract = {Most existing research on applying machine learning techniques to document summarization explores either classification models or learning-to-rank models. This paper presents our recent study on how to apply a different kind of learning models, namely regression models, to query-focused multi-document summarization. We choose to use Support Vector Regression (SVR) to estimate the importance of a sentence in a document set to be summarized through a set of pre-defined features. In order to learn the regression models, we propose several methods to construct the “pseudo” training data by assigning each sentence with a “nearly true” importance score calculated with the human summaries that have been provided for the corresponding document set. A series of evaluations on the DUC data sets are conducted to examine the efficiency and the robustness of the proposed approaches. When compared with classification models and ranking models, regression models are consistently preferable.}
}

@misc{johnson,
    doi = {10.48550/ARXIV.1504.01255},
    
    url = {https://arxiv.org/abs/1504.01255},
    
    author = {Johnson, Rie and Zhang, Tong},
    
    keywords = {Machine Learning (stat.ML), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    
    title = {Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding},
    
    publisher = {arXiv},
    
    year = {2015},
    
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@inbook{ontonotes,
    author = {Weischedel, Ralph and Hovy, Eduard and Marcus, Mitchell and Palmer, Martha and Belvin, Robert and Pradhan, Sameer and Ramshaw, Lance and Xue, Nianwen},
    year = {2011},
    month = {01},
    pages = {0-12},
    title = {OntoNotes: A Large Training Corpus for Enhanced Processing}
}

@inproceedings{tiedemann,
    title = "Parallel Data, Tools and Interfaces in {OPUS}",
    author = {Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf",
    pages = "2214--2218",
    abstract = "This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.",
}

@inproceedings{bamman,
    title = "An Annotated Dataset of Coreference in {E}nglish Literature",
    author = "Bamman, David  and
      Lewke, Olivia  and
      Mansoor, Anya",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.6",
    pages = "44--54",
    abstract = "We present in this work a new dataset of coreference annotations for works of literature in English, covering 29,103 mentions in 210,532 tokens from 100 works of fiction published between 1719 and 1922. This dataset differs from previous coreference corpora in containing documents whose average length (2,105.3 words) is four times longer than other benchmark datasets (463.7 for OntoNotes), and contains examples of difficult coreference problems common in literature. This dataset allows for an evaluation of cross-domain performance for the task of coreference resolution, and analysis into the characteristics of long-distance within-document coreference.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@misc{li,
  doi = {10.48550/ARXIV.2209.05034},
  
  url = {https://arxiv.org/abs/2209.05034},
  
  author = {Li, Yudong and Zhang, Yuqing and Zhao, Zhe and Shen, Linlin and Liu, Weijie and Mao, Weiquan and Zhang, Hui},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {CSL: A Large-scale Chinese Scientific Literature Dataset},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{mikolov,
  doi = {10.48550/ARXIV.1301.3781},
  
  url = {https://arxiv.org/abs/1301.3781},
  
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Efficient Estimation of Word Representations in Vector Space},
  
  publisher = {arXiv},
  
  year = {2013},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{jurafsky,
	author = {Daniel Jurafsky and James H. Martin},
	title = {Speech and Language Processing},
    subtitle = {An Introduction to Natural Language Processing,
Computational Linguistics, and Speech Recognition},
	year = {2020}
}

@misc{tomas,
  doi = {10.48550/ARXIV.1310.4546},
  
  url = {https://arxiv.org/abs/1310.4546},
  
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Distributed Representations of Words and Phrases and their Compositionality},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{forman,
author = {Forman, George},
title = {A Pitfall and Solution in Multi-Class Feature Selection for Text Classification},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015356},
doi = {10.1145/1015330.1015356},
abstract = {Information Gain is a well-known and empirically proven method for high-dimensional feature selection. We found that it and other existing methods failed to produce good results on an industrial text classification problem. On investigating the root cause, we find that a large class of feature scoring methods suffers a pitfall: they can be blinded by a surplus of strongly predictive features for some classes, while largely ignoring features needed to discriminate difficult classes. In this paper we demonstrate this pitfall hurts performance even for a relatively uniform text classification task. Based on this understanding, we present solutions inspired by round-robin scheduling that avoid this pitfall, without resorting to costly wrapper methods. Empirical evaluation on 19 datasets shows substantial improvements.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {38},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{yiming,
  title={A Comparative Study on Feature Selection in Text Categorization},
  author={Yiming Yang and Jan O. Pedersen},
  booktitle={International Conference on Machine Learning},
  year={1997}
}

@article{bergsta,
    author = {James Bergstra and Yoshua Bengio},
    year = {2012},
    month = {12},
    title = {Random Search for Hyper-Parameter Optimization},
    journal = {Journal of Machine Learning Research},
}

@inproceedings{meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}

@article{edmunson,
    author = {Edmundson, H. P.},
    title = {New Methods in Automatic Extracting},
    year = {1969},
    issue_date = {April 1969},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {16},
    number = {2},
    issn = {0004-5411},
    url = {https://doi.org/10.1145/321510.321519},
    doi = {10.1145/321510.321519},
    abstract = {This paper describes new methods of automatically extracting documents for screening purposes, i.e. the computer selection of sentences having the greatest potential for conveying to the reader the substance of the document. While previous work has focused on one component of sentence significance, namely, the presence of high-frequency content words (key words), the methods described here also treat three additional components: pragmatic words (cue words); title and heading words; and structural indicators (sentence location).The research has resulted in an operating system and a research methodology. The extracting system is parameterized to control and vary the influence of the above four components. The research methodology includes procedures for the compilation of the required dictionaries, the setting of the control parameters, and the comparative evaluation of the automatic extracts with manually produced extracts. The results indicate that the three newly proposed components dominate the frequency component in the production of better extracts.},
    journal = {J. ACM},
    month = {apr},
    pages = {264–285},
    numpages = {22}
}

@inproceedings{mihalcea,
    title = "A Language Independent Algorithm for Single and Multiple Document Summarization",
    author = "Mihalcea, Rada  and
      Tarau, Paul",
    booktitle = "Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts",
    year = "2005",
    url = "https://aclanthology.org/I05-2004",
}

@inproceedings{wan,
    title = "Improved Affinity Graph Based Multi-Document Summarization",
    author = "Wan, Xiaojun  and
      Yang, Jianwu",
    booktitle = "Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",
    month = jun,
    year = "2006",
    address = "New York City, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N06-2046",
    pages = "181--184",
}

@inproceedings{chen,
    author = {Chen, Ping and Verma, Rakesh},
    title = {A Query-Based Medical Information Summarization System Using Ontology Knowledge},
    year = {2006},
    isbn = {0769525171},
    publisher = {IEEE Computer Society},
    address = {USA},
    url = {https://doi.org/10.1109/CBMS.2006.25},
    doi = {10.1109/CBMS.2006.25},
    abstract = {As huge amounts of knowledge are created rapidly, effective information access becomes an important issue. Especially for critical domains, such as medical and financial areas, efficient retrieval of concise and relevant information is highly desired. In this paper we propose a new user query based text summarization technique that makes use of Unified Medical Language System, an ontology knowledge source from National Library of Medicine. We compare our method with keyword-only approach, and our ontologybased method performs clearly better. Our method also shows potential to be used in other information retrieval areas.},
    booktitle = {Proceedings of the 19th IEEE Symposium on Computer-Based Medical Systems},
    pages = {37–42},
    numpages = {6},
    keywords = {Text Summarization, UMLS, Biomedical Ontology, Information Retrieval},
    series = {CBMS '06}
}

@inproceedings{ren,
author = {Ren, Zhaochun and Liang, Shangsong and Meij, Edgar and de Rijke, Maarten},
title = {Personalized Time-Aware Tweets Summarization},
year = {2013},
isbn = {9781450320344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484028.2484052},
doi = {10.1145/2484028.2484052},
abstract = {We focus on the problem of selecting meaningful tweets given a user's interests; the dynamic nature of user interests, the sheer volume, and the sparseness of individual messages make this an challenging problem. Specifically, we consider the task of time-aware tweets summarization, based on a user's history and collaborative social influences from ``social circles.'' We propose a time-aware user behavior model, the Tweet Propagation Model (TPM), in which we infer dynamic probabilistic distributions over interests and topics. We then explicitly consider novelty, coverage, and diversity to arrive at an iterative optimization algorithm for selecting tweets. Experimental results validate the effectiveness of our personalized time-aware tweets summarization method based on TPM.},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {513–522},
numpages = {10},
keywords = {tweets summarization, twitter, data enrichment, topic modeling},
location = {Dublin, Ireland},
series = {SIGIR '13}
}

@inproceedings{wang,
    title = "Graph-based Dependency Parsing with Bidirectional {LSTM}",
    author = "Wang, Wenhui  and
      Chang, Baobao",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1218",
    doi = "10.18653/v1/P16-1218",
    pages = "2306--2315",
}

@article{francis,
    author = {Pelletier, Francis},
    year = {1994},
    month = {03},
    pages = {11-24},
    title = {The Principle of Semantic Compositionality},
    volume = {13},
    journal = {Topoi},
    doi = {10.1007/BF00763644}
}

@misc{pang,
    doi = {10.48550/ARXIV.CS/0409058},
    
    url = {https://arxiv.org/abs/cs/0409058},
    
    author = {Pang, Bo and Lee, Lillian},
    
    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
    
    title = {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},
    
    publisher = {arXiv},
    
    year = {2004},
    
    copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
}

@article{xinzhao,
  author    = {Lingyun Zhao and
               Lin Li and
               Xinhao Zheng},
  title     = {A {BERT} based Sentiment Analysis and Key Entity Detection Approach
               for Online Financial Texts},
  journal   = {CoRR},
  volume    = {abs/2001.05326},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.05326},
  eprinttype = {arXiv},
  eprint    = {2001.05326},
  timestamp = {Thu, 14 Jul 2022 09:10:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-05326.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nielker,
    author    = {Christopher Schr{\"{o}}der and
               Andreas Niekler},
    title     = {A Survey of Active Learning for Text Classification using Deep Neural
               Networks},
    journal   = {CoRR},
    volume    = {abs/2008.07267},
    year      = {2020},
    url       = {https://arxiv.org/abs/2008.07267},
    eprinttype = {arXiv},
    eprint    = {2008.07267},
    timestamp = {Fri, 21 Aug 2020 15:05:50 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-2008-07267.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}


@Article{kowasari,
    AUTHOR = {Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
    TITLE = {Text Classification Algorithms: A Survey},
    JOURNAL = {Information},
    VOLUME = {10},
    YEAR = {2019},
    NUMBER = {4},
    ARTICLE-NUMBER = {150},
    URL = {https://www.mdpi.com/2078-2489/10/4/150},
    ISSN = {2078-2489},
    ABSTRACT = {In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed.},
    DOI = {10.3390/info10040150}
}

@misc{torfi,
    doi = {10.48550/ARXIV.2003.01200},
    
    url = {https://arxiv.org/abs/2003.01200},
    
    author = {Torfi, Amirsina and Shirvani, Rouzbeh A. and Keneshloo, Yaser and Tavaf, Nader and Fox, Edward A.},
    
    keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    
    title = {Natural Language Processing Advancements By Deep Learning: A Survey},
    
    publisher = {arXiv},
    
    year = {2020},
    
    copyright = {Creative Commons Attribution 4.0 International}
}

@article{hold,
    doi = {10.1007/s13042-022-01553-3},
    journal = { International Journal of Machine Learning and Cybernetics},
    url = {https://link.springer.com/article/10.1007/s13042-022-01553-3},
    author = {Bayer M. and Kaufhold, MA. and Buchhold and B. et al.},
    title = {Data augmentation in natural language processing: a novel text generation approach for long and short text classifiers},
    year = {2022}
}

@INPROCEEDINGS{chai,  
    author={Chai, Junyi and Li, Anming},  
    booktitle={2019 International Conference on Machine Learning and Cybernetics (ICMLC)},   title={Deep Learning in Natural Language Processing: A State-of-the-Art Survey},   
    year={2019},  
    pages={1-6},  
    doi={10.1109/ICMLC48188.2019.8949185}
}

@article{gupta,
    title = {Abstractive summarization: An overview of the state of the art},
    journal = {Expert Systems with Applications},
    volume = {121},
    pages = {49-65},
    year = {2019},
    issn = {0957-4174},
    doi = {https://doi.org/10.1016/j.eswa.2018.12.011},
    url = {https://www.sciencedirect.com/science/article/pii/S0957417418307735},
    author = {Som Gupta and S. K Gupta},
    keywords = {Abstractive summarization, Concept finding, Semantic-Based summarization, Ontology-Based summarization, Deep learning},
    abstract = {Summarization, is to reduce the size of the document while preserving the meaning, is one of the most researched areas among the Natural Language Processing (NLP) community. Summarization techniques, on the basis of whether the exact sentences are considered as they appear in the original text or new sentences are generated using natural language processing techniques, are categorized into extractive and abstractive techniques. Extractive summarization has been a very extensively researched topic and has reached to its maturity stage. Now the research has shifted towards the abstractive summarization. The complexities underlying with the natural language text makes abstractive summarization a difficult and a challenging task. This paper presents a comprehensive review of the various works performed in abstractive summarization field. For this purpose, we have selected the recent papers on this topic from Elsevier, ACM, IEEE, Springer, ACL Anthology, Cornell University Library and Google Scholar. The papers are categorized according to the type of abstractive technique used. The paper lists down the various challenges and discusses the future direction for research in this field. Along with these, we have identified the advantages and disadvantages of various methods used for abstractive summarization. We have also listed down the various tools which have been used or developed by researchers for abstractive summarization. The paper also discusses the evaluation techniques being used for assessing the abstractive summaries.}
}

@article{hussein,
    title = {A survey on sentiment analysis challenges},
    journal = {Journal of King Saud University - Engineering Sciences},
    volume = {30},
    number = {4},
    pages = {330-338},
    year = {2018},
    issn = {1018-3639},
    doi = {https://doi.org/10.1016/j.jksues.2016.04.002},
    url = {https://www.sciencedirect.com/science/article/pii/S1018363916300071},
    author = {Doaa Mohey El-Din Mohamed Hussein},
    keywords = {Sentiment analysis, Text analysis, Sentiment analysis challenges, Sentiments, Review structure, Accuracy},
    abstract = {With accelerated evolution of the internet as websites, social networks, blogs, online portals, reviews, opinions, recommendations, ratings, and feedback are generated by writers. This writer generated sentiment content can be about books, people, hotels, products, research, events, etc. These sentiments become very beneficial for businesses, governments, and individuals. While this content is meant to be useful, a bulk of this writer generated content require using the text mining techniques and sentiment analysis. But there are several challenges facing the sentiment analysis and evaluation process. These challenges become obstacles in analyzing the accurate meaning of sentiments and detecting the suitable sentiment polarity. Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text. This paper presents a survey on the sentiment analysis challenges relevant to their approaches and techniques.}
}

@INPROCEEDINGS{Karamouzas2022,
  author={Karamouzas, Dionysios and Mademlis, Ioannis and Pitas, Ioannis},
  booktitle={2022 IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)}, 
  title={Neural Knowledge Transfer for Sentiment Analysis in Texts with Figurative Language}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/MLSP55214.2022.9943314}
}


@article{phan,
  author       = {Long N. Phan and
                  James T. Anibal and
                  Hieu Tran and
                  Shaurya Chanana and
                  Erol Bahadroglu and
                  Alec Peltekian and
                  Gr{\'{e}}goire Altan{-}Bonnet},
  title        = {SciFive: a text-to-text transformer model for biomedical literature},
  journal      = {CoRR},
  volume       = {abs/2106.03598},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.03598},
  eprinttype    = {arXiv},
  eprint       = {2106.03598},
  timestamp    = {Thu, 14 Oct 2021 09:17:30 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-03598.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nabizadeh,
    title = "{M}y{F}ixit: An Annotated Dataset, Annotation Tool, and Baseline Methods for Information Extraction from Repair Manuals",
    author = "Nabizadeh, Nima  and
      Kolossa, Dorothea  and
      Heckmann, Martin",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.260",
    pages = "2120--2128",
    abstract = "Text instructions are among the most widely used media for learning and teaching. Hence, to create assistance systems that are capable of supporting humans autonomously in new tasks, it would be immensely productive, if machines were enabled to extract task knowledge from such text instructions. In this paper, we, therefore, focus on information extraction (IE) from the instructional text in repair manuals. This brings with it the multiple challenges of information extraction from the situated and technical language in relatively long and often complex instructions. To tackle these challenges, we introduce a semi-structured dataset of repair manuals. The dataset is annotated in a large category of devices, with information that we consider most valuable for an automated repair assistant, including the required tools and the disassembled parts at each step of the repair progress. We then propose methods that can serve as baselines for this IE task: an unsupervised method based on a bags-of-n-grams similarity for extracting the needed tools in each repair step, and a deep-learning-based sequence labeling model for extracting the identity of disassembled parts. These baseline methods are integrated into a semi-automatic web-based annotator application that is also available along with the dataset.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{merchant,
  title={Nlp based latent semantic analysis for legal text summarization},
  author={Merchant, Kaiz and Pande, Yash},
  booktitle={2018 International conference on advances in computing, communications and informatics (ICACCI)},
  pages={1803--1807},
  year={2018},
  organization={IEEE}
}

@article{shuo,
  title={A long-text classification method of Chinese news based on BERT and CNN},
  author={Chen, Xinying and Cong, Peimin and Lv, Shuo},
  journal={IEEE Access},
  volume={10},
  pages={34046--34057},
  year={2022},
  publisher={IEEE}
}

@ARTICLE{lenet,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}


@article{alexnet,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    year = {2017},
    issue_date = {June 2017},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {60},
    number = {6},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/3065386},
    doi = {10.1145/3065386},
    abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
    journal = {Commun. ACM},
    month = {may},
    pages = {84–90},
    numpages = {7}
}

@article{koh,
author = {Koh, Huan Yee and Ju, Jiaxin and Liu, Ming and Pan, Shirui},
title = {An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics},
year = {2022},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3545176},
doi = {10.1145/3545176},
abstract = {Long documents such as academic articles and business reports have been the standard format to detail out important issues and complicated subjects that require extra attention. An automatic summarization system that can effectively condense long documents into short and concise texts to encapsulate the most important information would thus be significant in aiding the reader’s comprehension. Recently, with the advent of neural architectures, significant research efforts have been made to advance automatic text summarization systems, and numerous studies on the challenges of extending these systems to the long document domain have emerged. In this survey, we provide a comprehensive overview of the research on long document summarization and a systematic evaluation across the three principal components of its research setting: benchmark datasets, summarization models, and evaluation metrics. For each component, we organize the literature within the context of long document summarization and conduct an empirical analysis to broaden the perspective on current research progress. The empirical analysis includes a study on the intrinsic characteristics of benchmark datasets, a multi-dimensional analysis of summarization models, and a review of the summarization evaluation metrics. Based on the overall findings, we conclude by proposing possible directions for future exploration in this rapidly growing field.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {154},
numpages = {35},
keywords = {neural networks, language models, Document summarization, datasets, transformer}
}

@misc{omori,
      title={Computational Storytelling and Emotions: A Survey}, 
      author={Yusuke Mori and Hiroaki Yamane and Yusuke Mukuta and Tatsuya Harada},
      year={2022},
      eprint={2205.10967},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
	jose,
	author = {Andreas Lüschow and José Calvo Tello},
	title = {Towards Genre Classification in the Library Catalog},
	year = {2021},
	journal = {Proceedings of the Conference on Digital Curation Technologies (Qurator 2021)},
	abstract = {Library catalogs usually do not contain explicit information about the genre of literary works. However, both for readers and for researchers this information can be useful, e.g, when it comes to creating research corpora based on genre. In this proposal, we offer a first analysis about how genre information has been integrated in catalogs to date, train several algorithms to classify annotated instances and contrast the results to the fields from the library catalogs used as features. Our study is based on data sets from the union catalog (GVK) of the German Common Library Network(GBV).},
	url = {http://ceur-ws.org/Vol-2836/#qurator2021_paper_9},
}

@inproceedings{glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@article{elmo,
  author       = {Matthew E. Peters and
                  Mark Neumann and
                  Mohit Iyyer and
                  Matt Gardner and
                  Christopher Clark and
                  Kenton Lee and
                  Luke Zettlemoyer},
  title        = {Deep contextualized word representations},
  journal      = {CoRR},
  volume       = {abs/1802.05365},
  year         = {2018},
  url          = {http://arxiv.org/abs/1802.05365},
  eprinttype    = {arXiv},
  eprint       = {1802.05365},
  timestamp    = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1802-05365.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bahdanau,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{huynh,
  author={Huynh-The, Thien and Hua, Cam-Hao and Kim, Jae-Woo and Kim, Seung-Hwan and Kim, Dong-Seong},
  booktitle={2020 IEEE Wireless Communications and Networking Conference (WCNC)}, 
  title={Exploiting a low-cost CNN with skip connection for robust automatic modulation classification}, 
  year={2020},
  pages={1-6},
  doi={10.1109/WCNC45663.2020.9120667}
}

@article{kowsari,
AUTHOR = {Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
TITLE = {Text Classification Algorithms: A Survey},
JOURNAL = {Information},
VOLUME = {10},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {150},
URL = {https://www.mdpi.com/2078-2489/10/4/150},
ISSN = {2078-2489},
ABSTRACT = {In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed.},
DOI = {10.3390/info10040150}
}

@article{nal,
    author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
    title = {Deep Learning--Based Text Classification: A Comprehensive Review},
    year = {2021},
    issue_date = {April 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {54},
    number = {3},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3439726},
    doi = {10.1145/3439726},
    abstract = {Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.},
    journal = {ACM Comput. Surv.},
    month = {apr},
    articleno = {62},
    numpages = {40},
    keywords = {deep learning, Text classification, topic classification, sentiment analysis, question answering, news categorization, natural language inference}
}

@INPROCEEDINGS{wagh,

  author={Wagh, Vedangi and Khandve, Snehal and Joshi, Isha and Wani, Apurva and Kale, Geetanjali and Joshi, Raviraj},

  booktitle={TENCON 2021 - 2021 IEEE Region 10 Conference (TENCON)}, 

  title={Comparative Study of Long Document Classification}, 

  year={2021},

  volume={},

  number={},

  pages={732-737},

  doi={10.1109/TENCON54134.2021.9707465}
}

@article{poon,
    title = {Applications and Enhancement of Document-Based Sentiment Analysis in Deep learning Methods: Systematic Literature Review},
    journal = {Intelligent Systems with Applications},
    volume = {15},
    pages = {200090},
    year = {2022},
    issn = {2667-3053},
    doi = {https://doi.org/10.1016/j.iswa.2022.200090},
    url = {https://www.sciencedirect.com/science/article/pii/S2667305322000308},
    author = {Faisal Alshuwaier and Ali Areshey and Josiah Poon},
    keywords = {Deep Learning Models, Document Level, Sentiment Analysis, Applications, Improvement, SLR},
    abstract = {Sentiment analysis has become a highly effective research field in the natural language domain and has a large scope of real-world implementations. An existing active study concentration for sentiment analysis is the development of graininess at the document level, appearing with two featured objectives: subjectivity classification, which determines whether a document is objective or subjective and sentiment detection which defines whether or not a document has a sentiment. Deep learning approaches have featured as a chance for developing these objectives with their ability to present both syntactic and semantic characteristics of text without demands for high-level attribute engineering. In this paper, we focus to produce a systematic literature review of deep learning methods for document-based sentiment analysis to determine different features in the text. In addition, this systematic literature review presents a brief survey, evaluation, enhancement of recent developments in the field of sentiment analysis techniques and applications of documents for deep learning, starting with the Convolutional Neural Network, continues to cover the Recurrent Neural Network, including Long Short-Term Memory and Gated Repetitive Units. This review also contains the implementation and application of Recursive Neural Network, Deep Belief Network, Domain-Adversarial Network Models and Hybrid Neural Network. This work considers most of the papers published when the history of deep learning began, and specifically the sentiment analysis of the documents.}
}

@article{lulu,
  author       = {Lulu Wan and
                  George Papageorgiou and
                  Michael Seddon and
                  Mirko Bernardoni},
  title        = {Long-length Legal Document Classification},
  journal      = {CoRR},
  volume       = {abs/1912.06905},
  year         = {2019},
  url          = {http://arxiv.org/abs/1912.06905},
  eprinttype    = {arXiv},
  eprint       = {1912.06905},
  timestamp    = {Sat, 23 Jan 2021 01:20:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1912-06905.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{bambroo,

  author={Bambroo, Purbid and Awasthi, Aditi},

  booktitle={2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)}, 

  title={LegalDB: Long DistilBERT for Legal Document Classification}, 

  year={2021},

  volume={},

  number={},

  pages={1-4},

  doi={10.1109/ICAECT49130.2021.9392558}}

@InProceedings{vithya,
author="Yogarajan, Vithya
and Montiel, Jacob
and Smith, Tony
and Pfahringer, Bernhard",
editor="Tucker, Allan
and Henriques Abreu, Pedro
and Cardoso, Jaime
and Pereira Rodrigues, Pedro
and Ria{\~{n}}o, David",
title="Transformers for Multi-label Classification of Medical Text: An Empirical Comparison",
booktitle="Artificial Intelligence in Medicine",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="114--123",
abstract="Recent advancements in machine learning-based multi-label medical text classification techniques have been used to help enhance healthcare and aid better patient care. This research is motivated by transformers' success in natural language processing tasks, and the opportunity to further improve performance for medical-domain specific tasks by exploiting models pre-trained on health data. We consider transfer learning involving fine-tuning of pre-trained models for predicting medical codes, formulated as a multi-label problem. We find that domain-specific transformers outperform state-of-the-art results for multi-label problems with the number of labels ranging from 18 to 158, for a fixed sequence length. Additionally, we find that, for longer documents and/or number of labels greater than 300, traditional neural networks still have an edge over transformers. These findings are obtained by performing extensive experiments on the semi-structured eICU data and the free-form MIMIC III data, and applying various transformers including BERT, RoBERTa, and Longformer variations. The electronic health record data used in this research exhibits a high level of label imbalance. Considering individual label accuracy, we find that for eICU data medical-domain specific RoBERTa models achieve improvements for more frequent labels. For infrequent labels, in both datasets, traditional neural networks still perform better.",
isbn="978-3-030-77211-6"
}

@article{dale, title={Law and Word Order: NLP in Legal Tech}, volume={25}, DOI={10.1017/S1351324918000475}, number={1}, journal={Natural Language Engineering}, publisher={Cambridge University Press}, author={DALE, ROBERT}, year={2019}, pages={211–217}}

@article{borgir,
  title={A survey on sentence level sentiment analysis},
  author={Bongirwar, Vrushali K},
  journal={International Journal of Computer Science Trends and Technology (IJCST)},
  volume={3},
  number={3},
  pages={110--113},
  year={2015}
}

@inproceedings{guo,
author = {Ai, Qingyao and Yang, Liu and Guo, Jiafeng and Croft, W. Bruce},
title = {Analysis of the Paragraph Vector Model for Information Retrieval},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970409},
doi = {10.1145/2970398.2970409},
abstract = {Previous studies have shown that semantically meaningful representations of words and text can be acquired through neural embedding models. In particular, paragraph vector (PV) models have shown impressive performance in some natural language processing tasks by estimating a document (topic) level language model. Integrating the PV models with traditional language model approaches to retrieval, however, produces unstable performance and limited improvements. In this paper, we formally discuss three intrinsic problems of the original PV model that restrict its performance in retrieval tasks. We also describe modifications to the model that make it more suitable for the IR task, and show their impact through experiments and case studies. The three issues we address are (1) the unregulated training process of PV is vulnerable to short document over-fitting that produces length bias in the final retrieval model; (2) the corpus-based negative sampling of PV leads to a weighting scheme for words that overly suppresses the importance of frequent words; and (3) the lack of word-context information makes PV unable to capture word substitution relationships.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {133–142},
numpages = {10},
keywords = {paragraph vector, language model},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@article{andrew,
  author       = {Andrew M. Dai and
                  Christopher Olah and
                  Quoc V. Le},
  title        = {Document Embedding with Paragraph Vectors},
  journal      = {CoRR},
  volume       = {abs/1507.07998},
  year         = {2015},
  url          = {http://arxiv.org/abs/1507.07998},
  eprinttype    = {arXiv},
  eprint       = {1507.07998},
  timestamp    = {Mon, 13 Aug 2018 16:47:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/DaiOL15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{timothy,
  author       = {Shraey Bhatia and
                  Jey Han Lau and
                  Timothy Baldwin},
  title        = {An Automatic Approach for Document-level Topic Model Evaluation},
  journal      = {CoRR},
  volume       = {abs/1706.05140},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.05140},
  eprinttype    = {arXiv},
  eprint       = {1706.05140},
  timestamp    = {Mon, 13 Aug 2018 16:47:32 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BhatiaLB17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{jiwei,
  author       = {Jiwei Li and
                  Minh{-}Thang Luong and
                  Dan Jurafsky},
  title        = {A Hierarchical Neural Autoencoder for Paragraphs and Documents},
  journal      = {CoRR},
  volume       = {abs/1506.01057},
  year         = {2015},
  url          = {http://arxiv.org/abs/1506.01057},
  eprinttype    = {arXiv},
  eprint       = {1506.01057},
  timestamp    = {Sun, 12 Mar 2023 00:56:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/LiLJ15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wei,
  title={Hierarchical interaction networks with rethinking mechanism for document-level sentiment analysis},
  author={Wei, Lingwei and Hu, Dou and Zhou, Wei and Tang, Xuehai and Zhang, Xiaodan and Wang, Xin and Han, Jizhong and Hu, Songlin},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14--18, 2020, Proceedings, Part III},
  pages={633--649},
  year={2021},
  organization={Springer}
}

@article{patentnet,
  title={PatentNet: multi-label classification of patent documents using deep learning based language understanding},
  author={Haghighian Roudsari, Arousha and Afshar, Jafar and Lee, Wookey and Lee, Suan},
  journal={Scientometrics},
  pages={1--25},
  year={2022},
  publisher={Springer}
}

@INPROCEEDINGS{hsu,

  author={Hsu, Chung-Chian and Chang, Pei-Chi and Chang, Arthur},

  booktitle={2020 International Symposium on Community-centric Systems (CcS)}, 

  title={Multi-Label Classification of ICD Coding Using Deep Learning}, 

  year={2020},

  volume={},

  number={},

  pages={1-6},

  doi={10.1109/CcS49175.2020.9231498}}

@inproceedings{1453,
    title = "{E}mo{N}et: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks",
    author = "Abdul-Mageed, Muhammad  and
      Ungar, Lyle",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1067",
    doi = "10.18653/v1/P17-1067",
    pages = "718--728",
    abstract = "Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58{\%}). We also extend the task beyond emotion types to model Robert Plutick{'}s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68{\%}.",
}

@InProceedings{GRNN,
  title = 	 {Gated Feedback Recurrent Neural Networks},
  author = 	 {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2067--2075},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/chung15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/chung15.html},
  abstract = 	 {In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.}
}


@inproceedings{mehta,
    title = "From Extractive to Abstractive Summarization: A Journey",
    author = "Mehta, Parth",
    booktitle = "Proceedings of the {ACL} 2016 Student Research Workshop",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-3015",
    doi = "10.18653/v1/P16-3015",
    pages = "100--106",
}

@article{clement,
  author       = {Colin B. Clement and
                  Matthew Bierbaum and
                  Kevin P. O'Keeffe and
                  Alexander A. Alemi},
  title        = {On the Use of ArXiv as a Dataset},
  journal      = {CoRR},
  volume       = {abs/1905.00075},
  year         = {2019},
  url          = {http://arxiv.org/abs/1905.00075},
  eprinttype    = {arXiv},
  eprint       = {1905.00075},
  timestamp    = {Mon, 27 May 2019 13:15:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-00075.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{tf_datasets,
  key = {tf-datasets},
  title = {Tensorflow scientific papers dataset},
  howpublished = {\url{https://www.tensorflow.org/datasets/catalog/scientific_papers}},
  note = {Accessed: 2023-04-22},
  year={nodate}
}

@misc{kaggle_arxiv,
  title = {Kaggle arXiv dataset},
  howpublished = {\url{https://www.tensorflow.org/datasets/catalog/scientific_papers}},
  note = {Accessed: 2023-04-22},
  year={2020},
  author={Timo Bozsolik}
}

@misc{hugging_face_arxiv,
  key = {arxiv-hugging-face},
  title = {Hugging face library arXiv dataset},
  howpublished = {\url{https://www.tensorflow.org/datasets/catalog/scientific_papers}},
  note = {Accessed: 2023-04-22},
  year={nodate}
}

@article{franck,
  title={Pubmed 200k rct: a dataset for sequential sentence classification in medical abstracts},
  author={Dernoncourt, Franck and Lee, Ji Young},
  journal={arXiv preprint arXiv:1710.06071},
  year={2017}
}

@inproceedings{sumpubmed,
  title={SumPubMed: Summarization dataset of PubMed scientific articles},
  author={Gupta, Vivek and Bharti, Prerna and Nokhiz, Pegah and Karnick, Harish},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop},
  pages={292--303},
  year={2021}
}

@article{gong,
  title={End-to-end neural sentence ordering using pointer network},
  author={Gong, Jingjing and Chen, Xinchi and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:1611.04953},
  year={2016}
}

@misc{hugging_face_pubmed,
    key = {pubmed-hugging-face},
  title = {Hugging face library arXiv dataset},
  howpublished = {\url{https://huggingface.co/datasets/pubmed}},
  note = {Accessed: 2023-04-22},
  year={nodate}
}

@article{gales,
  author       = {Potsawee Manakul and
                  Mark J. F. Gales},
  title        = {Long-Span Dependencies in Transformer-based Summarization Systems},
  journal      = {CoRR},
  volume       = {abs/2105.03801},
  year         = {2021},
  url          = {https://arxiv.org/abs/2105.03801},
  eprinttype    = {arXiv},
  eprint       = {2105.03801},
  timestamp    = {Fri, 14 May 2021 12:13:30 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2105-03801.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{longformer,
  author       = {Iz Beltagy and
                  Matthew E. Peters and
                  Arman Cohan},
  title        = {Longformer: The Long-Document Transformer},
  journal      = {CoRR},
  volume       = {abs/2004.05150},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.05150},
  eprinttype    = {arXiv},
  eprint       = {2004.05150},
  timestamp    = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{child,
  author       = {Rewon Child and
                  Scott Gray and
                  Alec Radford and
                  Ilya Sutskever},
  title        = {Generating Long Sequences with Sparse Transformers},
  journal      = {CoRR},
  volume       = {abs/1904.10509},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.10509},
  eprinttype    = {arXiv},
  eprint       = {1904.10509},
  timestamp    = {Thu, 02 May 2019 15:13:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-10509.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ion,
  author       = {Ilias Chalkidis and
                  Manos Fergadiotis and
                  Prodromos Malakasiotis and
                  Ion Androutsopoulos},
  title        = {Large-Scale Multi-Label Text Classification on {EU} Legislation},
  journal      = {CoRR},
  volume       = {abs/1906.02192},
  year         = {2019},
  url          = {http://arxiv.org/abs/1906.02192},
  eprinttype    = {arXiv},
  eprint       = {1906.02192},
  timestamp    = {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1906-02192.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{junhua,
title = {A comparative study of automated legal text classification using random forests and deep learning},
journal = {Information Processing \& Management},
volume = {59},
number = {2},
pages = {102798},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102798},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002764},
author = {Haihua Chen and Lei Wu and Jiangping Chen and Wei Lu and Junhua Ding},
keywords = {Legal text classification, Machine learning, Deep learning, Domain concept, Word embedding, Random forests},
abstract = {Automated legal text classification is a prominent research topic in the legal field. It lays the foundation for building an intelligent legal system. Current literature focuses on international legal texts, such as Chinese cases, European cases, and Australian cases. Little attention is paid to text classification for U.S. legal texts. Deep learning has been applied to improving text classification performance. Its effectiveness needs further exploration in domains such as the legal field. This paper investigates legal text classification with a large collection of labeled U.S. case documents through comparing the effectiveness of different text classification techniques. We propose a machine learning algorithm using domain concepts as features and random forests as the classifier. Our experiment results on 30,000 full U.S. case documents in 50 categories demonstrated that our approach significantly outperforms a deep learning system built on multiple pre-trained word embeddings and deep neural networks. In addition, applying only the top 400 domain concepts as features for building the random forests could achieve the best performance. This study provides a reference to select machine learning techniques for building high-performance text classification systems in the legal domain or other fields.}
}

@article{qian,
  author       = {Qian Li and
                  Hao Peng and
                  Jianxin Li and
                  Congying Xia and
                  Renyu Yang and
                  Lichao Sun and
                  Philip S. Yu and
                  Lifang He},
  title        = {A Survey on Text Classification: From Shallow to Deep Learning},
  journal      = {CoRR},
  volume       = {abs/2008.00364},
  year         = {2020},
  url          = {https://arxiv.org/abs/2008.00364},
  eprinttype    = {arXiv},
  eprint       = {2008.00364},
  timestamp    = {Tue, 13 Dec 2022 09:59:49 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2008-00364.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ion2,
  author       = {Ilias Chalkidis and
                  Manos Fergadiotis and
                  Sotiris Kotitsas and
                  Prodromos Malakasiotis and
                  Nikolaos Aletras and
                  Ion Androutsopoulos},
  title        = {An Empirical Study on Large-Scale Multi-Label Text Classification
                  Including Few and Zero-Shot Labels},
  journal      = {CoRR},
  volume       = {abs/2010.01653},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.01653},
  eprinttype    = {arXiv},
  eprint       = {2010.01653},
  timestamp    = {Mon, 12 Oct 2020 17:53:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-01653.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mcbert,
  author       = {Ningyu Zhang and
                  Qianghuai Jia and
                  Kangping Yin and
                  Liang Dong and
                  Feng Gao and
                  Nengwei Hua},
  title        = {Conceptualized Representation Learning for Chinese Biomedical Text
                  Mining},
  journal      = {CoRR},
  volume       = {abs/2008.10813},
  year         = {2020},
  url          = {https://arxiv.org/abs/2008.10813},
  eprinttype    = {arXiv},
  eprint       = {2008.10813},
  timestamp    = {Tue, 27 Dec 2022 08:29:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2008-10813.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@Article{ammar,
AUTHOR = {Khadhraoui, Mayara and Bellaaj, Hatem and Ammar, Mehdi Ben and Hamam, Habib and Jmaiel, Mohamed},
TITLE = {Survey of BERT-Base Models for Scientific Text Classification: COVID-19 Case Study},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {6},
ARTICLE-NUMBER = {2891},
URL = {https://www.mdpi.com/2076-3417/12/6/2891},
ISSN = {2076-3417},
ABSTRACT = {On 30 January 2020, the World Health Organization announced a new coronavirus, which later turned out to be very dangerous. Since that date, COVID-19 has spread to become a pandemic that has now affected practically all regions in the world. Since then, many researchers in medicine have contributed to fighting COVID-19. In this context and given the great growth of scientific publications related to this global pandemic, manual text and data retrieval has become a challenging task. To remedy this challenge, we are proposing CovBERT, a pre-trained language model based on the BERT model to automate the literature review process. CovBERT relies on prior training on a large corpus of scientific publications in the biomedical domain and related to COVID-19 to increase its performance on the literature review task. We evaluate CovBERT on the classification of short text based on our scientific dataset of biomedical articles on COVID-19 entitled COV-Dat-20. We demonstrate statistically significant improvements by using BERT.},
DOI = {10.3390/app12062891}
}

@article{so,
    author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
    title = "{BioBERT: a pre-trained biomedical language representation model for biomedical text mining}",
    journal = {Bioinformatics},
    volume = {36},
    number = {4},
    pages = {1234-1240},
    year = {2019},
    month = {09},
    abstract = "{Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts.We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btz682},
    url = {https://doi.org/10.1093/bioinformatics/btz682},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/48983216/bioinformatics\_36\_4\_1234.pdf},
}

@misc{dai,
      title={Revisiting Transformer-based Models for Long Document Classification}, 
      author={Xiang Dai and Ilias Chalkidis and Sune Darkner and Desmond Elliott},
      year={2022},
      eprint={2204.06683},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{dai-etal-2019-transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}

@article{sparse_attention,
  author       = {Rewon Child and
                  Scott Gray and
                  Alec Radford and
                  Ilya Sutskever},
  title        = {Generating Long Sequences with Sparse Transformers},
  journal      = {CoRR},
  volume       = {abs/1904.10509},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.10509},
  eprinttype    = {arXiv},
  eprint       = {1904.10509},
  timestamp    = {Thu, 02 May 2019 15:13:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-10509.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wavenet,
  author       = {A{\"{a}}ron van den Oord and
                  Sander Dieleman and
                  Heiga Zen and
                  Karen Simonyan and
                  Oriol Vinyals and
                  Alex Graves and
                  Nal Kalchbrenner and
                  Andrew W. Senior and
                  Koray Kavukcuoglu},
  title        = {WaveNet: {A} Generative Model for Raw Audio},
  journal      = {CoRR},
  volume       = {abs/1609.03499},
  year         = {2016},
  url          = {http://arxiv.org/abs/1609.03499},
  eprinttype    = {arXiv},
  eprint       = {1609.03499},
  timestamp    = {Thu, 14 Oct 2021 09:15:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/OordDZSVGKSK16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{glue_gunner,
  author       = {Ilias Chalkidis and
                  Abhik Jana and
                  Dirk Hartung and
                  Michael J. Bommarito II and
                  Ion Androutsopoulos and
                  Daniel Martin Katz and
                  Nikolaos Aletras},
  title        = {LexGLUE: {A} Benchmark Dataset for Legal Language Understanding in
                  English},
  journal      = {CoRR},
  volume       = {abs/2110.00976},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.00976},
  eprinttype    = {arXiv},
  eprint       = {2110.00976},
  timestamp    = {Fri, 08 Oct 2021 15:47:55 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-00976.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mimic,
  author       = {Alistair E W Johnson and
                Tom J Pollard and
                  Lu Shen and
                   Li-Wei H Lehman and
                   Li-Wei H Lehman and
                  Mohammad Ghassemi and
                  Benjamin Moody and
                  Peter Szolovits and
                  Leo Anthony Celi and
                  Roger G Mark },
  title        = {MIMIC-III, a freely accessible critical care database },
  journal      = {Sci Data},
  year         = {2016},
  doi = {10.1038/sdata.2016.35},
}

@inproceedings{ion3,
    title = "Paragraph-level Rationale Extraction through Regularization: A case study on {E}uropean Court of Human Rights Cases",
    author = "Chalkidis, Ilias  and
      Fergadiotis, Manos  and
      Tsarapatsanis, Dimitrios  and
      Aletras, Nikolaos  and
      Androutsopoulos, Ion  and
      Malakasiotis, Prodromos",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.22",
    doi = "10.18653/v1/2021.naacl-main.22",
    pages = "226--241",
    abstract = "Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research.",
}

@inproceedings{koreeda,
    title = "{C}ontract{NLI}: A Dataset for Document-level Natural Language Inference for Contracts",
    author = "Koreeda, Yuta  and
      Manning, Christopher",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.164",
    doi = "10.18653/v1/2021.findings-emnlp.164",
    pages = "1907--1919",
    abstract = "Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose {``}document-level natural language inference (NLI) for contracts{''}, a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as {``}Some obligations of Agreement may survive termination.{''}) and a contract, and it is asked to classify whether each hypothesis is {``}entailed by{''}, {``}contradicting to{''} or {``}not mentioned by{''} (neutral to) the contract as well as identifying {``}evidence{''} for the decision as spans in the contract. We annotated and release the largest corpus to date consisting of 607 annotated contracts. We then show that existing models fail badly on our task and introduce a strong baseline, which (a) models evidence identification as multi-label classification over spans instead of trying to predict start and end tokens, and (b) employs more sophisticated context segmentation for dealing with long documents. We also show that linguistic characteristics of contracts, such as negations by exceptions, are contributing to the difficulty of this task and that there is much room for improvement.",
}

@misc{habernal,
      title={Mining Legal Arguments in Court Decisions}, 
      author={Ivan Habernal and Daniel Faber and Nicola Recchia and Sebastian Bretthauer and Iryna Gurevych and Indra Spiecker genannt Döhmann and Christoph Burchard},
      year={2022},
      eprint={2208.06178},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{alicia,
  author       = {Richard Yuanzhe Pang and
                  Alicia Parrish and
                  Nitish Joshi and
                  Nikita Nangia and
                  Jason Phang and
                  Angelica Chen and
                  Vishakh Padmakumar and
                  Johnny Ma and
                  Jana Thompson and
                  He He and
                  Samuel R. Bowman},
  title        = {QuALITY: Question Answering with Long Input Texts, Yes!},
  journal      = {CoRR},
  volume       = {abs/2112.08608},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.08608},
  eprinttype    = {arXiv},
  eprint       = {2112.08608},
  timestamp    = {Tue, 04 Jan 2022 08:25:10 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-08608.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{20groups,
    key={20 Groups},
  title = {20 Newsgroups},
  howpublished = {\url{http://qwone.com/~jason/20Newsgroups/}},
  note = {Accessed: 2023-04-30},
  year={nodate}
}

@inproceedings{ion_han,
    title = "Neural Legal Judgment Prediction in {E}nglish",
    author = "Chalkidis, Ilias  and
      Androutsopoulos, Ion  and
      Aletras, Nikolaos",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1424",
    doi = "10.18653/v1/P19-1424",
    pages = "4317--4323",
    abstract = "Legal judgment prediction is the task of automatically predicting the outcome of a court case, given a text describing the case{'}s facts. Previous work on using neural models for this task has focused on Chinese; only feature-based models (e.g., using bags of words and topics) have been considered in English. We release a new English legal judgment prediction dataset, containing cases from the European Court of Human Rights. We evaluate a broad variety of neural models on the new dataset, establishing strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT{'}s length limitation.",
}

@inproceedings{zichao,
    title = "Hierarchical Attention Networks for Document Classification",
    author = "Yang, Zichao  and
      Yang, Diyi  and
      Dyer, Chris  and
      He, Xiaodong  and
      Smola, Alex  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1174",
    doi = "10.18653/v1/N16-1174",
    pages = "1480--1489",
}

@inproceedings{yanguang,
    title = "Joint Entity and Relation Extraction for Legal Documents with Legal Feature Enhancement",
    author = "Chen, Yanguang  and
      Sun, Yuanyuan  and
      Yang, Zhihao  and
      Lin, Hongfei",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.137",
    doi = "10.18653/v1/2020.coling-main.137",
    pages = "1561--1571",
    abstract = "In recent years, the plentiful information contained in Chinese legal documents has attracted a great deal of attention because of the large-scale release of the judgment documents on China Judgments Online. It is in great need of enabling machines to understand the semantic information stored in the documents which are transcribed in the form of natural language. The technique of information extraction provides a way of mining the valuable information implied in the unstructured judgment documents. We propose a Legal Triplet Extraction System for drug-related criminal judgment documents. The system extracts the entities and the semantic relations jointly and benefits from the proposed legal lexicon feature and multi-task learning framework. Furthermore, we manually annotate a dataset for Named Entity Recognition and Relation Extraction in Chinese legal domain, which contributes to training supervised triplet extraction models and evaluating the model performance. Our experimental results show that the legal feature introduction and multi-task learning framework are feasible and effective for the Legal Triplet Extraction System. The F1 score of triplet extraction finally reaches 0.836 on the legal dataset.",
}

@inproceedings{ion6,
    title = "{LEGAL}-{BERT}: The Muppets straight out of Law School",
    author = "Chalkidis, Ilias  and
      Fergadiotis, Manos  and
      Malakasiotis, Prodromos  and
      Aletras, Nikolaos  and
      Androutsopoulos, Ion",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.261",
    doi = "10.18653/v1/2020.findings-emnlp.261",
    pages = "2898--2904",
    abstract = "BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications.",
}

@article{zheng,
  author       = {Lucia Zheng and
                  Neel Guha and
                  Brandon R. Anderson and
                  Peter Henderson and
                  Daniel E. Ho},
  title        = {When Does Pretraining Help? Assessing Self-Supervised Learning for
                  Law and the CaseHOLD Dataset},
  journal      = {CoRR},
  volume       = {abs/2104.08671},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.08671},
  eprinttype    = {arXiv},
  eprint       = {2104.08671},
  timestamp    = {Fri, 30 Apr 2021 12:52:17 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-08671.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{qi,
    title = "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling",
    author = "Wu, Chuhan  and
      Wu, Fangzhao  and
      Qi, Tao  and
      Huang, Yongfeng",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.107",
    doi = "10.18653/v1/2021.acl-short.107",
    pages = "848--853",
    abstract = "Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. More specifically, we first use a sentence Transformer to learn the representations of each sentence. Then we use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of Hi-Transformer in long document modeling.",
}

@misc{liu,
      title={ERNIE-SPARSE: Learning Hierarchical Efficient Transformer Through Regularized Self-Attention}, 
      author={Yang Liu and Jiaxiang Liu and Li Chen and Yuxiang Lu and Shikun Feng and Zhida Feng and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},
      year={2022},
      eprint={2203.12276},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hyperpartisan,
    key={Hyperpartisan},
  title = {Hyperpartisan dataset},
  howpublished = {\url{https://pan.webis.de/data.html\#pan-semeval-hyperpartisan-news-detection-19}},
  note = {Accessed: 2023-04-30},
  year={nodate}
}

@inproceedings{khandve,
author = {Khandve, Snehal Ishwar and Wagh, Vedangi Kishor and Wani, Apurva Dinesh and Joshi, Isha Mandar and Joshi, Raviraj Bhuminand},
title = {Hierarchical Neural Network Approaches for Long Document Classification},
year = {2022},
isbn = {9781450395700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529836.3529935},
doi = {10.1145/3529836.3529935},
abstract = {Text classification algorithms investigate the intricate relationships between words or phrases and attempt to deduce the document’s interpretation. In the last few years, these algorithms have progressed tremendously. Transformer architecture and sentence encoders have proven to give superior results on natural language processing tasks. But a major limitation of these architectures is their applicability for text no longer than a few hundred words. In this paper, we explore hierarchical transfer learning approaches for long document classification. We employ pre-trained Universal Sentence Encoder (USE) and Bidirectional Encoder Representations from Transformers (BERT) in a hierarchical setup to capture better representations efficiently. Our proposed models are conceptually simple where we divide the input data into chunks and then pass this through base models of BERT and USE. Then output representation for each chunk is then propagated through a shallow neural network comprising of LSTMs or CNNs for classifying the text data. These extensions are evaluated on 6 benchmark datasets. We show that USE + CNN/LSTM performs better than its stand-alone baseline. Whereas the BERT + CNN/LSTM performs on par with its stand-alone counterpart. However, the hierarchical BERT models are still desirable as it avoids the quadratic complexity of the attention mechanism in BERT. Along with the hierarchical approaches, this work also provides a comparison of different deep learning algorithms like USE, BERT, HAN, Longformer, and BigBird for long document classification. The Longformer approach consistently performs well on most of the datasets.},
booktitle = {2022 14th International Conference on Machine Learning and Computing (ICMLC)},
pages = {115–119},
numpages = {5},
keywords = {Hierarchical Approaches, LSTM, BERT, Transformer, Universal sentence encoder, CNN, Document Classification},
location = {Guangzhou, China},
series = {ICMLC 2022}
}

@article{use,
  author       = {Daniel Cer and
                  Yinfei Yang and
                  Sheng{-}yi Kong and
                  Nan Hua and
                  Nicole Limtiaco and
                  Rhomni St. John and
                  Noah Constant and
                  Mario Guajardo{-}Cespedes and
                  Steve Yuan and
                  Chris Tar and
                  Yun{-}Hsuan Sung and
                  Brian Strope and
                  Ray Kurzweil},
  title        = {Universal Sentence Encoder},
  journal      = {CoRR},
  volume       = {abs/1803.11175},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.11175},
  eprinttype    = {arXiv},
  eprint       = {1803.11175},
  timestamp    = {Mon, 13 Aug 2018 16:46:40 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1803-11175.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{big_bird,
  author       = {Manzil Zaheer and
                  Guru Guruganesh and
                  Avinava Dubey and
                  Joshua Ainslie and
                  Chris Alberti and
                  Santiago Onta{\~{n}}{\'{o}}n and
                  Philip Pham and
                  Anirudh Ravula and
                  Qifan Wang and
                  Li Yang and
                  Amr Ahmed},
  title        = {Big Bird: Transformers for Longer Sequences},
  journal      = {CoRR},
  volume       = {abs/2007.14062},
  year         = {2020},
  url          = {https://arxiv.org/abs/2007.14062},
  eprinttype    = {arXiv},
  eprint       = {2007.14062},
  timestamp    = {Mon, 03 Aug 2020 14:32:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2007-14062.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ernie-doc,
    title = "{ERNIE}-{D}oc: A Retrospective Long-Document Modeling Transformer",
    author = "Ding, SiYu  and
      Shang, Junyuan  and
      Wang, Shuohuan  and
      Sun, Yu  and
      Tian, Hao  and
      Wu, Hua  and
      Wang, Haifeng",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.227",
    doi = "10.18653/v1/2021.acl-long.227",
    pages = "2914--2927",
    abstract = "Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.",
}

@misc{GPT,
  added-at = {2020-07-14T16:37:42.000+0200},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords = {final thema:transformer},
  timestamp = {2020-07-14T16:49:42.000+0200},
  title = {Improving language understanding by generative pre-training},
  year = 2018
}

@misc{GPT2,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}

@article{GPT3,
  added-at = {2020-07-28T16:09:05.000+0200},
  author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  biburl = {https://www.bibsonomy.org/bibtex/27a2a9aee490ff30dd5b4d0470a8be8d8/albinzehe},
  interhash = {c02cbc3bfa91c08710d0db948c927dad},
  intrahash = {7a2a9aee490ff30dd5b4d0470a8be8d8},
  journal = {arXiv preprint arXiv:2005.14165},
  keywords = {gpt-3 kallimachos languagemodels proposal-knowledge transformer},
  timestamp = {2020-07-28T16:09:05.000+0200},
  title = {Language models are few-shot learners},
  year = 2020
}

@article{BART,
  author       = {Mike Lewis and
                  Yinhan Liu and
                  Naman Goyal and
                  Marjan Ghazvininejad and
                  Abdelrahman Mohamed and
                  Omer Levy and
                  Veselin Stoyanov and
                  Luke Zettlemoyer},
  title        = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
                  Generation, Translation, and Comprehension},
  journal      = {CoRR},
  volume       = {abs/1910.13461},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.13461},
  eprinttype    = {arXiv},
  eprint       = {1910.13461},
  timestamp    = {Thu, 31 Oct 2019 14:02:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{tanyangxing,
  author       = {Bowen Tan and
                  Zichao Yang and
                  Maruan Al{-}Shedivat and
                  Eric P. Xing and
                  Zhiting Hu},
  title        = {Progressive Generation of Long Text},
  journal      = {CoRR},
  volume       = {abs/2006.15720},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.15720},
  eprinttype    = {arXiv},
  eprint       = {2006.15720},
  timestamp    = {Wed, 01 Jul 2020 15:21:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-15720.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{biRNN,
  author={Schuster, M. and Paliwal, K.K.},
  journal={IEEE Transactions on Signal Processing}, 
  title={Bidirectional recurrent neural networks}, 
  year={1997},
  volume={45},
  number={11},
  pages={2673-2681},
  doi={10.1109/78.650093}}

@article{biLSTM,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Graves, Alex and rahman Mohamed, Abdel and Hinton, Geoffrey E.},
  biburl = {https://www.bibsonomy.org/bibtex/278016fc05a4b1a979c09ec9b6952d4fe/dblp},
  ee = {http://arxiv.org/abs/1303.5778},
  interhash = {1cd74ce6e8dae51149cd2b2d0f08f81a},
  intrahash = {78016fc05a4b1a979c09ec9b6952d4fe},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T12:34:30.000+0200},
  title = {Speech Recognition with Deep Recurrent Neural Networks},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1303.html#abs-1303-5778},
  volume = {abs/1303.5778},
  year = 2013
}




@inproceedings{akter,
    title = "Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than {ROUGE}?",
    author = "Akter, Mousumi  and
      Bansal, Naman  and
      Karmaker, Shubhra Kanti",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.122",
    doi = "10.18653/v1/2022.findings-acl.122",
    pages = "1547--1560",
    abstract = "It has been the norm for a long time to evaluate automated summarization tasks using the popular ROUGE metric. Although several studies in the past have highlighted the limitations of ROUGE, researchers have struggled to reach a consensus on a better alternative until today. One major limitation of the traditional ROUGE metric is the lack of semantic understanding (relies on direct overlap of n-grams). In this paper, we exclusively focus on the extractive summarization task and propose a semantic-aware nCG (normalized cumulative gain)-based evaluation metric (called Sem-nCG) for evaluating this task. One fundamental contribution of the paper is that it demonstrates how we can generate more reliable semantic-aware ground truths for evaluating extractive summarization tasks without any additional human intervention. To the best of our knowledge, this work is the first of its kind. We have conducted extensive experiments with this new metric using the widely used CNN/DailyMail dataset. Experimental results show that the new Sem-nCG metric is indeed semantic-aware, shows higher correlation with human judgement (more reliable) and yields a large number of disagreements with the original ROUGE metric (suggesting that ROUGE often leads to inaccurate conclusions also verified by humans).",
}

@inproceedings{nallapati2,
    title = "Abstractive Text Summarization using Sequence-to-sequence {RNN}s and Beyond",
    author = "Nallapati, Ramesh  and
      Zhou, Bowen  and
      dos Santos, Cicero  and
      Gulcehre, caglar  and
      Xiang, Bing",
    booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K16-1028",
    doi = "10.18653/v1/K16-1028",
    pages = "280--290",
}

@misc{tf_cnn_dailymail,
    key={TensorFlow CNN-DailyMail Dataset},
  title = {TensorFlow CNN-DailyMail Dataset},
  howpublished = {\url{https://www.tensorflow.org/datasets/catalog/cnn_dailymail}},
  note = {Accessed: 2023-05-30},
  year={nodate}
}

@misc{kaggle_cnn_dailymail,
    key={Kaggle CNN-DailyMail Dataset},
  title = {Kaggle CNN-DailyMail Dataset},
  howpublished = {\url{https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail}},
  note = {Accessed: 2023-05-30},
  year={nodate}
}

@misc{hugging_face_cnn_dailymail,
    key={Hugging Face CNN-DailyMail Dataset},
  title = {Hugging Face CNN-DailyMail Dataset},
  howpublished = {\url{https://pan.webis.de/data.html\#pan-semeval-hyperpartisan-news-detection-19}},
  note = {Accessed: 2023-05-30},
  year={nodate}
}

@inproceedings{graham,
    title = "Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}",
    author = "Graham, Yvette",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1013",
    doi = "10.18653/v1/D15-1013",
    pages = "128--137",
}

@article{fabbri,
  author       = {Alexander R. Fabbri and
                  Wojciech Kryscinski and
                  Bryan McCann and
                  Caiming Xiong and
                  Richard Socher and
                  Dragomir R. Radev},
  title        = {SummEval: Re-evaluating Summarization Evaluation},
  journal      = {CoRR},
  volume       = {abs/2007.12626},
  year         = {2020},
  url          = {https://arxiv.org/abs/2007.12626},
  eprinttype    = {arXiv},
  eprint       = {2007.12626},
  timestamp    = {Wed, 29 Jul 2020 15:36:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2007-12626.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{blanc,
  author       = {Oleg V. Vasilyev and
                  Vedant Dharnidharka and
                  John Bohannon},
  title        = {Fill in the {BLANC:} Human-free quality estimation of document summaries},
  journal      = {CoRR},
  volume       = {abs/2002.09836},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.09836},
  eprinttype    = {arXiv},
  eprint       = {2002.09836},
  timestamp    = {Wed, 01 Sep 2021 11:33:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-09836.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ bert_score,
title={BERTScore: Evaluating Text Generation with BERT},
author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@inproceedings{pitler,
    title = "Automatic Evaluation of Linguistic Quality in Multi-Document Summarization",
    author = "Pitler, Emily  and
      Louis, Annie  and
      Nenkova, Ani",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P10-1056",
    pages = "544--554",
}

@inproceedings{s3,
    title = "Learning to Score System Summaries for Better Content Selection Evaluation.",
    author = "Peyrard, Maxime  and
      Botschen, Teresa  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the Workshop on New Frontiers in Summarization",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4510",
    doi = "10.18653/v1/W17-4510",
    pages = "74--84",
    abstract = "The evaluation of summaries is a challenging but crucial task of the summarization field. In this work, we propose to learn an automatic scoring metric based on the human judgements available as part of classical summarization datasets like TAC-2008 and TAC-2009. Any existing automatic scoring metrics can be included as features, the model learns the combination exhibiting the best correlation with human judgments. The reliability of the new metric is tested in a further manual evaluation where we ask humans to evaluate summaries covering the whole scoring spectrum of the metric. We release the trained metric as an open-source tool.",
}

@inproceedings{lapata_bert,
    title = "Text Summarization with Pretrained Encoders",
    author = "Liu, Yang  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1387",
    doi = "10.18653/v1/D19-1387",
    pages = "3730--3740",
    abstract = "Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.",
}

@article{histruct,
  title={Histruct+: Improving extractive text summarization with hierarchical structure information},
  author={Ruan, Qian and Ostendorff, Malte and Rehm, Georg},
  journal={arXiv preprint arXiv:2203.09629},
  year={2022}
}

@article{dirichet,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}

@misc{keybert,
  author       = {Maarten Grootendorst},
  title        = {MaartenGr/KeyBERT: BibTeX},
  month        = jan,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.1.3},
  doi          = {10.5281/zenodo.4461265},
  url          = {https://doi.org/10.5281/zenodo.4461265}
}

@misc{hegel,
      title={HEGEL: Hypergraph Transformer for Long Document Summarization}, 
      author={Haopeng Zhang and Xiao Liu and Jiawei Zhang},
      year={2022},
      eprint={2210.04126},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{matchsum,
      title={Extractive Summarization as Text Matching}, 
      author={Ming Zhong and Pengfei Liu and Yiran Chen and Danqing Wang and Xipeng Qiu and Xuanjing Huang},
      year={2020},
      eprint={2004.08795},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pan_bottom,
      title={Long Document Summarization with Top-down and Bottom-up Inference}, 
      author={Bo Pang and Erik Nijkamp and Wojciech Kryściński and Silvio Savarese and Yingbo Zhou and Caiming Xiong},
      year={2022},
      eprint={2203.07586},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{poland,
      title={BookSum: A Collection of Datasets for Long-form Narrative Summarization}, 
      author={Wojciech Kryściński and Nazneen Rajani and Divyansh Agarwal and Caiming Xiong and Dragomir Radev},
      year={2022},
      eprint={2105.08209},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{summset    ,
  author       = {Mingda Chen and
                  Zewei Chu and
                  Sam Wiseman and
                  Kevin Gimpel},
  title        = {SummScreen: {A} Dataset for Abstractive Screenplay Summarization},
  journal      = {CoRR},
  volume       = {abs/2104.07091},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.07091},
  eprinttype    = {arXiv},
  eprint       = {2104.07091},
  timestamp    = {Sun, 02 Oct 2022 15:32:12 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-07091.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{shin,
      title={Lexicon Integrated CNN Models with Attention for Sentiment Analysis}, 
      author={Bonggun Shin and Timothy Lee and Jinho D. Choi},
      year={2017},
      eprint={1610.06272},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@Article{morocco,
    AUTHOR = {Rhanoui, Maryem and Mikram, Mounia and Yousfi, Siham and Barzali, Soukaina},
    TITLE = {A CNN-BiLSTM Model for Document-Level Sentiment Analysis},
    JOURNAL = {Machine Learning and Knowledge Extraction},
    VOLUME = {1},
    YEAR = {2019},
    NUMBER = {3},
    PAGES = {832--847},
    URL = {https://www.mdpi.com/2504-4990/1/3/48},
    ISSN = {2504-4990},
    DOI = {10.3390/make1030048}
}

@Article{LSTM,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@misc{doc2vec,
      title={Distributed Representations of Sentences and Documents}, 
      author={Quoc V. Le and Tomas Mikolov},
      year={2014},
      eprint={1405.4053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{nyt_dataset,
    title = "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
    author = "Durrett, Greg  and
      Berg-Kirkpatrick, Taylor  and
      Klein, Dan",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1188",
    doi = "10.18653/v1/P16-1188",
    pages = "1998--2008",
}

@inproceedings{gehrmann,
    title = "Bottom-Up Abstractive Summarization",
    author = "Gehrmann, Sebastian  and
      Deng, Yuntian  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1443",
    doi = "10.18653/v1/D18-1443",
    pages = "4098--4109",
    abstract = "Neural summarization produces outputs that are fluent and readable, but which can be poor at content selection, for instance often copying full sentences from the source document. This work explores the use of data-efficient content selectors to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences making it easy to transfer a trained summarizer to a new domain.",
}

@inproceedings{wei2,
    title = "Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling",
    author = "Li, Wei  and
      Xiao, Xinyan  and
      Lyu, Yajuan  and
      Wang, Yuanzhuo",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1205",
    doi = "10.18653/v1/D18-1205",
    pages = "1787--1796",
    abstract = "Information selection is the most important component in document summarization task. In this paper, we propose to extend the basic neural encoding-decoding framework with an information selection layer to explicitly model and optimize the information selection process in abstractive document summarization. Specifically, our information selection layer consists of two parts: gated global information filtering and local sentence selection. Unnecessary information in the original document is first globally filtered, then salient sentences are selected locally while generating each summary sentence sequentially. To optimize the information selection process directly, distantly-supervised training guided by the golden summary is also imported. Experimental results demonstrate that the explicit modeling and optimizing of the information selection process improves document summarization performance significantly, which enables our model to generate more informative and concise summaries, and thus significantly outperform state-of-the-art neural abstractive methods.",
}

@inproceedings{kryscinski,
    title = "Evaluating the Factual Consistency of Abstractive Text Summarization",
    author = "Kryscinski, Wojciech  and
      McCann, Bryan  and
      Xiong, Caiming  and
      Socher, Richard",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.750",
    doi = "10.18653/v1/2020.emnlp-main.750",
    pages = "9332--9346",
    abstract = "The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents.The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.",
}
@inproceedings{kim,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}

@article{hota,
  title={Time series data prediction using sliding window based RBF neural network},
  author={Hota, HS and Handa, Richa and Shrivas, Akhilesh Kumar},
  journal={International Journal of Computational Intelligence Research},
  volume={13},
  number={5},
  pages={1145--1156},
  year={2017}
}

@article{saeed,
title = {Online fault monitoring based on deep neural network \& sliding window technique},
journal = {Progress in Nuclear Energy},
volume = {121},
pages = {103236},
year = {2020},
issn = {0149-1970},
doi = {https://doi.org/10.1016/j.pnucene.2019.103236},
url = {https://www.sciencedirect.com/science/article/pii/S0149197019303427},
author = {Hanan A. Saeed and Hang Wang and Minjun Peng and Anwar Hussain and Amjad Nawaz},
keywords = {Fault monitoring, Fault diagnosis, Neural network, Deep learning, Integrated pressurized water reactor, Sliding window technique, Convolution neural network},
abstract = {Nuclear power plants have proved their worth in energy sector by providing clean and uninterrupted power over decades. However, a Nuclear Power Plant (NPP) is a complex, dynamic system with potential radioactive release risk which makes it crucial to achieve highest standards of safety. Specially, in preview of massive monitoring data received in modern NPPs which makes it difficult for operators to extract vital information about actual plant state in a timely and accurate manner. On the other hand, advancements in latest machine learning methods have made it possible to process such massive data for operators to act accordingly. However, current machine learning approaches cited for this field, fall short of required capabilities needed for such safety critical industry. In manuscript, an online fault monitoring system is proposed which utilizes deep neural networks and sliding window technique. The proposed model not only fulfills the requirement of validity but also encompass all necessary diagnosis functions like detection, identification, assessment and robustness. The model allows for a fault to be identified and assessed in different plant states and then validate the predicted results through online correlation of simulation vs original data. The study was conducted for IP-200 NPP utilizing RELAP5 thermal-hydraulic code. The proposed model was verified by inducing 04 different faults for different states and severities. The results were found to be conducive for improving reliability and accuracy of next generation fault monitoring systems of Nuclear Power Plants.}
}

@inproceedings{odysseas,
author = {Krystalakos, Odysseas and Nalmpantis, Christoforos and Vrakas, Dimitris},
title = {Sliding Window Approach for Online Energy Disaggregation Using Artificial Neural Networks},
year = {2018},
isbn = {9781450364331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3200947.3201011},
doi = {10.1145/3200947.3201011},
abstract = {Energy disaggregation is the process of extracting the power consumptions of multiple appliances from the total consumption signal of a building. Artificial Neural Networks (ANN) have been very popular for this task in the last decade. In this paper we propose two recurrent network architectures that use sliding window for real-time energy disaggregation. We compare this approach to existing techniques using six metrics and find that it scores better for multi-state devices. Finally, we compare ANNs that use Gated Recurrent Unit neurons against those using Long Short-Term Memory neurons and find that they perform equally.},
booktitle = {Proceedings of the 10th Hellenic Conference on Artificial Intelligence},
articleno = {7},
numpages = {6},
keywords = {non-intrusive load monitoring, artificial neural networks, energy disaggregation},
location = {Patras, Greece},
series = {SETN '18}
}

@INPROCEEDINGS{kiros,

  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},

  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 

  title={Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books}, 

  year={2015},

  volume={},

  number={},

  pages={19-27},

  doi={10.1109/ICCV.2015.11}}

@article{trinh,
  title={A simple method for commonsense reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}

@inproceedings{seq2seq,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}


@inproceedings{tokens,
  title={Tokenization as the initial phase in NLP},
  author={Webster, Jonathan J and Kit, Chunyu},
  booktitle={COLING 1992 volume 4: The 14th international conference on computational linguistics},
  year={1992}
}

@article{ngrams,
title = {Syntactic N-grams as machine learning features for natural language processing},
journal = {Expert Systems with Applications},
volume = {41},
number = {3},
pages = {853-860},
year = {2014},
note = {Methods and Applications of Artificial and Computational Intelligence},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413006271},
author = {Grigori Sidorov and Francisco Velasquez and Efstathios Stamatatos and Alexander Gelbukh and Liliana Chanona-Hernández},
keywords = {Syntactic n-grams, sn-Grams, Parsing, Classification features, Syntactic paths, Authorship attribution, SVM, NB, J48},
abstract = {In this paper we introduce and discuss a concept of syntactic n-grams (sn-grams). Sn-grams differ from traditional n-grams in the manner how we construct them, i.e., what elements are considered neighbors. In case of sn-grams, the neighbors are taken by following syntactic relations in syntactic trees, and not by taking words as they appear in a text, i.e., sn-grams are constructed by following paths in syntactic trees. In this manner, sn-grams allow bringing syntactic knowledge into machine learning methods; still, previous parsing is necessary for their construction. Sn-grams can be applied in any natural language processing (NLP) task where traditional n-grams are used. We describe how sn-grams were applied to authorship attribution. We used as baseline traditional n-grams of words, part of speech (POS) tags and characters; three classifiers were applied: support vector machines (SVM), naive Bayes (NB), and tree classifier J48. Sn-grams give better results with SVM classifier.}
}

@article{sa_example_1,
  title={Online sentiment analysis in marketing research: a review},
  author={Rambocas, Meena and Pacheco, Barney G},
  journal={Journal of Research in Interactive Marketing},
  year={2018},
  publisher={Emerald Publishing Limited}
}

@techreport{sa_example_2,
  title={Marketing research: The role of sentiment analysis},
  author={Rambocas, Meena and Gama, Jo{\~a}o and others},
  year={2013},
  institution={Universidade do Porto, Faculdade de Economia do Porto}
}

@article{sa_example_3,
  title={Analyzing user sentiment in social media: Implications for online marketing strategy},
  author={Micu, Adrian and Micu, Angela Eliza and Geru, Marius and Lixandroiu, Radu Constantin},
  journal={Psychology \& Marketing},
  volume={34},
  number={12},
  pages={1094--1100},
  year={2017},
  publisher={Wiley Online Library}
}

@INPROCEEDINGS{sa_example_4,

  author={Ramteke, Jyoti and Shah, Samarth and Godhia, Darshan and Shaikh, Aadil},

  booktitle={2016 International Conference on Inventive Computation Technologies (ICICT)}, 

  title={Election result prediction using Twitter sentiment analysis}, 

  year={2016},

  volume={1},

  number={},

  pages={1-5},

  doi={10.1109/INVENTIVE.2016.7823280}}

@INPROCEEDINGS{sa_example_5,

  author={Kaya, Mesut and Fidan, Güven and Toroslu, Ismail H.},

  booktitle={2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology}, 

  title={Sentiment Analysis of Turkish Political News}, 

  year={2012},

  volume={1},

  number={},

  pages={174-180},

  doi={10.1109/WI-IAT.2012.115}}

  @article{sa_applications,
title = {Sentiment Analysis in Social Media and Its Application: Systematic Literature Review},
journal = {Procedia Computer Science},
volume = {161},
pages = {707-714},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.174},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931885X},
author = {Zulfadzli Drus and Haliyana Khalid},
keywords = {Sentiment analysis, Big data, Social media},
abstract = {This paper is a report of a review on sentiment analysis in social media that explored the methods, social media platform used and its application. Social media contain a large amount of raw data that has been uploaded by users in the form of text, videos, photos and audio. The data can be converted into valuable information by using sentiment analysis. A systematic review of studies published between 2014 to 2019 was undertaken using the following trusted and credible database including ACM, Emerald Insight, IEEE Xplore, Science Direct and Scopus. After the initial and in-depth screening of paper, 24 out of 77 articles have been chosen from the review process. The articles have been reviewed based on the aim of the study. The result shows most of the articles applied opinion-lexicon method to analyses text sentiment in social media, extracted data on microblogging site mainly Twitter and sentiment analysis application can be seen in world events, healthcare, politics and business.}
}
