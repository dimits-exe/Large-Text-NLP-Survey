\documentclass[preprint,review,10pt]{elsarticle}

\pdfoutput=1
%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

\usepackage{amssymb}
\usepackage{tikz}
\usepackage{colortbl}
\usepackage[greek, british]{babel}
\usepackage{alphabeta}
\usepackage{libertine}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{float}
\usepackage{amsmath}
\graphicspath{ {./images/} }
\usepackage[a4paper,margin=1in,footskip=0.25in]{geometry}
\usepackage{etoolbox}
\usepackage{svg}
\makeatletter
\def\ps@pprintTitle{%
	\let\@oddhead\@empty
	\let\@evenhead\@empty
	\def\@oddfoot{\footnotesize\itshape
		{Submitted preprint} \hfill\today}%
	\let\@evenfoot\@oddfoot
}
\makeatother

\newcommand{\xmark}{%
	\tikz[scale=0.23] {
		\draw[line width=0.7,line cap=round] (0,0) to [bend left=6] (1,1);
		\draw[line width=0.7,line cap=round] (0.2,0.95) to [bend right=3] (0.8,0.05);
}}




\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\pagenumbering{arabic}
%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

% \journal{Engineering Applications of Artificial Intelligence}

\begin{document}
	
	\begin{frontmatter}
		
		\title{Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art}
		
		\author{Dimitrios Tsirmpas\textsuperscript{$\dagger$\textsection}, Ioannis Gkionis\textsuperscript{$\dagger$\textsection}, Georgios Th. Papadopoulos\textsuperscript{\textparagraph}, Ioannis Mademlis\textsuperscript{*}\textsuperscript{\textsection}\textsuperscript{\textparagraph}}
		\address{\textsuperscript{\textsection} Department of Informatics, Athens University of Economics and Business}
		\address{\textsuperscript{\textparagraph} Department of Informatics and Telematics, Harokopio University of Athens}
		\fntext[myfootnote1]{\textsuperscript{$\dagger$} The first two authors contributed equally and are joint first authors.}
		\fntext[myfootnote2]{\textsuperscript{*} Ioannis Mademlis is the corresponding author.}
		
		\begin{abstract}
			The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of lengthy texts a critical issue. Relevant applications include automated Web mining, legal document review, medical records analysis, financial reports analysis, contract management, environmental impact assessment, news aggregation, etc. Despite the relatively recent development of efficient algorithms for analyzing long documents, practical tools in this field are currently flourishing. This article serves as an entry point into this dynamic domain and aims to achieve two objectives. Firstly, it provides an overview of the relevant neural building blocks, serving as a concise tutorial for the field. Secondly, it offers a brief examination of the current state-of-the-art in long document NLP, with a primary focus on two key tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Consequently, this article presents an introductory exploration of document-level analysis, addressing the primary challenges, concerns, and existing solutions. Finally, the article presents publicly available annotated datasets that can facilitate further research in this area.
		\end{abstract}
		
		%%Graphical abstract
		%\begin{graphicalabstract}
		%\includegraphics{grabs}
		%\end{graphicalabstract}
		
		%%Research highlights
		% \begin{highlights}
			% \item Research highlight 1
			% \item Research highlight 2
			% \end{highlights}
		
		\begin{keyword}
			Natural Language Processing \sep Long Document \sep Document Classification \sep Document Summarization \sep Sentiment Analysis \sep Deep Neural Networks
		\end{keyword}
		
	\end{frontmatter}
	
	\section{Introduction}
	Understanding written text has always drawn significant interest within the Artificial Intelligence (AI) community. Nowadays, it also enjoys increasingly many commercial applications: successfully parsing and analyzing texts expressed in natural language is crucial for a variety of practical tasks traditionally performed by humans, which require the extraction of sentiments, meaning or themes. Books \cite{worsham}, academic papers \cite{gales}, technical manuals \cite{nabizadeh}, news articles \cite{shuo}, legal documents \cite{merchant} and many other types of long texts can be the target of such analysis. Common application domains for existing, real-world practical systems include the automated processing of legal, medical, scientific or journalistic documents.
	
	Natural Language Processing (NLP) is the field of AI dedicated to developing algorithms for the semantic understanding of written and spoken language. NLP methods can be differentiated by the level of granularity they operate on. \textit{Sentence-level} NLP examines individual sentences and their structure, grammar, and meaning. This type of analysis is useful for tasks such as sentiment analysis or named entity recognition, which can be performed on a sentence-by-sentence basis \cite{borgir}. \textit{Paragraph-level} NLP analysis involves examining the larger context in which sentences are used. This type of analysis can help identify the topic or theme of a paragraph, as well as the relationships between sentences within the paragraph and is usually used as an intermediate stage between sentence-level and document-level analysis \cite{andrew} \cite{guo} \cite{jiwei}. \textit{Document-level} NLP analysis involves analyzing an entire document, such as a book, article, or email. This type of analysis can provide insights into the overall content, sentiment, and style of the document \cite{wei}\cite{timothy}. Document-level analysis can also involve tasks such as document classification or summarization, which require understanding the content of the entire document. This paper focuses exclusively on document-level analysis, or long document NLP, for texts that are longer than just a few paragraphs.
	
	Similarly to the more common case of short text analysis, long document NLP has been revolutionized during the past decade by Deep Neural Networks (DNNs), which greatly surpassed traditional statistical and machine learning approaches in accuracy and abilities. Nevertheless, even DNNs face severe challenges when analyzing long documents, due to a higher chance of ambiguities, varying context, potentially lower coherence, etc. \cite{hold}. Despite such limitations though, long document NLP has already become very important in the industry. It can be used to extract medical categories from electronic health records, enabling better patient care and treatment plans \cite{vithya}, or for automatically classifying lengthy legal documents \cite{lulu} \cite{bambroo}. In fact, the legal domain is currently one of the major application areas of long document NLP, with relevant algorithms exploited for desk research, electronic discovery, contract review, document automation, and even legal advice \cite{dale}.
	
	This survey focuses on two key NLP tasks that present peculiarities for the long document case: \textit{document classification} and \textit{document summarization}, The first one involves categorizing entire documents into predefined classes, based on their content. This enables efficient organization and retrieval of information. The second one aims to generate concise and coherent summaries of longer documents. It involves distilling the key information and main ideas from a document, while preserving its meaning. \textit{Sentiment analysis} is also investigated as a particular variant of document classification. The task consists in determining the sentiment or emotional tone expressed in a piece of text, such as positive, negative, or neutral. It involves analyzing subjective opinions, attitudes, and emotions expressed in reviews, social media posts, or customer feedback.
	
	Before the rise of DNNs, traditional machine learning methods were typically used for executing NLP tasks. For instance, Latent Dirichlet Allocation (LDA) \cite{dirichet} is a popular unsupervised learning method that categorizes documents into $k$ topics, according to their \textit{tokens}. Tokens in this case would be individual words, but for other algorithms may be sentences, expressions, or subwords \cite{tokens}. The concept of tokens serves as a building block in defining more complex textual structures, such as \textit{n-grams} \cite{ngrams}. These are continuous sequences of $n$ tokens, which can be exploited by NLP algorithms to capture the context and/or the sequential dependencies within text. For a comprehensive coverage of relevant traditional approaches, the reader is referred to \cite{korde}.
	
	Despite the presence of review papers discussing the development, methodology and applications of NLP tasks in general \cite{kowasari} \cite{torfi} \cite{chai}, or in specific thematic fields \cite{nielker} \cite{qian} \cite{gupta} \cite{hussein}, there are currently no survey/review articles discussing and aggregating recent research on long document NLP using DNNs. To remedy such gaps in existing literature, this article specifically focuses on document-level analysis for long texts. It overviews the relevant neural building blocks and systematically reviews existing solutions to three main NLP tasks for long documents (classification, summarization, sentiment analysis). It then identifies current challenges, discussing how they impact long document NLP and presents existing ways to circumvent them. The methods and algorithms that have been selected to be presented are ones especially tackling the peculiarities of long documents; not text classification, summarization or sentiment analysis in general. Overall, the goal of this article is two-fold: i) to make the barrier of entry for this relatively young section of active research more accessible, and ii) to aggregate common issues and solutions across various document-level long text NLP tasks, thus encouraging cross-pollination of research and ideas.
	
	The remainder of this article is organized in the following manner. Section \ref{sec::existingReviews} details previous recent surveys/reviews that overlap with this article, highlighting the main differences. Section \ref{sec::DNNs} briefly overviews basic to state-of-the-art neural architectures and building blocks for NLP. Sections \ref{sec::Classification}, \ref{sec::Summarization} and \ref{sec::Sentiment} detail the challenges and proposed solutions to the document classification, summarization and sentiment analysis tasks, respectively, emphasizing methods designed specifically for long documents. To better focus this overview, these Sections only examine dedicated algorithms and pretraining-finetuning approaches. Zero/few-shot prompting alternatives based on generic pretrained Large Language Models (LLMs) are not covered, since they fall outside the scope of the article. Section \ref{sec::Datasets} presents publicly available, annotated long document datasets, which can be utilized for relevant research. Section \ref{sec::Summary} summarizes and organizes the above methods and challenges, discusses current and future trends and identifies open issues. Finally, Section \ref{sec::Conclusions} draws conclusions from the preceding discussion, regarding the current state and future of long document NLP.
	
	\section{Relevant previous reviews}
	\label{sec::existingReviews}
	
	\begin{table}[H]
		\centering
		\begin{tabular}
			{ |p{4cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{2cm}|}
			\hline
			\textbf{Review}&\cite{kowasari}&\cite{wagh}&\cite{koh}&\cite{omori}&\cite{poon}&This article\\
			\hline
			Document Classification&\checkmark&\checkmark&\xmark&\xmark&\xmark&\checkmark\\
			\hline
			Document Summarization&\xmark&\xmark&\checkmark&\xmark&\xmark&\checkmark\\
			\hline
			Sentiment Analysis&\xmark&\xmark&\xmark&\checkmark&\checkmark&\checkmark\\
			\hline
			Relevant Neural Networks&\checkmark&\checkmark&\xmark&\xmark&\checkmark&\checkmark\\
			\hline
			Long Texts&\xmark&\checkmark&\checkmark&\checkmark&\xmark&\checkmark\\
			\hline
			Issue Discussion&\xmark&\checkmark&\checkmark&\xmark&\xmark&\checkmark\\
			\hline
			Issue Aggregation&\xmark&\xmark&\xmark&\xmark&\xmark&\checkmark\\
			\hline
			Long Document Datasets&\xmark&\xmark&\checkmark&\xmark&\xmark&\checkmark\\
			\hline
		\end{tabular}
		\caption{Comparisons between notable existing reviews and this article.}
		\label{tab::review_table}
	\end{table}
	
	Document classification has been covered extensively by \cite{kowasari}, where the terminology, basic concepts, metrics, preprocessing strategies and current learning approaches are covered. In \cite{wagh}, this analysis is extended to long documents and traditional machine learning models are compared with modern DNNs. Document summarization, by its very nature, is closely tied with the challenges of long documents. In \cite{koh}, a thorough survey of state-of-the-art models, datasets and metrics is offered. Sentiment analysis, also known as \textit{opinion mining}, typically concerns short documents, such as reviews, comments and social media posts. However, a review of modern approaches to a specific subtask of sentiment analysis is conducted in \cite{omori}, where the goal is to analyze entire literary works with respect to their story and emotions. More generally, \cite{poon} provides an overview of sentiment analysis algorithms and current DNN solutions.
	
	While these reviews overlap with this article, there is no previous effort for aggregating and comparing the issues and challenges posed by long documents across multiple NLP tasks, while focusing exclusively on modern DNNs. Thus, this article attempts to expose common issues and encourage the sharing of tried-and-tested  solutions across tasks. Additionally, it covers long document classification, which seems to be absent from relevant literature so far.
	
	A simplified comparison between the above-mentioned reviews and this article can be found in Table \ref{tab::review_table}. The row ``Issue Discussion" indicates that the paper specifically discusses issues with long documents, while the row ``Issue Aggregation" indicates that issues are aggregated and compared across different NLP tasks.
	
	\section{Deep Neural Networks for Long Document Analysis}
	\label{sec::DNNs}
	NLP tasks place specific demands on DNNs: for instance, a DNN model needs to uncover on its own the structure, context and the meaning of a single word. This calls for specialized architectures whose properties allow the DNN to navigate through the complexities of human-generated natural text. Long documents exacerbate these challenges, since: i) the context, the tone and the theme may change repeatedly as the text progresses, and ii) the DNN may be required to correlate semantic cues which are far apart to each other within the text, in order to successfully execute the desired task.
	
	Since the state-of-the-art in NLP has been revolving around DNNs for the last several years, this Section briefly presents the most widespread and effective neural architectures used for NLP. The complexities and the individual variations of each architecture are out of the scope of this article, since its focus is on DNN-enabled long document NLP and not on DNNs themselves. All architectures presented in this Section are typically trained with variants of error back-propagation and stochastic gradient descent \cite{rumelhart} \cite{jordan}, but only the inference stage is described here.
	
	The following presentation is split into two Subsections. Initially, generic neural architectures are briefly described for reference purposes (MLP, CNN, RNN/LSTM, Transformer). Subsequently, specific NLP-oriented neural architectures commonly used for analysing texts are detailed.
	
	
	\subsection{General Neural Architectures}
	
	\subsubsection{MultiLayer Perceptrons}
	The most common and traditional form of neural networks utilized for NLP is the well-known MultiLayer Perceptron (MLP). It is a relatively simple feed-forward architecture that consists of an input layer, one or more hidden layers and an output layer. An MLP with more than a few hidden layers typically qualifies as a deep architecture, although there is no precise threshold for discriminating strictly between shallow and deep models. Each hidden neuron acts as a postsynaptic neuron across different weighted synapses, separately connecting it with each of the neurons of the previous layer; thus, a main property of MLPs is that their layers are \textit{fully connected}. The scalar output of each neuron is transformed by an activation function, before being transmitted as input to the neurons of the next layer. The input layer typically receives as its input a fixed-size vector and, therefore, has as many neurons as the fixed and known dimensionality of the supported data points. Î¤he majority of popular feed-forward neural architectures follow this basic concept at a coarse level and have essentially evolved from MLP. Notably, significant developments in neural network algorithms emerged gradually through attempts to solve the well-known \textit{vanishing gradient} problem: an issue of back-propagation-based training, which originally limited the realistically achievable number of hidden layers in MLPs (depth).
	
	Eq. (\ref{eq::MLP1}) below succinctly captures the inference-stage processing of the $l$-th MLP layer:
	
	\begin{equation}
		\mathbf{o}^l = f^l(\mathbf{W}^l\mathbf{o}^{l-1} + \mathbf{b}^l),
		\label{eq::MLP1}
	\end{equation}
	
	\noindent where $f^l$ and $d$ are the activation function and the number of neurons of the $l$-th layer, respectively. $c$ is the number of neurons of the previous $(l-1)$-th layer, while $\mathbf{o}^l \in \mathbb{R}^{d}$ and $\mathbf{o}^{l-1} \in \mathbb{R}^{c}$ are the activation vectors containing the final outputs of the $l$-th and the $(l-1)$-th layer, respectively. $\mathbf{W}^l \in \mathbb{R}^{d \times c}$ and $\mathbf{b}^l \in \mathbb{R}^{d}$ contain the learnt parameters of the $l$-th layer: the synaptic weights and the neuronal biases, respectively. At the first hidden layer, $\mathbf{o}^{l-1}$ is the input data point. Note that the activation function is separately applied to each scalar component of its vector argument, thus generating a vector output.
	
	\subsubsection{Convolutional Neural Networks}
	A Convolutional Neural Network (CNN) \cite{lenet}\cite{alexnet} is a feed-forward DNN which differs from an MLP by introducing \textit{convolutional} and \textit{pooling layers}, along with the traditional fully connected layers. A CNN architecture can be designed with these three types of layers interleaved as desired. A convolutional layer differs from a fully connected one in two main respects: a) it replaces full connectivity between consecutive layers with a set of learnable convolution operators, where the synaptic weights act as convolutional coefficients, b) it replaces the linear/vector arrangement of fully connected layers with a third-order tensor structure.
	
	Each 2D channel of such a tensor essentially encodes in its synaptic weights one fixed-size 2D convolution filter, applied to the activations of each of the output channels of the previous layer. During inference, each such filter applies a different learnt 2D convolution kernel to each of the input channels and the results are summed. Thus, the current layer's width (number of channels) equals the number of different filters it learns during training, but each filter contains as many 2D kernels as the previous layer's width. Pooling layers are typically exploited for subsampling the output of an immediately preceding convolutional layer. Overall, during inference, each successive layer transforms the input features it receives from the previous layer into a more refined, semantically richer representation of the raw input data that have been fed to the input layer. At each convolutional layer, each channel extracts a different type of features. Due to the nature of the convolution operator, the captured features are mostly spatially local in the early layers, until successive pooling and convolution operations eventually increase the neuronal receptive field in the late layers, due to repeated downsampling. Finally, certain CNN innovations introduced to facilitate training deeper architectures, such as skip connections \cite{huynh} or the ReLU activation function \cite{alexnet}, are not really tied to CNNs and have in fact been applied to various architectures.
	
	The above description refers to the most common form of CNNs, also known as \textit{2D CNNs}, since they were originally designed to recognize patterns in images. In this case, each learnt filter can be thought of as a small grid of ``pixels" systematically shifting spatially across each input channel, ``looking" for specific patterns. However, variants with 1D or 3D convolutions have also emerged, mostly for analyzing 1D timeseries/sequences or videos, respectively. Thus, beyond their use in computer vision \cite{scuttle} \cite{yang} \cite{patrick}, CNNs have been successfully exploited in NLP as well \cite{dauphin}. Due to their tendency to compute local features in sequential data, they have shown exceptional performance in automatically extracting n-grams of neighbouring words, after proper training.
	
	Eq. (\ref{eq::CNN1}) below succinctly captures the inference-stage processing taking place inside a 2D convolutional layer of $K$ channels:
	\begin{equation}
		\mathbf{O}_i^l = f^l(\sum_{j=1}^M \left(\mathbf{W}_{ij}^l * \mathbf{O}_j^{l-1}\right) + b_i^l),
		\label{eq::CNN1}
	\end{equation}
	
	\noindent where $*$ is the 2D convolution operator, $M$ is the number of channels in the previous $(l-1)$-th layer and $\mathbf{O}_i^l$ is the $i$-th output feature map of the $l$-th layer, with $1 \leq i \leq K$. The $j$-th feature map of the previous $(l-1)$-th layer is denoted by $\mathbf{O}_j^{l-1}$, with $1 \leq j \leq M$. The size $q \times q$ of the convolution window along the 2 spatial dimensions is a manually chosen hyperparameter of the $l$-th layer; the most common choice is $q = 3$. Thus, $\mathbf{W}_{ij}^l \in \mathbb{N}^{q \times q}$ is the $i$-th learnt 2D convolution kernel of the $l$-th layer for the $j$-th channel of the $(l-1)$-th layer. Finally, $b_i^l$ is the $i$-th learnt bias parameter of the $l$-th layer. At the first convolutional layer, $\mathbf{O}^{l-1}$  typically is the input tensor. For instance, in computer vision this can be an RGB image ($M = 3$), while in NLP it can be a matrix containing vector representations of a sentence's tokens as its rows ($M = 1$).
	
	\subsubsection{Recurrent Neural Networks}
	Recurrent Neural Networks (RNN) are extensions of simple MLPs with limited internal memory, thus they are suitable for analyzing sequential data points instead of fixed-size vector inputs. In the case of NLP, this memory allows an RNN to access the local context of each word in order to make more informed predictions for the task at hand.
	
	A typical RNN is similar to an ordinary feed-forward MLP with one hidden layer, except that each forward pass during the inference stage takes place in $K$ discrete, sequential time steps. At each step, each hidden neuron receives an input vector and calculates a corresponding activation value, just like a classical artificial neuron. But, in addition, the hidden activations of each time step are retained until the next one in the form of an internal state vector with a dimensionality equal to the number of hidden neurons. This way, the activation of the $i$-th hidden neuron at the $t$-th instance is given by an activation function (e.g., sigmoid) which transforms the sum of the weighted input dot product and a weighted state dot product, where the state vector was generated during the previous, $(t-1)$-th instant. This new activation of the $i$-th hidden neuron at the $t$-th time step will constitute an entry of the stored state vector at the next time step ($t+1$). Finally, an ordinary linear output layer accepts the hidden activations of time $t$ and produces the final output vector of the network as a function of an output weights matrix. The number of discrete time steps $K$ is arbitrarily parameterizable and does not affect the number of model parameters: both the topology and the weights of the network remain constant throughout, since the the only time-varying quantities are the input, state (i.e., the vector of hidden neuron activations) and output vectors. Of course, during inference the RNN cannot identify long-term dependencies across input sequence elements that go beyond the value of $K$ that was employed during training.
	
	The crucial difference of dividing each neuron's computations into $K$ consecutive time steps gives the RNN its ability to process time series and sequences, even giving sequences as its output. The forward pass in RNN is implemented by ``unfolding" the entire network in time so that it resembles $K$ MLPs that are executed sequentially. Training is implemented by an inverse ``error back-propagation through time" (BPTT) pass, in which normal derivatives of the cost function with respect to the parameters of each neuron are calculated, taking into account time unfolding. Obviously, an RNN has many more parameters than an MLP of similar complexity (number of hidden layers and neurons per layer), due to the presence of the state weight vector of each hidden neuron. Its computational and memory complexity is linear in $K$. On the other hand, an RNN is theoretically much more computationally powerful than an MLP: while the latter has the property of universal function approximation, the RNN is a Universal Turing Machine and is able to simulate any algorithm \cite{siegelman}. In practice, its main advantage is the ability to learn from sequences: at the $i$-th time step, the $i$-th element of the sequence (e.g., a vector representation of the $i$-th token of the analyzed document in long document NLP) is given as input to the input layer, and the network implicitly associates it with the inputs of the previous time steps, through the stored internal state and the values that the state weights vector has taken during training, in order to finally produce a correct prediction at the output. The RNN uses the internal state and the current input to compute a new internal state and a new output at each time step. Its main drawback is that, due to chaining the cost derivatives with respect to the hidden layer parameters across multiple time steps during error back-propagation, as if computing across multiple layers of different depth, the derivatives tend to become zero for the initial time steps and, thus, make gradient descent training very difficult for a large $K$. This is an issue of vanishing gradients almost identical to the one found in deep MLPs and CNNs, only here it is a problem spanning over time steps, rather than layers/depth.
	
	Eqs. (\ref{eq::RNN1})-(\ref{eq::RNN2}) below succinctly capture the inference-stage processing taking place inside a typical RNN during the $t$-th time step:
	\begin{equation}
		\mathbf{s}(t+1) = f(\mathbf{W}_{xs} \mathbf{x}(t) + \mathbf{W}_{ss} \mathbf{s}(t) + \mathbf{b}_x)
		\label{eq::RNN1}
	\end{equation}
	\noindent and
	\begin{equation}
		\mathbf{y}(t) = \mathbf{W}_{os} \mathbf{s}(t+1) + \mathbf{b}_o,
		\label{eq::RNN2}
	\end{equation}
	\noindent where $\mathbf{x}(t)$ and $\mathbf{y}(t)$ are the current input and output vectors, respectively, while $\mathbf{s}(t)$ is the current state vector. $\mathbf{W}_{xs}$ and $\mathbf{W}_{ss}$ are the learnt weight matrices of the hidden layer, while $\mathbf{W}_{os}$ is the weight matrix of the output layer. $\mathbf{b}_x$/$\mathbf{b}_o$ is the learnt bias vector of the hidden/output layer, respectively, and $f$ is the activation function.
	
	\subsubsection{Long Short-Term Memory Networks}
	The issue of gradient vanishing, especially pronounced when training RNNs for a large $K$, limits the ability of the trained model to capture long-term dependencies between entries that are distant to each other in the input sequence. Long Short-Term Memory networks (LSTMs) are an improvement over simple RNNs, specifically aiming to address this problem \cite{sepp}.
	
	Essentially, this is implemented by a more complex internal neuronal architecture, where memory cells and gates controlled by additional learnable parameters jointly determine how much information from the previous time steps should be passed to each current time step, during inference. The gates inside each neuron enable the network to selectively control the flow of information across time and choose what to retain or discard. The resulting modifications to the back-propagation equations facilitate an unchanged gradient flow during training, thus significantly ameliorating the vanishing gradient issue \cite{calin}. The optimal parameter values are identified during the training process, to optimize the performance of the overall network.
	
	Multiple LSTM layers, of multiple neurons each, are typically sequentially stacked to form deep architectures, able to learn to extract semantically rich features from raw input sequences. All of the additional elements introduced by LSTMs are completely independent between different LSTM neurons in the same layer, as well as between different layers in a stacked LSTM architecture.
	
	The operation of an LSTM neuron (also called \textit{LSTM cell}) at the $t$-th time step of the inference stage is briefly described below. The cell state $\mathbf{c}_{t-1}$ is a vector containing the scalar stored state values from each of this layer's neurons of the previous time step. $\mathbf{h}_{t-1}$ is a vector containing the layer's outputs of the previous time step. Both $\mathbf{c}_{t-1}$ and $\mathbf{h}_{t-1}$ have a dimensionality equal to the number of neurons in the layer. $\mathbf{x}_{t}$ is the current input vector. The output of the forget gate $f_t \in (0, 1)$ is a weighting multiplier determining what percentage of the received cell state should be discarded. The output of the input gate $i_t \in (0, 1)$ is a weighting multiplier determining what percentage of the regular RNN neuron's output for this time step should be retained. The two quantities are then summed to construct this neuron's respective scalar entry of the current cell state for the $t$-th time step ($\mathbf{c}_{t}$). Finally, the output of the output gate $o_t \in (0, 1)$ is a weighting multiplier determining whether the current state value will be allowed to influence the neuron's current final activation $h_t$.
	
	Many variations of this basic algorithm have been proposed over the years. One particularly important variant is the \textit{Bidirectional LSTM} \cite{biLSTM}. In this architecture, each regular, forward-flow LSTM layer is accompanied by a reverse-flow LSTM layer which receives and processes the input sequence with its tokens reversed in order. Thus, each hidden layer receives input from both the preceding forward and the preceding reverse layer. Overall, this approach allows the network to capture both past and future context information for each sequence timestep.
	
	Eqs. (\ref{eq::LSTM1})-(\ref{eq::LSTM6}) below succinctly capture the inference-stage processing taking place inside an entire LSTM layer during the $t$-th time step:
	\begin{equation}
		\mathbf{i}(t) = \sigma(\mathbf{W}_{xi}\mathbf{x}(t) + \mathbf{W}_{si}\mathbf{h}(t-1) + \mathbf{b}_i),
		\label{eq::LSTM1}
	\end{equation}
	\begin{equation}
		\mathbf{f}(t) = \sigma(\mathbf{W}_{xf}\mathbf{x}(t) + \mathbf{W}_{sf}\mathbf{h}(t-1) + \mathbf{b}_f),
		\label{eq::LSTM2}
	\end{equation}
	\begin{equation}
		\mathbf{o}(t) = \sigma(\mathbf{W}_{xo}\mathbf{x}(t) + \mathbf{W}_{so}\mathbf{h}(t-1) + \mathbf{b}_o),
		\label{eq::LSTM3}
	\end{equation}
	\begin{equation}
		\mathbf{g}(t) = tanh(\mathbf{W}_{xc}\mathbf{x}(t) + \mathbf{W}_{sc}\mathbf{h}(t-1) + \mathbf{b}_c),
		\label{eq::LSTM4}
	\end{equation}
	\begin{equation}
		\mathbf{c}(t) = \mathbf{f}(t) \otimes \mathbf{c}(t-1) + \mathbf{i}(t) \otimes \mathbf{g}(t),
		\label{eq::LSTM5}
	\end{equation}
	\begin{equation}
		\mathbf{h}(t) = \mathbf{o}(t) \otimes tanh(\mathbf{c}(t)),
		\label{eq::LSTM6}
	\end{equation}
	\noindent where $\otimes$ is the Hadamard product, $\sigma$ is the sigmoid activation function, $tanh$ is the hyperbolic tangent activation function, $\mathbf{f}$ is the forget gate vector, $\mathbf{i}$ is the input gate vector, $\mathbf{o}$ is the output gate vector and $\mathbf{c}$ is the cell state, while $\mathbf{x}$ and $\mathbf{h}$ are the input and output vectors, respectively. Figure \ref{fig::LSTM} depicts schematically an LSTM cell.
	
	\begin{figure}
		\centering
		\includesvg[width=10cm]{images/lstm.svg}
		\caption{A standard LSTM cell.}
		\label{fig::LSTM}
	\end{figure}
	
	The computational and memory demands of LSTM are higher than the traditional RNN's, resulting in a large number of faster variants that emerged over the years. The most popular LSTM variant is Gated Recurrent Unit (GRU), with a slightly simplified architecture and an always-on output gate \cite{Cho2014}.
	
	\subsubsection{Encoder-Decoder Architectures and the Attention Mechanism}
	Any RNN/LSTM of one or more layers which unfolds across $K$ time steps accepts a sequence of $K$ input vectors in order to finally produce a corresponding sequence of $K$ output vectors. However, in a wide range of practical tasks the temporal lengths of the input/output sequences are not identical. If the length of one is $K$ while the other's is 1, the problem is trivially solved by unfolding the network across $K$ time steps and considering only the final activation of the $K$-th time step as the network prediction. However, \textit{sequence-to-sequence} mapping tasks between sequences of different lengths other than 1 are more complicated.
	
	Typically, RNNs are adapted to solve such problems (e.g., machine translation) using a mixed Encoder-Decoder scheme. The input sequence is fed to a recurrent coding network which processes it over $K$ time instants (where $K$ is, e.g., the length of the input sequence), but only the final network output from the $K$-th time step is stored. Thus, it constitutes a coded vector (of fixed dimensionality equal to the number of the Encoder's output neurons, independent of $K$) representing the entire input sequence. This vector is then fed to a recurrent decoding network unfolding across $L$ time steps ($L$ can be arbitrary and not necessarily equal to $K$ or 1). At the first time step of the Decoder's inference stage it is given as input the coded vector. During the following time steps, its processing is based only on the stored internal state. The $L$ Decoder outputs constitute the final output sequence. The overall network is trained uniformly by BPTT.
	
	As an improvement over this basic idea, the \textit{neural attention mechanism} was introduced in \cite{bahdanau} to allow the Decoder of a mixed Encoder-Decoder RNN architecture to consider all the outputs of the Encoder, and not just that of the last encoding time step, with a different weighting factor. These weighting coefficients, known as \textit{attention weights}, are given by an attention distribution that the Decoder produces at each time step of its inference stage, in order to read selectively and at will the Encoder's outputs according to its own current internal state. This is achieved by the attention mechanism: a differentiable and continuous way to access a sequence of stored discrete \textit{key vectors}. Thus, the mechanism allows a neural network to learn suitable access patterns on its own from proper training on a dataset, through usual back-propagation and gradient descent optimization.
	
	Attention operates by generating a continuous probability distribution defined over the entries of the key sequence. This distribution is produced in the following way. First, the network generates a \textit{query vector} that essentially needs to be searched for in the key sequence. Then, the dot product of that query with each key vector is separately computed, with the ordered results essentially forming a new sequence of cosine similarities between the query and each key vector. This similarity sequence is converted to a probability distribution by passing it through a softmax function. Then, a weighted sum of all key vectors is generated, using the attention weights as coefficients. Optionally, this weighted sum can be computed on a different set of \textit{value vectors}, instead of the key vectors through which the attention weights were estimated. Queries, vectors and keys can be derived by the DNN during inference through projecting given vectors (linearly or non-linearly) via learnt separate transformations. These operations are differentiable and their parameters are learnt while training the overall network with a typical, task-specific cost function. The process of constructing the weighted vector sum using the attention weights is referred to as \textit{the query attending to the keys and/or values}.
	
	In the case of Encoder-Decoder RNN architectures, the key/value sequence is the temporal sequence of Encoder internal states/outputs across $K$ time steps, respectively, while the $L$ separate queries are the Decoder's internal states at each time step of its inference stage. Alternatively, the key and the value sequence may coincide. This setup allows the Decoder to read at each time step an aggregate \textit{context vector} before it generates its current output: such a context is a parameterized summary of all $K$ Encoder outputs, adjusted according to the Decoder's own current state.
	
	\subsubsection{Transformers}
	The Transformer architecture \cite{ilia} was an attempt to replace RNNs for sequence-to-sequence mapping tasks, while simultaneously maintaining the mixed Encoder-Decoder scheme. Both the Encoder and the Decoder consist of $N$ consecutive macrolayers each. The Decoder operates at discrete time steps, with its final layer outputting at the end of each step the next element of the requested output sequence. Its first layer's input at the beginning of each step is the output of its final layer from the previous step, i.e., the Decoder is \textit{autoregressive}. However, the Encoder operates at a single pass, processing the entire input sequence at once. The first Encoder layer receives as its input a list of at most $K$ vectors, i.e., an ordered list of all elements/tokens in the input sequence. Each vector in this sequence is the sum of a suitable representation of the corresponding token (e.g., a sparse one-hot encoding of a word from the supported vocabulary) and a ``positional encoding", i.e., a dense vector representation of the index of the respective input token within the overall sequence. Each of the subsequent Encoder layers receives as its input a list of $K$ corresponding, transformed vectors, i.e., the outputs of the previous Encoder layer.
	
	Each layer of the Encoder or the Decoder is composed of two consecutively placed sublayers: a \textit{self-attention} sublayer and a succeeding small MLP. The self-attention mechanism enables the layer to process its input sequence in parallel, while the following fully connected layers transform the output of the self-attention process. However, a self-attention sublayer is itself composed of $M$ parallel, independent self-attention heads. Each head receives a linearly transformed version of each input vector as a query and two linearly transformed versions of the overall input sequence as a key and as a value sequence. Within a head, a common key/value sequence representation is utilized for all queries. The transformations are performed by multiplying each input vector with suitable weight matrices, separately learnt per self-attention head as model parameters. Thus, although all heads of one layer separately receive the same input sequence as a list of queries, of keys and of values, this sequence is differently transformed per head befored being fed to them. The output of each self-attention head is one new vector representation per input token, which inherently incorporates appropriate context information from the entire sequence. The outputs of all heads are concatenated and subsequently linearly transformed, using an additional learnt weights matrix, before being fed to the succeeding MLP.
	
	The Decoder has a structure similar to the Encoder, but includes an additional, regular attention mechanism within each of its layers, for also attending to the Encoder's final outputs. However, the use of both an Encoder and a Decoder is not strictly necessary in tasks that do not involve sequence-to-sequence mapping with input/output sequences of different lengths. The original Transformer's Encoder-Decoder architecture is depicted in Figure \ref{fig::TransformerVanilla}.
	
	
	\begin{figure}
		\centering
		\includesvg[width=15cm]{images/encoder_decoder.svg}
		\caption{The original Transformer Encoder-Decoder architecture.}
		\label{fig::TransformerVanilla}
	\end{figure}
	
	Overall, the simultaneous processing from multiple, parallel self-attention heads and the resulting lack of recurrence in the Encoder render the Transformer able to achieve faster training and inference times, compared to previous RNNs/LSTMs that process the input sequentially. Additionally, the learnable self-attention mechanism allows the Transformer to easily, selectively and adaptively capture contextual information and long-term dependencies from the entire input sequence, when analyzing each individual token during inference. Thus, Transformer layers effectively have a global receptive field from the get-go. Finally, the use of multiple self-attention heads per layer allows each layer to simultaneously compute multiple different representations from its input sequence, thus capturing different features at once.
	
	Eq. (\ref{eq::Att1}) below succinctly captures the inference-stage processing of a self-attention head:
	\begin{equation}
		\label{eq::Att1}
		Attention(\textbf{Q},\textbf{K},\textbf{V}) = softmax(\frac{\textbf{QK}^{T}}{\sqrt{d_k}})\textbf{V},
	\end{equation}
	\noindent where $\textbf{Q} \in \mathbb{R}^{N_q \times d_k}$ are the $N_q$ queries, $\textbf{K} \in \mathbb{R}^{N_k \times d_k}$ are the $N_k$ keys and $\textbf{V} \in \mathbb{R}^{N_k \times d_v}$ are the $N_k$ values. Each query vector and each key vector has a dimension of $d_k$, while each value vector has a dimension of $d_v$.
	
	Multihead self-attention can be formulated as:
	\begin{equation}
		Multihead(\textbf{Q},\textbf{K},\textbf{V}) = [\textbf{h}_1;...;\textbf{h}_{M}]\textbf{W}^O,
	\end{equation}
	\noindent where
	\begin{equation}
		\textbf{h}_i = Attention(\textbf{Q}\textbf{W}^{Q}_i, \textbf{K}\textbf{W}^{K}_i, \textbf{V}\textbf{W}^{V}_i).
	\end{equation}
	\noindent In this formulation, $\textbf{W}^{Q}_i \in \mathbb{R}^{d_a \times d_k}$, $\textbf{W}^{K}_i \in \mathbb{R}^{d_a \times d_k}$, $\textbf{W}^{V}_i \in \mathbb{R}^{d_a \times d_v}$, $\textbf{W}^{O}_i \in \mathbb{R}^{M d_v \times d_a}$ are learnt parameter matrices, $M$ is the number of heads, $d_k=d_v=\frac{d_a}{M}$, and the operator $[;]$ implies concatenation.
	
	Note that the softmax function in Eq. (\ref{eq::Att1}) is separately applied to each row of its argument, resulting in a matrix output. Each row of each $\textbf{h}_i$ matrix is a convex combination of the rows of $\textbf{V}\textbf{W}^{V}_i$, with the respective row of the output of the softmax providing the corresponding weighting coefficients. Scaling by $\sqrt{d_k}$ in Eq. (\ref{eq::Att1}) serves to ensure training stability.
	
	\subsection{Neural Building Blocks for Long Document NLP}
	\subsubsection{Word Embeddings}
	Modern NLP algorithms typically require a very important preprocessing phase: during this phase, each input token is transformed into a semantically meaningful, fixed-size, dense vector, which is typically called \textit{word embedding}. The resulting sequence of word embeddings is the actual input to the main DNN that executes the desired NLP task \cite{mikolov}. Such word embeddings are utilized both at the training and at the inference stage. One trivial way to achieve this is to simply append an initial \textit{embedding layer} at the employed DNN architecture, which is subsequently trained end-to-end. Thus, the embedding layer receives the raw tokens as its input, e.g., as sparse, one-hot encoded vectors, and transforms them to a semantically meaningful, dense vector representation. However, the most widespread approach is to exploit pretrained, separate, dedicated \textit{embedding neural networks} and utilize their outputs as dense word embedding vectors.
	
	One of the most historically significant word embedding neural networks was Google's \textit{Word2Vec} \cite{mikolov}. It is a shallow MLP with one hidden layer, able to compute a useful representation of a word token in the form of a real, dense, fixed-size vector with a dimensionality equal to $H$, i.e., the number of hidden neurons in the model. Word2Vec\cite{mikolov} representations/embeddings have rich semantic content: words with similar or related meaning are mapped to vectors that are approximately parallel in the representation/embedding space (i.e., they have high cosine similarity), while applying typical vector operations to such representations is semantically meaningful (e.g., corresponds to semantic analogies between the represented words) \cite{jurafsky}.
	
	The Word2Vec MLP receives as its input a non-semantic (e.g., one-hot) vector encoding of a word with a dimensionality of $L$ (equal to the supported vocabulary size). It has been pretrained according to the following self-supervised objective. First, for each occurrence of each significant word (e.g., nouns, verbs, adjectives, proper nouns) in the available training dataset/text corpus, the $D$ words immediately before and the $D$ words immediately after the one in question are selected. Each such mapping from a word to its $2D$ neighboring ones in the context of each occurrence is exploited as a training pattern/label pair, using typical error back-propagation and gradient descent. Thus, the co-occurrence of words within a given context is implicitly utilized to learn the relationships between these words.
	
	The input/hidden/output layer has $L$/$H$/$2DL$ neurons, respectively. After the model has been trained, the output layer is discarded and the remaining network can be exploited to produce a semantically rich representation of each input word: this is a dense, $H$-dimensional real vector. Obviously, a very large data set is required to properly train such a model, and in Word2Vec's case that was GoogleNews (100 billion words).
	
	Other word embedding algorithms followed-up and improved upon the Word2Vec concept, such as GloVE \cite{glove}. However, the next significant milestone was the development of context-sensitive word embedding networks, which were able to generate different vector representations for a single word depending on its context, i.e., its position in a sentence. ELMo \cite{elmo} was able to achieve this using an LSTM architecture, but it was Bidirectional Encoder Representations from Transformers (BERT) \cite{toutanova} which originally exploited the power of Transformers in order to significantly advance the relevant state-of-the-art. Such DNNs that learn to generate contextualized word embeddings, by being trained to predict the next word of an input sentence, are essentially \textit{language models}.
	
	\subsubsection{BERT}
	BERT is a bidirectional, deep Transformer Encoder, without an attached Decoder. It receives an input sequence consisting of numerical representations of a text's tokens/words (e.g., non-semantic indices to a supported vocabulary), to generate a corresponding sequence of refined, semantically encoded output vectors as this text's representation. Initially, the special token [CLS] is externally appended to the beginning of the overall input sequence, so that its output semantic representation will aggregate global contextual information about the entire textual sequence. Also, the special token [SEP] is inserted after each input sentence to separate consecutive sentences. The ordered token representations are then transformed by a preliminary, trainable embedding layer. Subsequently, a learnt dense ``segment embedding" vector and a learnt position embedding vector\footnote{This differs from the original Transformer's statically defined positional encoding vector.} are added to the vector representation of each input token. Finally, the first actual Transformer layer receives these ordered vector sums as its input sequence. The segment embedding of a token belonging to the $i$-th sentence simply indicates whether $i$ is odd or even\footnote{The baseline BERT supports only 2 sentences, but each of these is a segment of consecutive textual content and not an actual individual sentence in the linguistic sense.}.
	
	The output word embeddings have specialized semantic content in comparison to corresponding Word2Vec representations, i.e., adapted to the context of the specific input sentence. This is because the network processes a complete input text at each iteration of its training stage, taking into account all its words and their order at a single pass, through the self-attention mechanism. Thus, during inference, it processes each input word in the context of its bilaterally ordered phrasal contexts. Therefore, the same word in the context of different sentences, or even placed at a different position in an otherwise identical sentence, can be mapped to different embedding vectors by the pretrained network.
	
	More difficult objectives are employed for pretraining BERT, compared to simply predicting the previous and next words of the current sentence, as in Word2Vec. These objectives are tailored to the Transformer's special features. The most common goal is to predict a few randomly chosen masked words of a complete input sentence based on its remaining words. This is a ``Masked Language Model" (MLM) pretraining objective, inspired by the Cloze task \cite{taylor}, where a subset of the input's tokens are randomly masked, i.e., each one is replaced by a special [MASK] token, and the goal is to predict the original words based on the context. The MLM objective encourages the model to fuse the left and the right context and, thus, is particularly suited to deep bidirectional Transformers \cite{toutanova}. A complementary pretraining objective is to classify two jointly fed input sentences as either consecutive or non-consecutive ones (Next Sentence Prediction, NSP). This teaches BERT to understand longer-term dependencies across sentences. In both cases, after training the final prediction head or classification layers are discarded and only the trained Encoder is retained, to generate the word embeddings of input sentences.
	
	A shallow, pretrained Word2Vec model encodes a fixed 1-1 mapping of words to representations/embedding vectors and, thus, can be used in a variety of NLP tasks only for feature extraction at a preprocessing stage. In contrast, a pretrained deep BERT model can optionally be adapted itself to the desired downstream NLP task, by appending appropriate additional final neural layers (usually fully connected or recurrent) and performing minimal fine-tuning on the overall network. During inference, BERT must be given as input a complete sentence, so as to generate the most contextually appropriate embeddings of its words. Due to the fixed maximum sequence size accepted by Transformers, BERT is restricted to receiving at most 512 tokens as its input text. Finally, a pretrained BERT comes with a dictionary of words it has been trained to embed. Thus, if during inference it is found out that the current word is unknown, it is broken into subwords which are considered by the network as different, consecutive elements of the input sequence (\textit{subword tokenization}). In the case of compound words, such a subword is indeed potentially known, otherwise the token decomposition eventually reaches the level of the individual alphanumeric characters of the original word, which always belong to the known dictionary. The final representation of an input word can be derived as the vector average of the embeddings of its subwords. Thus, BERT can easily handle ``unsupported" out-of-vocabulary words. Figure \ref{fig::BERTInput} depicts how the input to BERT is constructed.
	
	% Examples of BERT models include BERTBASE (L=12, H=768, A=12, parameters=110M) and BERTLARGE (L=24, H=1024, A=16, parameters=340M) where $L$ = number of layers, $H$ = word embedding dimensionality, $A^{3}$ = number of attention heads. 
	
	\begin{figure}
		\centering
		\includesvg[width=16cm]{images/bert.svg}
		\caption{BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.}
		\label{fig::BERTInput}
	\end{figure}
	
	\subsubsection{ELECTRA}
	Various improvements over the original BERT have been proposed over the past few years. For instance, ``Robustly Optimized BERT pre-training Approach" (RoBERTa) \cite{roberta} aggregated several minor improvements in the training process and slightly modified the MLM pretraining objective, by dynamically changing the masking pattern applied to the training data.
	
	One of the most significant recent improvements over BERT is ``Efficiently Learning an Encoder that Classifies Token Replacements Accurately" (ELECTRA) \cite{karta}, which was also motivated by the shortcomings of the MLM self-supervised pretraining objective. The latter ``corrupts" the input sentence by replacing a small subset of its tokens with [MASK] and then training the model to reconstruct/predict the original ones. This requires a lot of training iterations/sample to be effective, since the task is essentially defined only over the masked tokens. Thus, ELECTRA proposed ``Replaced Token Detection" (RTD) as an alternative self-supervised pretraining task. In RTD, the model learns to distinguish real input tokens from plausible but synthetically generated replacements: a randomly selected subset of the input tokens are replaced with synthetic alternatives (typically the output of a small language model), instead of being masked. The pretraining task is defined over all input tokens, since each of them has to be classified as original or synthetic. Learning from all input tokens leads to a learning process which is both much faster and more efficient; thus, ceteris paribus, ELECTRA achieves increased performance in downstream tasks due to better contextual word embeddings. Figure \ref{fig::MLM-RTDTasks} graphically compares the MLM and the RTD tasks.
	
	\begin{figure}[H]
		\includesvg[width=12cm]{images/MLM.svg}
		\centering
		\caption{Graphical comparison between the MLM and the RTD tasks.}
		\label{fig::MLM-RTDTasks}
	\end{figure}
	
	\subsubsection{T5}
	T5 \cite{t5} is a renowned, general-use Transformer architecture for NLP. The modus operandi of T5 is to transform each desired NLP task, either discrimnative or generative, into a sequence-to-sequence mapping problem (text-to-text). Thus, it follows the standard Encoder-Decoder architectural paradigm, a design choice that allows it to easily handle input and output sequences of different sizes. This is unlike BERT, which is a bidirectional Encoder-only Transformer. Similarly to other language models, T5 is first pretrained on a large-scale corpus with a self-supervised objective, before being finetuned on the desired, typically supervised downstream task. Pretraining enables the model to learn general language patterns and representations, while downstream finetuning specializes it to task-specific nuances. Both training modes fall under the text-to-text format, therefore a common maximum likelihood loss function is employed regardless of the task.
	
	The text-to-text nature of the architecture allows T5 to be pretrained with a masked language modelling objective that uses targets composed of multiple consecutive words per each [MASK] token. Unlike BERT, this is done through random corruptions of text segments with varying masking ratios and segment sizes. A simplified example of the T5 self-supervised objective is depicted in Figure \ref{fig::T5objective}. The use of a complete Transformer Encoder-Decoder architecture, coupled with this fitting self-supervised pretraining task and a very large-scale pretraining dataset, jointly permit T5 to achieve remarkable performance across a variety of different language tasks (e.g., document summarization, sentiment analysis, etc.), after proper finetuning.
	
	\begin{figure}[H]
		\includesvg[width=12cm]{images/T5.svg}
		\centering
		\caption{The T5 pretraining objective.}
		\label{fig::T5objective}
	\end{figure}
	
	\subsubsection{BART}
	Similarly to T5, BART \cite{bart} is also a Transformer language model that follows a full Encoder-Decoder architecture and is pretrained with a self-supervised sequence-to-sequence mapping objective. As in BERT, the Encoder is bidirectional and pretraining consists in mapping arbitrarily corrupted textual sentences to their original form. However, BART generalizes the BERT objective since a single [MASK] token can cover several consecutive target tokens (as in T5), while alternative corruptions are also possible. After pretraining, downstream finetuning can take place without any corruptions to the input. With this setup, BART achieves very good results in a variety of downstream language analysis tasks.
	
	Notably, both T5 and BART utilize a full Transformer Encoder-Decoder architecture with an autoregressive Decoder and an inherent suitability to text-to-text mapping. An important consequence of this choice is that they can easily be finetuned for generative NLP tasks (text generation, e.g., for document summarization) without \textit{any} architectural additions after pretraining, while this is not possible with the bidirectional Encoder-only BERT. In this sense, BERT and its derivatives can be considered deprecated as of 2023 \cite{tay2022ul2}.
	
	\subsubsection{GPT and Large Language Models}
	Recently, NLP algorithms employing pretrained GPT (Generative Pretrained Transformer) \cite{GPT} neural models have been gaining a lot of traction. Similarly to other state-of-the-art language models, GPT is a large-scale autoregressive Decoder-only Transformer pretrained in a self-supervised manner and on a gigantic corpus for predicting the next token in a sequence. Due to this traditional causal language modelling objective, pretrained GPT models can be directly employed for text generation without any finetuning: they are given a text \textit{prompt} as input and they generate a corresponding textual response as an output. Although pretrained GPT models can be finetuned in a typical supervised manner for a specific downstream task, their most impressive capability is that of zero/few-shot downstream language task execution during inference, through careful prompting and without any finetuning. In this case, the desired task is essentially described in natural language on-the-fly via the prompt. However, this setting falls outside the scope of this article.
	
	When it comes to long document analysis, the latest openly available pretrained GPT model is GPT 2.0 \cite{GPT2}, which remains regularly outperformed by BERT-derived language models, such as RoBERTa \cite{tanyangxing}, in the finetuning setting. However, the publicity GPT models have attracted due to their text generation capabilities makes them a serious competitor for BERT and its variants. GPT 3.0 achieves similar results to most BERT-based language models \cite{tanyangxing}, but is not openly available to the public. Similarly closed-source competitors to the GPT family do exist, such as \cite{chowdhery2022palm}, while open-source alternatives are BLOOM \cite{scao2022bloom} and OPT \cite{zhang2022opt}. The scale factor, i.e., highly complex Decoder-only Transformer DNN architectures and a gigantic pretraining corpus/dataset, has proven instrumental for obtaining good performance in all the above cases of Large Language Models (LLMs). However, the observation that scaling up the DNN architecture is only beneficial when accompanied by a corresponding increase in the dataset size has recently led to more reasonably sized models of this nature, i.e., less complex ones, such as Chinchilla \cite{hoffmann2022empirical} and LLaMA \cite{Touvron2023llama}. 
	
	Table \ref{tab::ModelComplexity} showcases the escalation of language model complexity in recent years.
	
	\begin{table}
		\centering
		\begin{tabular}{ |p{3cm}|p{3cm}|p{3cm}|p{3cm}| }
			\hline
			\cellcolor{blue!25}\textbf{Name} & \cellcolor{blue!25}\textbf{Year} & \cellcolor{blue!25}\textbf{\#Parameters} & \cellcolor{blue!25}\textbf{Input Size (\#tokens)} \\
			\hline
			BERT & 2019 & 110M & 512 \\
			\hline
			BART & 2019 & 140M & 1024 \\
			\hline
			T5 & 2019 & Up to 11B & 512 \\
			\hline
			GPT & 2018 & 120M &  - \\
			\hline
			GPT-2 & 2018 & 1.5B & 1024 \\
			\hline
			GPT-3 & 2020 & 175B & 4096\\
			\hline
			BLOOM & 2022 & 176B & - \\
			\hline
			OPT & 2022 & 125M to 175B & - \\
			\hline
			PALM & 2022 & 540B & 3072 \\
			\hline
			Chincilla & 2022 & 70M to 16B & - \\
			\hline
			LLaMa & 2023 & 7B to 65B & 2048 \\
			\hline
		\end{tabular}
		\caption{Comparing modern language models by model complexity and input tokens limit (where available).}
		\label{tab::ModelComplexity}
	\end{table}
	
	
	\section{Document Classification}
	\label{sec::Classification}
	Document classification is the automatic assignment of one or more class labels to a piece of text, based purely on its contents and assuming a known finite set of discrete labels. In long document NLP, the text in question is typically an entire document. As an example, a book of fiction could automatically be assigned a genre label by a text classifier deployed in a library, supporting labels such as ``Romance" or ``Science Fiction".
	
	Overall, recent methods for neural long text/document classification mostly rely on pretrained Transformer language models (e.g., BERT) with a classification head appended at the end. The language model may be finetuned for the desired downstream classification task, while training the head. This approach is suitable for most relevant tasks \cite{qian} \cite{ion} \cite{ion2} \cite{mcbert} \cite{ammar} \cite{so}. However Transformers may be suboptimal for specialized tasks or domains \cite{junhua}, allowing CNN and RNN architectures to occasionally outperform them on long documents. Relevant issues and solutions are discussed below.
	
	\subsection{Challenges}
	\label{ssec::ClassificationChallenges}
	Long text classification algorithms inevitably face challenges that are common to all NLP tasks, as well as issues specific to the case of long texts. Below, follows a non-exhaustive list of the most significant relevant challenges:
	
	\begin{itemize}
		\item \textbf{Input size}. The size of the input document is the most significant challenge in long document NLP. For example, the computational overhead of CNNs tends to grow proportionally to their inputs; thus, they are unable to process documents with thousands of sentences, without specialized hardware. On the other hand, RNNs/LSTMs struggle to conserve information and identify long-term dependencies over huge spans of text \cite{worsham_book}. Transformer DNNs, such as BERT and most of its variants, are typically constrained by a rather low maximum token sequence length; usually 512 or 1024 tokens. This limits their training to relatively short texts, forcing brutal token selection during inference. As a result, their performance on long texts is severely limited \cite{dai}. Additionally, traditional Transformer architecture \cite{ilia} rely on the typical \textit{global self-attention} mechanism, comparing each token to all other input tokens at each self-attention head, leading to a quadratic ($O(n^2)$) increase in computational and memory requirements with respect to input size.
		
		\item \textbf{The curse of dimensionality}. As the supported vocabulary increases in size, the possible inference-stage combinations of its words grow exponentially large, unlike the fixed training dataset size.
		
		\item \textbf{Polysemy}. A word may express multiple different meanings in different contexts. For example the word ``bank" may mean a financial institution or a building. DNN models must learn to appropriately differentiate the exact meaning of a word depending both on the local context (sentence, paragraph) and the global context (document).
		
		\item \textbf{Homonymy}. Homonyms are words that either share spelling (homographs), pronunciation (homophones) or both, but are not semantically related. For instance, the term ``bank" may refer to a financial institution or to the shore of a river. The difference between homonymy and polysemy is that homonyms are not at all related to each other in meaning.
		
		\item \textbf{Figurative language}. Sarcasm, irony and metaphors pose serious challenges to NLP, since the real meaning of a phrase is different from the immediately obvious one \cite{Karamouzas2022}. Figurative language is common to both short and long texts, but in the latter case it can take the form of sustained allegories (e.g., in literary books).
		
		\item \textbf{Unstructured text}. Long texts may be very unstructured for the most part, while communicating information in rather indirect and abstract ways. For example consider a novel, partitioned in a few dozens of chapters, and contrast it with short texts, such as product reviews or tweets. In the first scenario, crucial information and layers of meaning may be distributed across a large number of very long chapters \cite{worsham}. Furthermore, long documents tend to follow constantly evolving textual and contextual conventions, leading to a higher chance of shifts between learnt and actual probability distributions. This is especially problematic for literary works, which are fluid and susceptible to changing cultural biases \cite{brazil}.
		
		\item \textbf{Multiple labels}. The longer an input document is, the more probable is that it belongs to multiple classes concurrently. Classifiers should specifically take it into account for meaningful analysis, since each document may have a varying numbers of ground-truth labels.
		
		\item \textbf{Foreign language datasets}. There is limited availability of training content for languages other than the most popular ones (e.g., English, French, Spanish, etc.). Given that both the word embedding networks and the main, task-specific NLP DNNs are typically trained on one language at a time, the lack of extensive datasets in rarer languages leads to less optimal embeddings and, as a result, reduced performance in downstream tasks. The situation is aggravated by limited training sets for the downstream tasks themselves. Thus, less data-intensive machine learning algorithms may be preferred over DNNs for rare-language texts, such as decision tree classifiers \cite{brazil}, or the existing datasets can be artificially augmented \cite{geroge}.
	\end{itemize}
	
	
	\subsection{Solutions}
	\label{ssec::ClassificationSolutions}
	
	Traditional machine learning algorithms, such as Naive Bayes \cite{russel}, SVMs (Support Vector Machines) \cite{cortes} and ID3 decision trees \cite{quinlan} have been repeatedly applied for document classification \cite{brazil} \cite{xu}, being competitive even in large literary texts \cite{sicong}. However, the field has generally moved decidedly towards DNNs and this Subsection presents the relevant state-of-the-art. The innovations of the various existing approaches mostly fall under the following categories, which are more or less tied to the challenges mentioned in Subsection \ref{ssec::ClassificationChallenges}:
	
	
	\subsubsection{Multi-label Classification}
	Multiple labels are essential for many real-world applications \cite{hsu} \cite{patentnet}, thus many methods attempt to identify a document as belonging to multiple classes concurrently. This may be trivial to achieve in itself, using well-known DNN models that have been properly adapted. The varying number of labels per document and class imbalance issues, which may lead to learning extremely biased patterns from the training data, can be handled by applying weighted loss functions. However, multi-label classification poses unique challenges with regard to how accuracy is measured.
	
	One representative state-of-the-art approach attempting to tackle this scenario is \cite{sicong}, where multiple different classifiers (including a pretrained BERT) are comparatively evaluated in multi-label multi-class document classification, using the book genre recognition task as a benchmark. The steps taken in \cite{sicong} to build a successful genre recognizer are telling of more general difficulties with long document classification. Regarding the issue of multiple labels, a modified accuracy metric is exploited to measure classification accuracy: the true positive and true negative classifier predictions are first computed separately per class label, with the proposed metric being the weighted average of correctness over all the label classes. Per-class weights are proportional to the class label frequency and normalized to sum to 1. Similarly, the loss function employed during training (Binary Cross Entropy) is modified to include weights for each label, so that each class is scaled inversely to the prominence of the class in the training set.
	
	\subsubsection{Feature Pruning}
	This is the traditional way to handle long textual inputs. It involves systematically discarding as much text as possible at the preprocessing stage, while minimizing information loss. In order to cope with the potentially huge input size in long document analysis, almost all relevant NLP methods are prefaced by a preprocessing stage, where less useful input elements are removed. Appropriately reducing the input size improves both computational efficiency and accuracy. To this end, the book genre recognizer of \cite{sicong} uses an aggressive sampling method which first discards:
	
	\begin{itemize}
		\item ``stopwords", i.e., known, extremely frequent words such as ``a", ``and", ``he", etc.,
		\item words that appear less than 20 times,
		\item words that appear in more than 75\% of all books,
		\item words that appear in more than 75\% of all classes,
		\item paragraphs where the frequency of the remaining words is less than a certain threshold,
		\item the contents of each remaining paragraph after the 512th token, because of technical limitations imposed by BERT.
	\end{itemize}
	
	Thus, each remaining paragraph is represented by a subset of its words that fall within a restricted, book-level set of ``keywords". Finally, the sampled paragraphs are only those with the highest concentration of keywords. The goal of this ruthless sampling is to keep the input's length manageable for documents containing many thousands of pages. Methods similar or identical to this one are present in most NLP models specialized for long texts; thus, they are utilized extensively in most research cited in this paper.
	
	On the other hand, the book genre recognizer of \cite{worsham} employs only minimal preprocessing, just enough to allow the DNN to run relatively efficiently. It relies on an index, consisting of the 5000 most common words from the supported vocabulary. No other preprocessing/sampling takes place, under the intuition that exposing the DNN to even statistically unimportant words facilitates an understanding of patterns and grammatical modifiers, such as tense and plurality. Words are represented by typical neural embeddings. Different strategies were evaluated for feeding the document to the classification DNN:
	\begin{itemize}
		\item Feed only the first 5000 words of the document.
		\item Feed only the last 5000 words of the document.
		\item Feed only 5000 random words of the document.
		\item Feed the entire document, split into chapters.
		\item Utilize a traditional bag-of-words approach.
	\end{itemize}
	
	These strategies were then employed on a series of different architectures. Ultimately, the random word selection proved to consistently outperform other selection strategies for a multitude of classifiers, with the best performer being a CNN-Kim \cite{kim} architecture.
	
	\subsubsection{Sparse Attention Transformers}
	Given the quadratic computational complexity of Transformer DNNs and their prominence in recent years, attempts have been made to reduce their cost. Such approaches try to achieve linear complexity with respect to the input size, by keeping track of a small, fixed-size window of neighbouring tokens around each input token, instead of considering all $n$ tokens of the input sequence within each self-attention head. However, the unavoidable trade-off is a reduced ability of the DNN to capture long-term dependencies, which is as important for long texts as the complexity with respect to the input size. Below, the terms token and element are used interchangeably, although strictly speaking only the first Transformer layer receives actual tokens as input (e.g., word embeddings).
	
	\paragraph{Sparse Transformer} The Sparse Transformer was introduced in \cite{child}, so as to lower asymptotic complexity from $O(n^2)$ to $O(n\sqrt{n})$. This is done by applying sparse factorizations to each \textit{self-attention matrix} computed by the Transformer during inference, which contains the current attention weights of each input element against each of the other elements of the input sequence. Given that such a matrix is separately constructed at each self-attention head of each layer, it is important to effectively reduce the computational and memory cost of this operation. The sparse attention mechanism relies on replacing full attention with several small, optimized attention operations, which jointly approximate the original mechanism sufficiently. Essentially, each token attends to only a subset of the overall input sequence tokens. Additional optimizations, such as efficient sparse attention kernels and improved weight initialisation contribute to a less demanding architecture, which is thus able to handle longer sequential inputs. The Sparse Transformer is a general model suitable for various applications involving very long input sequences, ranging from image compression to document analysis.
	
	\begin{figure}
		\includesvg[width=16cm]{longformer.svg}
		\centering
		\caption{Comparing the full self-attention pattern and the configuration of attention patterns in a standard Longformer.}
		\label{fig::longformer}
	\end{figure}
	
	\paragraph{Longformer} Surpassing the Sparse Transformer's computational gains, the Longformer \cite{longformer} achieves a linear asymptotic cost and is specialized for long document NLP. Its self-attention mechanism can be directly incorporated within other Transformer models and operates by employing a \textit{dilated sliding attention window}, which is a modification of the classical sliding window approach for \textit{local attention} \cite{saeed, odysseas}. In the typical case, each token's window is comprised of $w$ tokens surrounding it: $\frac{w}{2}$ tokens to its left and right, respectively. The underlying intuition is that the semantic context of a token can be mostly derived from its neighbouring tokens, while the computational complexity of this operation is $O(n \times w)$. In a sense, the end effect resembles the local receptive fields of neurons in convolutional layers. Similarly to CNNs, stacking $l$ Longformer layers gradually increases this receptive field, so that representations of tokens faraway from the query can be attended within the later layers.
	
	The \textit{dilated sliding window} approach modifies the sliding window with a predetermined dilation factor $d$ added at each step, which determines the spacing between adjacent window positions and allows data analysis at multiple scales simultaneously. Thanks to the introduced ``gaps" in the attention pattern, a larger range of tokens within the self-attention matrix can be covered without increasing calculations and computation time. Thus the effective receptive field of the window is of length $l \times d \times w$, while memory and processing costs remain steady.
	
	Since windowed and dilated attentions may not be enough to extract a suitable sequence representation, certain additional, pre-selected locations of the full self-attention matrix are also allowed to be considered as global context. These global tokens attend to/are attended by the entire sequence as normal, but their number is kept constant and relatively small so as to limit complexity to $O(n)$. The [CLS] token is a good candidate for being selected as a global one, since its generated representations should convey information about the entire sequence. Overall, as a result of Longformer's innovations, it has been successfully evaluated with inputs of length up to 4096 tokens (compared to BERT's maximum of 512 tokens). A comparison between full, sliding, dilated sliding and global-dilated sliding window self-attention patterns is depicted in Figure \ref{fig::longformer}.
	
	\paragraph{BigBird} BigBird \cite{big_bird} can be seen as a variation of the Longformer, attempting to support much longer input sequences by reducing the complexity of self-attention from quadratic to linear. It also employs a sliding window, since in typical NLP tasks the semantic context of a token can be mostly derived by its neighbouring tokens. Instead of the Longformer's dilated sliding windows, it employs random token selection to reach faraway tokens and provide context. This complements the global attention and the regular sliding window attention schemes, adopted and adapted from the Longformer. It has been demonstrated mathematically that this modified sparse attention mechanism satisfies many theoretical properties of the original full attention mechanism, such as universal approximation and Turing-completeness, but seems to require more layers to retain accuracy comparable to full attention architectures.
	
	\subsubsection{Hierarchical Transformers}
	Hierarchical models attempt to handle the large input size of long texts by appropriately building upon original Transformers, instead of modifying them. This is usually achieved in one of two ways:
	
	\begin{itemize}
		\item By transforming the Transformer's input via a suitable DNN (e.g., a CNN or RNN/LSTM). The latter one generates a single document representation (document embedding) able to fit into a standard Transformer (e.g., BERT), thus bypassing the input size limitations.
		\item By segmenting the input document. The input segments are independently truncated to the Transformer's input size and the network (e.g., BERT) generates embeddings for each of these segments. A separate DNN then combines the segment embeddings and predicts the document label.
	\end{itemize}
	
	\paragraph{Hierarchical Attention Network (HAN)} The first attempt towards a Hierarchical Transformer \cite{ion_han} \cite{zichao} was built on top of standard BERT. During inference, the input document is split into paragraphs that are truncated to fit into BERT. The latter one generates independent paragraph embeddings, which are combined by the succeeding HAN into a single output. The end result is an increased ability to handle long texts, by exploiting the natural partitioning of documents into paragraphs.
	
	A similar approach was later used in \cite{glue_gunner}, where the base models were BERT, RoBERTa, DeBERT \cite{yanguang}, Legal-BERT \cite{ion6} and CaseLaw-Bert \cite{zheng}. In long text analysis tasks, the hierarchical versions proved competitive with Sparse Attention Transformers, such as Longformer and BigBird.
	
	The method in \cite{khandve}, relying on BERT or on Universal Sentence Encoders (USEs) \cite{use} (models learning sentence representations for transfer learning), follows up on this methodology, but replaces the final HAN with a CNN or an LSTM architecture. The BERT+LSTM combination proved to be the best among the evaluated hierarchical approaches, which however were generally outperformed by Sparse Attention Transformers.
	
	\paragraph{Hi-Transformer} The main alternative to \cite{glue_gunner} and its variants is to essentially reverse the process: that is, to employ a hierarchical DNN for obtaining a fixed-size document-level representation/embedding from the entire input, which is then fed to a regular classification DNN. \textit{Hi-Transformer} \cite{qi} achieves this via a Transformer acting as a ``Sentence Encoder" (SE), i.e., aggregating and projecting the regular, individual word embeddings of the input document into sentence-level embeddings. Once every sentence has passed through SE, these embeddings are subsequently ordered by positional embedding and fed as input to a subsequent Transformer called ``Document Encoder" (DE). The latter's output is a context-aware document embedding, which is fed as input to the next SE along with the original sentences. Thus, the generated, revised sentence embeddings are aware of the global, document-level context. This process is repeated by stacking multiple such layers, until the final output is passed to a pooling layer that extracts the final document embedding. According to \cite{qi}, two such layers seem to be sufficient for outperforming Sparse Attention Transformers in long text analysis. This can be attributed to the readily accessible global/document-level context. Since the final classifier has to analyze a fixed-size document representation and the SE's complexity is linear to the number of sentences, computational demands are not higher than in the sparse attention case. More impressively, Hi-Transformer's accuracy has been observed to actually increase with longer documents, since more relevant context can be extracted.
	
	\paragraph{Hierarchical Sparse Transformer} Hierarchical Transformers have been combined with the sparse attention mechanism, in order to ameliorate the latter's inability to capture all the necessary global/document-level context, in the case of long texts. This inability arises from the low level of global context diffusion in the generated token representations, since the global tokens are limited in number. The \textit{ERNIE-SPARSE} architecture \cite{liu} merges the two schools of thought by using hierarchical attention, which is then fed to a Sparse Transformer in order to increase the information extracted from the global context.
	
	ERNIE-SPARSE utilizes a modified attention mechanism, where each query is allowed to attend to a limited number of additional representative tokens within each fixed-size window of the self-attention matrix, along with regular global and local tokens from the input sequence. These inserted representative tokens carry global context from the entire document and are derived in two steps: first, the regular Sparse Transformer is applied to the unaltered input sequence and, then, regular full self-attention is applied to a small collection of selected outputs. ERNIE-SPARSE has been shown to outperform the competition on well-known document classification datasets. It can be considered both a hierarchical and a sparse attention approach, since it modifies the Transformer's internal architecture. Figure \ref{fig::ERNIE} depicts the modified attention mechanism of ERNIE-SPARSE, in comparison to that of the Sparse Transformer.
	
	\begin{figure}
		\centering
		\includesvg[width=14cm]{images/hst.svg}
		\caption{Comparison of Sparse Transformer (ST) and Hierarchical Sparse Transformer (HST). (a) Sparse Transformer comprises global attention and local attention. (b) ST faces a limitation where all sequence details are compressed into a fixed vector size. (c) HST addresses this limitation by incorporating representative tokens into local attention, enabling hierarchical attention. (d) The information flow in HST demonstrates how the interaction among representative nodes can enhance global information interaction pathways.}
		\label{fig::ERNIE}
	\end{figure}
	
	\subsubsection{Recurrent Transformers}
	The fixed input size of baseline Transformers leads, during both training and inference, to \textit{context fragmentation} in long document analysis: essentially, the DNN analyzes the input as independent, consecutive segments. Thus, a complementary direction of the state-of-the-art is to allow the Transformer to learn and extract long-term dependencies beyond the preset fixed input sequence size, while maintaining linear computational complexity and retaining local and global context.
	
	\paragraph{Transformer-XL} The Transformer-XL architecture \cite{dai-etal-2019-transformer} introduces recurrence into the Transformer, thus integrating the RNN concept of retained hidden states. These states are passed within the Transformer from one segment to the succeeding one, thus effectively transmitting the previously established context and allowing the identification of long-term dependencies across segments. Unlike RNN/LSTM recurrence, where each layer's stored state is exploited at the next time step by the same layer, the corresponding Transformer-XL mechanism shifts the transmitted state one layer downwards. For example, layer $l=n$ has access to 2 hidden states from layer $l=n-1$, 4 hidden states from layer $l=n-2$ and $2^n$ from layer $l=1$, which are the original input word embeddings.
	
	This mechanism guarantees $O(n \times l)$ cost, where $l$ are the number of layers, and significantly faster inference compared to baseline Transformer, since representations computed for previous segments are re-used instead of being computed from scratch for each new segment. It also allows different input sequence sizes between the training and the inference stage. Ultimately, Transformer-XL achieves competitive results on extremely long document analysis, while being significantly more efficient than the baseline Transformer.
	
	\paragraph{ERNIE-Doc} ERNIE-Doc \cite{ernie-doc} builds upon Transformer-XL but makes two passes over the input sequence, similarly to how humans first ``skim" a document before paying attention to important sections. To achieve this, the cached hidden state is computed as follows:
	\begin{equation}
		\hat{\mathbf{H}} = [\hat{\mathbf{H}}^{1}_{1:T};...;\hat{H}^{N}_{1:T}] \text{ (skim phase)}
	\end{equation}
	\begin{equation}
		\tilde{\mathbf{h}}_{r+1}^{n-1} = [\tilde{\mathbf{H}};\mathbf{h}_{r+1}^{n-1}] \text{ (retrospective phase)},
	\end{equation}
	\noindent where $T$ is the number of consecutive document segments, $L$ is the length of the document, $N$ is the number of layers. $\hat{\mathbf{H}}^{i}_{1:T} = [\hat{\mathbf{h}}^{i}_1;...;{\mathbf{h}}^{i}_T]$ is the concatenation of the $T$ cached hidden states initially derived in the skimming phase for the $i$-th layer. Thus, $\tilde{\mathbf{H}} \in \mathbb{R}^{(LTN) \times d}$ is the concatenation of all $\hat{\mathbf{H}}^{i}_{1:T}$ (one per layer), Thus, context from the entire document can be exploited in the retrospective phase in order to form the extended hidden state $\tilde{\mathbf{h}}^{n}_t$.
	
	This skimming architecture is incompatible with the recurrence mechanism from \cite{dai-etal-2019-transformer} and, due to its linear cost with regard to the number of layers, it may be prohibitive for long documents. To solve this, \cite{ernie-doc} ``flattens" the hidden state dependency from one-layer-downwards recurrence to same-layer recurrence, similarly to RNNs. Additionally, a novel, document-level task for self-supervised pretraining is introduced that is called ``Segment-Reordering Objective" (SRO). It entails randomly splitting a long document into $m$ segments, shuffling them, then letting the Transformer reorganize them in order to learn their interrelations. After pretraining on both the MLM task and on SRO, ERNIE-Doc proved highly competitive in several long document analysis datasets for a range of tasks such as classification, question answering and key-phrase extraction. Figure \ref{fig::ERNIEDoc} depicts the modified recurrent mechanism of ERNIE-Doc, in comparison to that of basic Recurrent Transformers.
	
	\begin{figure}
		\centering
		\includesvg[width=15cm]{images/ernie_doc.svg}
		\caption{Comparison of 3-layer ERNIE-Doc and Recurrent Transformer, assuming a long document $D$ that is divided into four segments (S1, S2, S3, S4). \textit{Recurrent Transformer} (upper): While training on S4, it can only incorporate contextual information from the preceding two consecutive segments, S2 and S3, due to the linear growth of the effective context length in relation to the number of layers. \textit{ERNIE-Doc} (lower): With the assistance of an enhanced recurrence mechanism, the effective context length is significantly increased. Consequently, S4 can assimilate the information from S1, which was discarded by the Recurrent Transformer. Segments in the retrospective phase contain the contextual information of the entire document.}
		\label{fig::ERNIEDoc}
	\end{figure}
	
	\subsection{Sparse, Hierarchical or Recurrent Transformers?}
	Sparse attention, hierarchical and recurrent approaches dominate the current literature for long document analysis using Transformers. However, a number of challenges remain such as the following ones:
	\begin{itemize}
		\item Sparse Attention Transformers tend to suffer in accuracy, due to their reliance on a small number of global tokens that must encapsulate all document-level context.
		\item Hierarchical Transformers are particularly susceptible to context fragmentation, since the individual segment embeddings are derived either without sufficient local context \cite{dai}, or with no access to the global document context \cite{qi}.
	\end{itemize}
	Currently, there is no consensus on which approach performs better, nor any empirical rule on which tasks each architecture performs best in. Hybrid algorithms and recurrence seem to be the most promising avenues for advancing long text analysis.
	
	\section{Document Summarization}
	\label{sec::Summarization}
	Automatic Text Summarization (ATS) is the task of generating a short summary from a source input document. In the context of long documents, such as medical and legal records, it is often called ``document summarization". The generated summary must be coherent, concise and avoid redundancy, while accurately maintaining the meaning of the most important information in the source material. There are two main approaches to ATS: \textit{extractive} and \textit{abstractive} summarization.
	
	\textbf{Extractive summarization} generates its summary by selecting verbatim sentences found in the source text. This practically reduces summarization into finding a binary selection vector $\mathbf{s} = [s_1, s_2,..., s_n]^T, s_i \in \{0,1\} \forall i \in [1,n]$, where $s_i = 1$/$s_i = 0$ if the $i$-th source input text sentence is/is not included in the summary, respectively. The simplicity and the tolerable computational cost of this approach have made it very common for on-line article summarization. However, its applicability is inherently limited by the fact that not all document types can be summarized well by just a few of their sentences taken verbatim and re-arranged. For example, there is no way to describe the content of most books by using a selection of their own sentences. Additionally, concerns have been raised that simple extractive approaches are reaching their peak and, therefore, current research should gravitate towards: a) ensembles of multiple extractive models, or b) abstractive summarization methods \cite{mehta}.
	
	\textbf{Abstractive summarization} is the alternative approach of generating new text. It is the method humans use when trying to manually summarize text, by reading the source, extracting its meaning and then writing a condensed document which contains as much of the source's meaning as possible. As expected, modern abstractive algorithms are DNNs \cite{cho}. Compared to extractive alternatives, they generate more meaningful and condensed summaries, while being more adaptable to the input document type. However, there is a much larger risk of the output summary misrepresenting facts and/or not being coherent at all. The most commonly used method for abstractive summarization involves employing a sequence-to-sequence mapping (Seq2Seq) model \cite{seq2seq}, via an Encoder-Decoder DNN architecture. RNNs and Transformers have both been employed in this context, where the Encoder captures suitable representations of the document's tokens and feeds them to the Decoder that generates a summary.
	
	\subsection{Challenges}
	\label{ssec::SummarizationChallenges}
	Document summarization comes with its own set of challenges. These are essentially specific to the case of long texts, since there is typically no reason to summarize a short text. Yet, a subset of them are shared with the challenges faced by long document classifiers.
	
	\begin{itemize}
		\item \textbf{Input size}: The length of the input document gives rise to significant issues which go beyond simple computational and hardware requirements. Extractive DNNs particularly struggle with long documents \cite{xiao}; the greater the length of the input, the more topics it typically covers, thus the harder it is for a DNN to generate a summary covering effectively all of them.
		
		\item \textbf{Evaluation metrics}: The ROUGE evaluation metric \cite{rouge}, which is the most commonly used metric for summarization tasks, might not be sufficient for abstractive DNNs. It has been claimed to perform considerably worse than in the case of extractive summarization, since it lacks semantic understanding of the words it compares \cite{akter}.
		
		\item \textbf{Sentence extraction}: Framing extractive summarization as a binary decision task assumes that the meaning of individual sentences is independent from that of others, leading to high redundancy. Even accounting for this fact, such DNNs are still vulnerable to greedily picking general sentences, instead of finding a combination of specific sentences which together summarize the document much more effectively \cite{matchsum}.
		
		\item \textbf{Repetition}: One of the most common issues of abstractive DNNs dealing with long texts is sentence repetition; this is potentially caused by the over-reliance of a Decoder to its input, which gives rise to an endless loop of phrase repetition \cite{abigail}. Internal issues concerning RNNs with attention mechanisms have been pointed out as the root cause of both this issue and of false information appearing in the output summary \cite{suleiman}.
		
		\item \textbf{Transformer issues}: Most relevant state-of-the-art deep neural architectures feature the Encoder-Decoder pattern, implemented via Transformers. The challenges plaguing Transformer architectures such as BERT have already been largely analyzed in Section \ref{ssec::ClassificationChallenges}. Many of these remain as equally critical issues in long document summarization as well.
	\end{itemize}
	
	
	\subsection{Solutions}
	This Section reviews the commonly employed ROUGE evaluation score, as well as extractive and abstractive text summarization methods. The evolution of relevant algorithms, as well as how each family attempted to handle the challenges it faces, are briefly discussed.
	
	\subsubsection{Evaluation Metrics}
	\label{sss::SummarizationEvaluation}
	ROUGE is a very commonly used metric for evaluating ATS algorithms, featuring as many as 192 variations. The base ROUGE metric \cite{rouge} measures the number of overlapping n-grams/sequences between the generated and ground-truth summary. This is sufficient for many extractive summarization tasks, but faces severe challenges when used for abstractive summarization. ROUGE is unable to distinguish between different grammatical versions of the same word \cite{suleiman} and synonyms \cite{akter}, while being agnostic towards sentence ordering \cite{graham}. Most importantly, the underlying assumption made by ROUGE, that the quality of a summary is correlated to its coverage compared to the human-generated summary, may not be statistically significant \cite{pitler}.
	
	The reasons why ROUGE is still popular despite such severe drawbacks are multiple, including the unavailability of proven competitors and the lack of easy-to-use software implementations or standardized benchmarks for ATS evaluation \cite{fabbri}. Besides, alternative metrics may suffer from similar weaknesses, since they typically do not account for factual consistency between the source document and its evaluated summary \cite{kryscinski}. For a detailed discussion of text summarization metrics, the reader is referred to \cite{fabbri}. For the purposes of this article, a number of common and/or recent alternatives to ROUGE are briefly mentioned below:
	
	\begin{itemize}
		\item METEOR \cite{meteor} (recommended by \cite{suleiman}), functions similarly to ROUGE, while being largely invariant to grammatical variants and synonyms.
		\item Sem-nCG \cite{akter} is a recent, semantically-aware metric focused on extractive summarization.
		\item BERTScore \cite{bert_score} uses the similarity of the contextual embeddings between the generated and the ground-truth summaries.
		\item $S^{3}$ \cite{s3} combines other evaluation metrics through a model, in order to produce an aggregated evaluation of the summary.
		\item BLANC \cite{blanc} is an unsupervised evaluation metric which does not depend on human-generated ground-truth summaries, by comparing BERT's performance on the Cloze task (on the source document) when it has and when it has not additional access to the generated summary.
	\end{itemize}
	Many more evaluation metrics have been proposed, but none has taken foothold yet. The question of a capable successor to ROUGE is still an open one.
	
	\subsubsection{Extractive summarization}
	\label{sssec:extractive}
	This Subsection presents the gradual evolution of extractive summarization DNNs towards the current state-of-the-art.
	
	\paragraph{Building Summaries Using Hierarchies} The first attempts to use DNNs for extractive summarization, while certainly obsolete today, developed core concepts that were later re-used and improved.
	
	Thus, the method in \cite{lapata} developed a neural Encoder-Decoder architecture, which is trained in a supervised manner by preprocessing human-written summaries to construct extractive ground-truth summaries (binary selection vectors). Architecturally, the Encoder is composed of a shallow CNN, called ``Convolutional Sentence Encoder", and a subsequent LSTM, called ``Recurrent Document Encoder". The first one analyzes a matrix of Word2Vec word embeddings, separately for each sentence of the source text, aggregating them to dense sentence embeddings. The second one sequentially receives these embeddings as input and generates a document-level embedding in the form of a fixed-size vector.
	
	The Decoder is one of two models following different, mutually exclusive strategies: a ``Sentence Extractor", which approaches the extraction task as a sentence selection problem, and a ``Word Extractor" which approaches it as a word generation task. The latter one is practically a limited-efficiency abstractive summarizer, with its supported vocabulary constrained to that of the source document, so it will not be detailed here. The Sentence Extractor is an LSTM-MLP combination, sequentially labeling individual source sentences for inclusion in the summary based on two criteria: i) whether they are relevant, and ii) whether they are mutually redundant with other sentences. The selection decision for the $t$-th sentence is computed as follows:
	\begin{equation}
		\mathbf{\hat{h}_t} = LSTM(p_{t-1}\mathbf{s_{t-1}}, \mathbf{\hat{h}_{t-1}}),
	\end{equation}
	\begin{equation}
		p_t = \sigma(MLP(\mathbf{\hat{h}_t}; \mathbf{h_t})),
	\end{equation}
	\noindent where $\hat{\mathbf{h}}_t$/$\mathbf{h}_t$ is the Extractor's/Encoder LSTM's hidden state for the $t$-th sentence/time step, respectively. $p_{t-1}$ represents the assigned probability that the previous sentence $\mathbf{s}_{t-1}$ belongs to the summary and $;$ is the concatenation operation.
	
	% The datasets used for either of the Decoders are adapted from the CNN/Daily Mail dataset. The extractive dataset for the Sentence Encoder is generated by selecting the sentences which feature the most semantic similarity with the ground-truth summary. The dataset for the Word Extractor on the other hand, was generated by selecting the words with the highest lexical overlap between the highlights (ground-truth summary sentences) of the article with Out Of Vocabulary (OOV) words being replaced by the most semantically equivalent word in the article.
	
	Following-up on \cite{lapata}, ``SummaRuNNer" \cite{nallapati} was proposed as an extractive summarization RNN with the useful ability to be directly trainable on abstractive ground-truth summaries. It contains multiple layers of bidirectional GRUs and a hierarchical Encoder. Similarly to \cite{lapata}, the first subnetwork generates sentence embeddings from individual word representations, which are sequentially fed as input to the second GRU. The latter one analyzes them and outputs a document-level embedding from the sentence representations; this is done by average pooling the second subnetwork's hidden states. The Decoder is simply a logistic binary classifier, assigning to each source sentence a ``summary"/``non-summary" label. To be directly trainable on datasets with abstractive ground-truth summaries, a separate RNN Decoder can be optionally attached to the Encoder \textit{only} during training and tasked to emit the most probable word at each time step. Figure \ref{fig::SummaRunner} depicts the SummaRuNNer architecture.
	
	
	\begin{figure}
		\centering
		\includesvg[width=12cm]{images/summarunner.svg}
		\caption{The SummaRuNNer architecture. Bidirectional RNNs are represented by double-pointed arrows.}
		\label{fig::SummaRunner}
	\end{figure}
	
	
	% \begin{equation}
		%     d = tanh(\mathbf{W_{d}} \frac{1}{N} \sum^{N^{d}}_{j=1}[\mathbf{h^{f}_{j}};\mathbf{h^{b}_{j}}] + \mathbf{b})
		% \end{equation}
	% where $\mathbf{h^{f}_{j}}$ and $\mathbf{h^{b}_{j}}$ are the hidden states corresponding to the $j$th sentence of the forward and backward sentence-level RNNs respectively, $N^{d}$ is the number of sentences in the document and â;â represents vector concatenation. The Decoder then classifies documents taking into account the salience, novelty as well as the absolute and relative positional embeddings of the content. % The model can be trained via extractive training, on datasets where the summary is a binary vector, where $1$ means a sentence needs to be included and $0$ that it should be skipped, and on abstractive summaries. This allows the model to train on many datasets which feature human-generated summaries without further pre-processing, which was the case in \cite{lapata}.% In the case of an extractive dataset the loss function is a standard CrossEntropyLoss function between the predicted and ground-truth binary summary vectors.
	
	The method ``LSTM-Ext" \cite{xiao} improves on \cite{lapata} and \cite{nallapati} by considering both global and local context information along with each sentence, in order to decide on its inclusion into the generated summary. Inspired by how humans read lengthy text, it was reasonably hypothesized that hierarchical information could benefit ATS. The ``LSTM-minus" algorithm \cite{wang} is applied to generate local context-aware embeddings at the section level. Similarly to \cite{lapata} and \cite{nallapati}, an Encoder-Decoder architecture is employed. The Encoder is again composed of a Sentence Encoder (SE) and a Document Encoder (DE), but now also includes a ``Topic Encoder" (TE) that provides access to local (section) context surrounding each sentence.
	
	SE is a GRU that maps input word embeddings to a fixed-size sentence-level vector, while DE is a bidirectional GRU that encodes the sentence embeddings into a document-level vector representation. The final document embedding is computed as the concatenation of the final hidden states of the backward and the forward GRU. TE \cite{wang} separately embeds local context about each sentence's section by employing the LSTM-minus method; the result is an additional vector per section. Subsequently, for each sentence, the embeddings of itself, of its section and of the entire document are concatenated, optionally with weights assigned by an attention mechanism, and fed to the Decoder. The latter one is simply a binary MLP classifier that predicts whether each sentence should be included in the summary or not.
	
	The architecture's efficiency seems to increase with increasing document length, making it a powerful architecture for long documents, while the most benefit comes from the use of local rather than global context. Thus, LSTM-Ext was among the first modern document summarization methods that explicitly handle the fact that longer texts tend to cover multiple topics. On the other hand, it is not designed to avoid unnecessary repetitions in the generated summary, resulting in outputs with potentially high redundancy.
	
	\paragraph{Hierarchical Summaries using BERT} In recent years, Transformer and BERT variants gradually replaced RNN and Word2Vec/GloVe variants in extractive summarization. However, older ideas have been retained and improved, with their efficiency increased in an attempt to resolve long-standing challenges of document summarization.\label{sssec: HierBERT}
	
	The method in \cite{lapata_bert} uses BERT for both extractive and abstractive summarization (BERTSUM-EXT and BERTSUM-ABS, respectively). Instead of using sentence-level vector representations/embeddings, BERT inherently generates individual token representations. Thus, BERTSUM-EXT inserts special [CLS] tokens in-between sentences as separators, to notify the DNN of distinct sentences and capture sentence-level context within the respective generated token representations. This modified pretrained BERT is utilized as the Sentence Encoder component within an Encoder-Decoder architecture. The Document Encoder is a Transformer that receives the sentence-level embeddings of the [CLS] tokens that are derived by BERT and transforms them so as to contain richer document-level context. The final resulting sentence embeddings are fed to a shallow, fully connected binary classifier that predicts whether each sentence is to be included in the summary or not. All input documents are truncated to a fixed number of supported tokens. The overall architecture is finetuned in a supervised manner, using ground-truth binary selection vectors.
	
	Despite the relatively strong performance of BERTSUM-EXT, it faces the typical challenges encountered by Transformers in long document analysis, such as varying and diverse topics. To counter this, the approach in \cite{histruct} extends BERTSUM-EXT by taking advantage of the usually strong hierarchical structure of long texts (individual sentences compose titled sections, which jointly form the document). Thus, it computes hierarchical positional embeddings that capture the relative position of a token within each section and the position of that section in the document. These representations, along with additional representations of the section titles, are incorporated into the sentence-level embeddings by the Document Encoder, which feeds the final shallow, fully connected binary classifier. Of course, the overall ``HiStruct+" architecture is finetuned for extractive summarization. The method seems to perform well in long scientific articles with a rigid section structure, while still achieving state-of-the-art performance in less formal documents of short-to-medium length. Finally, by evaluating the method using pretrained Transformer language models other than BERT, the Longformer seems to come at the top.
	
	\paragraph{HyperGraph Transformers} HyperGraph Transformers are a recently proposed approach to long document extractive summarization, exploiting ideas from previous hierarchical methods and implementing them using Graph Neural Networks (GNNs).
	
	The method in \cite{hegel} attempts to deal with the Transformer's quadratic complexity and context retention problems by developing a hybrid architecture called ``HEGEL" (HypErGraph transformer for Extractive Long document summarization), relying on the so-called \textit{Hypergraph Transformer}. HEGEL models the document as a graph $G = (V, E)$, where $V$ is a set of nodes and $E$ a set of \textit{hyperedges}: an edge that can connect two \textit{or more} nodes. We use the notations $u \in e$ and $u \notin e$ to represent whether a node is connected to a hyperedge in graph $G$ or not, respectively.
	
	In HEGEL, the graph nodes are the document's sentences and extractive summarization is modelled as a node classification task. Each sentence is initially semantically embedded to a dense vector using a pretrained BERT. There are three kinds of hyperedges in this document graph:
	\begin{itemize}
		\item A \textit{section hyperedge} connects all sentences that belong to the same section. For instance, in the context of a scientific article, a section could be the problem statement, the methodology or the conclusions. An incidence matrix $A^{sec}$ is defined as:
		
		\begin{equation}
			\mathbf{A}^{sec}_{i,j} =
			\begin{cases}
				1, \text{ if } s_{i} \in e^{sec}_{j} \\
				0, \text{ if } s_{i} \notin e^{sec}_{j},
			\end{cases}
		\end{equation}
		where $s_{i} \in e^{kw}_{j}$ means the $i$-th sentence belongs in the $j$-th section.
		
		\item A \textit{topic hyperedge} connects all sentences sharing the same topic. Topic clustering is implemented by using Latent Dirichlet Allocation \cite{dirichet}. The incidence matrix
		$\mathbf{A}^{topic}$ is defined as:
		
		\begin{equation}
			\mathbf{A}^{topic}_{i,j} =
			\begin{cases}
				1, \text{ if } s_{i} \in e^{topic}_{j} \\
				0, \text{ if } s_{i} \notin e^{topic}_{j},
			\end{cases}
		\end{equation}
		where $s_{i} \in e^{kw}_{j}$ means that the $i$-th sentence belongs to the $j$-th topic.
		
		\item A \textit{keyword hyperedge} connects all sentences containing the same keywords. These keywords are extracted using Key-BERT \cite{keybert} from the training sample. The incidence matrix $\mathbf{A}^{kw}$ is defined as:
		
		\begin{equation}
			\mathbf{A}^{topic}_{i,j} =
			\begin{cases}
				1, \text{ if } s_{i} \in e^{kw}_{j} \\
				0, \text{ if } s_{i} \notin e^{kw}_{j},
			\end{cases}
		\end{equation}
		where $s_{i} \in e^{kw}_{j}$ means that the $i$-th sentence contains the $j$-th keyword.
	\end{itemize}
	
	The overall incidence matrix $A \in \mathbb{R}^{n\times m}$ is generated by concatenating the three incidence matrices as:
	\begin{equation}
		\mathbf{A} = \mathbf{A}_{sec};\mathbf{A}_{topic};\mathbf{A}_{kw}.
	\end{equation}
	
	HEGEL receives as its input both the sequence of initial sentence embeddings and the overall incidence matrix. It adopts the hierarchical positional embeddings from \cite{histruct} and defines an alternative ``hypergraph attention" mechanism, along with the corresponding multihead hypergraph attention module, instead of regular self-attention heads. As in the vanilla Transformer, these components are contained within a deep neural architecture with standard fully connected layers, skip connections and layer normalization mechanisms. This Hypergraph Transformer extracts semantically rich sentence-level representations that are well aware of local context, global context and semantic interrelations between different parts of the document. The overall architecture is trained end-to-end for binary sentence classification using a ground-truth binary selection vector per document.
	
	HEGEL manages to outperform all extractive and abstractive baselines on several datasets of scientific articles, demonstrating the potential of GNNs for long document summarization. It remains to see if such architectures can retain this level of performance on less rigid and hierarchical document types. Figure \ref{fig::HEGEL} depicts schematically HEGEL's architecture.
	
	\begin{figure}
		\centering
		\includesvg[width=14cm]{images/hegel.svg}
		\caption{The overall HEGEL architecture.}
		\label{fig::HEGEL}
	\end{figure}
	
	\subsubsection{Abstractive Summarization}
	\label{sssec::AbstractiveSum}
	Abstractive summarization is typically approached as a sequence-to-sequence language modeling task, where a neural Decoder predicts the next word of the summary at each time step. The vast majority of modern relevant methods are trained in a supervised manner, using ground-truth summaries. This is arguably a more advanced task than its extractive alternative.
	
	\paragraph{Pointer-Generator Networks} One of the first truly successful abstractive summarizers is essentially a hybrid one: a DNN that dynamically chooses whether to select a source sentence verbatim, as an extractive method, or synthesize the next word in the summary \cite{abigail}. Inspired by the human ability to refer to unknown objects by pointing at them, such Pointer-Generator Networks (PGNs) are an improvement over the ``Word Extractor" proposed by \cite{lapata}.
	
	PGN can copy out-of-vocabulary (OOV) words from the source document, making it possible to generate text with rare words and keep a smaller vocabulary, thus reducing computational and memory costs. Essentially, it is a modified, attention-augmented sequence-to-sequence Encoder-Decoder architecture implemented similarly to \cite{nallapati}, where both the Encoder and the Decoder are one-layer, bidirectional LSTMs. The Decoder ends at a softmax layer and at each time step generates a new word, originating either in the source document or in a global supported vocabulary. Thus, at each time step of the Encoder's/Decoder's unfolding, the next token is read/outputted from the input/to the generated summary, respectively. In order to decide on the course of action for the next word, the PGN defines its output distribution over the union of the global and the document's vocabulary (called the ``extended vocabulary"), thus deciding at each time step whether or not it should copy or generate the next word. If a word is OOV, it will be copied from the source document by sampling from the attention distribution, while if it does not exist in the document a new word will be generated.
	
	The attention mechanism is defined as in \cite{bahdanau}. During inference, a \textit{coverage vector} is updated at each Decoder time step, containing the sum of attention distributions over all previous time steps. It is provided as an additional input to the attention mechanism, so that the latter avoids giving attention to already extracted words and, thus, repeating them in the summary. To enhance this behaviour, PGN is also guided during training by an extra \textit{coverage loss}: a regularizer that penalizes repeated attention to the same input tokens. Thus, the overall loss function computed at the $t$-th Decoder's time step is:
	\begin{equation}
		L_{o}^t = L_{ML}^t + \lambda \sum_i min(a_i^t, c_i^t),
	\end{equation}
	\noindent where $\lambda$ is a scalar weighting hyperparameter, $\mathbf{a}^t$ is the current attention distribution, index $i$ traverses the Encoder's time steps, $\mathbf{c}^t$ is the current coverage vector, while $L_{ML}^t$ is the primary loss term at the $t$-th Decoder's time step. Typically, $L_{ML}^t$ is the negative log likelihood of the $t$-th ground-truth summary word according to the final Decoder output, i.e., a generated probability distribution defined over all supported words of the extended vocabulary. The overall loss is summed over all of the Decoder's time steps.%Figure \ref{fig::PGN} depicts schematically the inference-stage operation of PGN.
	% \begin{figure}
		%     \centering
		%     \includesvg[width=16cm]{images/pointer.svg}
		%     \caption{Baseline sequence-to-sequence model with attention. The model may attend to relevant words in the source text to generate novel words, e.g., to produce the novel word beat in the abstractive summary Germany beat Argentina 2-0 the model may attend to the words victorious and win in the source text.}
		%     \label{fig::PGN}
		% \end{figure}
	
	PGN does not use any pretrained word embeddings, learning instead token representations from scratch. At the time of its appearance it showed remarkable success in handling OOV words, while also featuring a smaller vocabulary size and faster training times. It thus successfully handled most of the challenges outlined in Subsection \ref{ssec::SummarizationChallenges}. However, although historically significant, PGN cannot handle long documents well. As a result, more efficient and specialized architectures were later developed.
	
	\paragraph{BERT \& Sparse Attention Transformers}  As described in Subsection \ref{sssec:extractive}, BERT has been adapted for extractive summarization in the form of BERTSUM-EXT \cite{lapata_bert}. That neural architecture has also been employed for abstractive summarization in the form of BERTSUM-ABS.
	
	The Encoder of BERTSUM-ABS is a pretrained BERTSUM-EXT model, a choice which provides efficiency gains, due to transfer learning, and task-specific gains, due to emerging synergies between extractive and abstractive summarization \cite{wei2, gehrmann}. The Decoder is a regular, randomly initialized 6-layer Transformer. Different finetuning and training schedules are utilized for the Encoder and the Decoder, respectively, to avoid stability issues during optimization. Overall, the approach attained only marginal performance gains over competitors.
	
	Section \ref{ssec::ClassificationSolutions} reviewed the issues vanilla Transformers face with very long documents, with the biggest challenge being the quadratic cost of the self-attention mechanism. The various sparse attention solutions that attempt to bypass this restriction can also be employed for document summarization, by replacing a regular Encoder with a Sparse Transformer variant. One such architecture is the ``Longformer-Encoder-Decoder" (LED), i.e., where Longformers are utilized both for the Encoder and the Decoder. Critically, the Decoder pays full attention to the entire Encoder and to already decoded positions of the input. As expected, LED scales linearly with the input size, and has been shown to achieve highly competitive performance.
	
	Similarly, BigBird has also been employed for abstractive document summarization \cite{big_bird}. The sparse attention mechanism is used only in the Encoder, which handles the long input, while the Decoder exploits full attention, as the length of the summary (in tokens) is small by definition. BigBird achieved significant performance gains on long document summarization and competitive performance on smaller ones.
	
	BigBird makes use of Gap-Sentences Generation (GSG) for abstractive summarization \cite{pegasus}. It is a self-supervised pretraining objective especially constructed to aid downstream finetuning for text summarization. Essentially being a variant of T5's pretraining objective, it consists in masking an entire sentence with a single [MASK] token. Then, during pretraining, each input is a set of consecutive sentences, with a subset of them selected and hidden according to GSG. The DNN is tasked to generate the masked sentences jointly, in the form of a pseudo-summary, based on the seen ones. The selection of which input sentences to mask is not random, but based on the ROUGE score of each sentence with the remaining input text. Thus, the ones  finally hidden with [MASK] tokens are those that have been automatically judged as the most important ones. It must be noted that GSG was originally introduced as the main novelty of the PEGASUS neural language model, following a Transformer Encoder-Decoder architecture, like BART or T5.
	
	GSG and a variant of the sparse attention mechanism is also employed for abstractive document summarization in LongT5, which is a modification of the basic T5 neural architecture \cite{Guo22LongT5}. Thus, the LongT5 Encoder contains self-attention sublayers where each input token attends to its local neighbourhood plus to all global tokens. The latter ones are being dynamically constructed before each attention operation, by summing and normalizing different, non overlapping subsets of the input token embeddings. Despite the approximate nature of this sparse attention mechanism, the resulting ability to process longer inputs leads to overall improved accuracy in long document summarization.
	
	\paragraph{Local Attention and Content Selection} The majority of modern abstractive summarizers learn to implicitly select pertinent content from the input in order to generate the summary. Recently, attempts have been made to combine the advantages of Sparse Attention Transformers, which allow the handling of longer documents, with explicit content selection before the abstractive summary generation \cite{gales}.
	
	The main abstractive DNN in \cite{gales} is a modified version of BART \cite{bart}, called LoBART. Vanilla BART supports input sequences with a length up to 1024 tokens. LoBART modifies its layers so as to employ the sparse attention scheme of Longformer, with the difference that only local and no global attention is utilized. This allows LoBART to drop its memory requirements significantly, at the cost of missing context over large spans of text. The reduced costs result in a sliding window of $W=700$ tokens, which is calculated as $2 \times D$ where $D$ is the empirical average attention distance of $D \in [250, 350]$ tokens for an attention mechanism with uniform weights, attending to a distance of $1024$. Even with this compromise, important context may still be missed, especially in long documents.
	
	As a result, the method in \cite{gales} feeds LoBART with suitably preprocessed input sequences, so as to effectively increase its context span. This preprocessing takes place separately during the training and the test stage. At training, the input data are first passed through a content selection algorithm. This can either be an ``oracle" (ORC), which selects sentences based on the ground-truth (the ROUGE score of the selected input vs its abstract), or a hierarchical, extractive DNN pretrained for multitask content selection (MCS). The straightforward ``ORC" preselection keeps an average of 21.3\% of sentences, drastically reducing the input's size and leads to 56\% of the input documents being shorter than the typical BART input length of 1024. Missing sentences are padded with random sentences sampled from the input document. On the other hand, MCS is a GRU-based, attention-augmented Encoder-Decoder architecture followed by a classification layer. It employs a hierarchical design similar to that of \cite{liu} (see Section \ref{ssec::ClassificationSolutions}), but in this case it is only utilized to preprocess the input document. In order to exploit synergies between extractive and abstractive summarization, similarly to \cite{lapata_bert}, MCS is jointly pretrained (in a multitask fashion) both for predicting the ground-truth summary token-by-token and for binary sentence classification as "summary" or ``non-summary".
	
	During inference, each sentence is first scored for retention by suitably combining the two different predictions (one per task) made by MCS. The top-scoring ones are fed to LoBART as the input sequence. LoBART has been trained for abstractive summarization in the usual manner, but the two-stage content preselection stage (explicit during preprocessing, implicit during the main summarization process) allows the method to efficiently handle longer documents.
	
	% the MCS model is simultaneously trained on an abstractive summarization task ($L_{seq2seq}$), in order to utilize the task-specific transfer learning property previously mentioned. The model is then also trained on the extractive summarization task ($L_{label}$). The training task is defined as follows:
	
	% \begin{equation}
		%     L_{seq2seq} = - \sum^{M}_{m=1} logP(y_m | y _{<m}, X)
		% \end{equation}
	
	% \begin{equation}
		%     L_{label} = - \sum^{N_1}_{i=1}(z_i log\hat{z}_i + (1-z_i) log(1-\hat{z}_i))
		% \end{equation}
	
	% \begin{equation}
		%     \hat{z}_i = sigmoid(\mathbf{W}^T_{cls} \mathbf{h}_i + \mathbf{b}_{cls}) 
		% \end{equation}
	% where $\mathbf{h}_i$ is the sentence-level encoder output associated with sentence $i$ and $\mathbf{W}_{cls}, \mathbf{b}_{cls}$ are the parameters of the classification layer.
	
	% \begin{equation}
		%     L_{MCS} = \gamma L_{label} + (1-\gamma)L_{seq2seq}
		% \end{equation}
	% where $\gamma$ is a hyperparameter.
	
	% The sentences are scored as follows:
	% \begin{equation}
		%     score_{i, label} = \hat{z}_i, score_{i, seq2seq} = \sum^{M}_{m=1} \mathbf{a}^s_{m,i}
		% \end{equation}
	% where $\mathbf{a}^s_{m,i}$ is the sentence-level attention weight at decoder step $m$ over input sentence $i$.
	
	% During inference, the test data is first processed by the MCS model since ORC "cheats" by utilizing the input labels, and it can therefore not be used on the test data. Then the input is fed to the LoBART model. This approach leads to significant improvements in memory and processing time, as well as state-of-the art results in the arXiv, PubMed and Spotify Podcast datasets. %A diagram showing the training and inference pipeline can be found in Figure \ref{gales_fig1}.
	
	\paragraph{Combining Top-Down and Bottom-Up Inference} An innovative abstractive methodology relying on a composite Transformer architecture is proposed in \cite{pan_bottom}, aiming to facilitate the sharing of global and local context between different token representations. This is done by allowing information to flow both from the individual token representations to the document embedding (bottom-up) and vice versa (top-down).
	
	The Encoder in \cite{pan_bottom} features a hierarchical approach similar to certain ones utilized for extractive summarization (e.g., \cite{xiao}): individual token representations computed by an initial (bottom-level) deep Transformer are pooled in segments to form top-level embeddings of coarse granularity at a paragraph/section-level. This is the so-called ``bottom-up" route. The preliminary top-level embeddings (one per section) are then transformed via a separate, shallow Transformer. However, the main novelty of \cite{pan_bottom} is that these final top-level representations are followingly injected to the individual token embeddings via a separate, subsequent deep``top-down" Transformer. The latter's layers are equipped with regular self-attention, as well as with cross-attention between the top-level representations and the individual token embeddings. The final output token representations are attended by the Decoder to generate the desired abstractive summary in the usual manner.
	
	By properly adapting Sparse Attention Transformers, the bottom-level and the top-down layers are assigned efficient local attention mechanisms only. Top-level layers can afford to be assigned full attention due to the much smaller number of sections, compared to the number of tokens. Thus, the top-down information flow enriches the lower-level embeddings with global context and the ability to capture long-range dependencies: individual token representations effectively attend to aggregated global information from the entire document, despite the local attention-only mechanisms of bottom-level layers.
	
	The methodology is rather generic and model-agnostic, allowing it to be used with efficient Sparse Attention Transformers. This fact and the hierarchical design facilitate exceptional summarization performance in long documents, even at book-length texts. Exploring its applicability to other tasks, such as document classification, is an promising research direction.
	
	\paragraph{Instruction-tuned summarization} An orthogonal direction of recent research is that of supervised finetuning of pretrained language models on ``instruction" and corresponding ground-truth summary pairs. The instruction is a command to summarize in natural language, followed by the long text to be summarized. The goal is to then use, during inference, the finetuned DNN like one would use a pretrained LLM under a zero/few-shot prompting mode. This can be done both with Encoder-Decoder Transformer architectures pretrained on masked token objectives and with LLMs, which typically are Decoder-only Transformer architectures pretrained on traditional next token prediction objectives. Both T5-based architectures \cite{victor2022multitask} \cite{chung2022scaling} and LLM-based architectures \cite{ouyang2022training} \cite{iyer2022opt} \cite{muennighoff2022crosslingual} have recently followed this approach, typically under a multitask instruction tuning setting where text summarization is one among multiple tasks. According to common automated evaluation metrics, this approach typically leads to subpar summarization results in common public benchmarks, compared to state-of-the-art dedicated summarization DNNs. However, such summaries generated from instruction-tuned LLMs can actually be better than ones obtained from dedicated summarizers and even surpass the ground-truth ones in quality, according to manual, subjective evaluation \cite{goyal2022news}. This is a still an open area of cutting-edge research, where progress in automated evaluation metrics is a crucial prerequisite for further advances (as discussed in Subsection \ref{sss::SummarizationEvaluation}). Moreover, popular LLMs do not support well very long textual inputs.
	
	\section{Sentiment Analysis}
	\label{sec::Sentiment}
	Opinion mining (OM) or sentiment analysis (SA) is the extraction of \textit{polarity sentiment} (positive/negative) from a piece of text using NLP algorithms \cite{pakistan, sergio}. It is typically approached as a particular case of text classification, although sentiment score regression is also common \cite{Karamouzas2022}. Thus, possible opinions are modelled either as a set of discrete classes (e.g., ``positive", ``neutral", ``negative"), or as a continuous range of sentiment scores (e.g., in the $[-1, 1]$ range). Additionally, emotional dimensions besides polarity are occasionally considered (e.g., classes such as ``fear", ``joy", ``sadness", etc.), potentially by relying on psychological theory (e.g., \cite{Plutchik2001}). Common use-cases mainly concern SA of short texts, such as social media posts, product reviews and online discussions, with obvious applications in marketing and brand management \cite{sa_example_1, sa_example_2, sa_example_3} or political analysis \cite{sa_example_4, sa_example_5, karamouzas2022SNAM}. For an in-depth review of SA applications, the reader is referred to \cite{sa_applications}.
	
	Long document SA is not that common; for instance, extracting the sentiment of medical or legal documents is not typically a priority, at least compared to classifying or summarizing them. However, SA can be useful for automatic analysis of long journalistic articles, to extract their political viewpoint, financial texts, to predict how markets will react to the presented news, or literary books, in order to process narrative hints. For example, it can be used to determine characters' emotional states, or for identifying key narrative points, sections and anticipated reader feelings \cite{omori}. These predictions are potentially useful in themselves, but they can also be exploited as auxiliary input for particular book classifications tasks (e.g., automatic genre recognition in library software). Yet, the challenges faced by long document NLP constrain this to be more of a theoretical future vision, rather than a currently blooming field.
	
	\subsection{Challenges}
	Traditional SA algorithms rely on ``sentiment lexicons", i.e., predefined vocabularies of words that are heavily weighed towards positive or negative emotions. For example, words such as ``good", ``bad", ``great", ``terrible", etc. are likely candidates for inclusion in such as lexicon \cite{shelly}. Algorithms relying on lexicons and/or traditional classifiers (e.g., Naive Bayes, SVMs, etc.) were common before the advent of DNNs \cite{pang2} \cite{pang}. They are, however, limited by significant barriers that limit their performance, such as the following ones:
	
	\begin{enumerate}
		\item \textit{Sarcasm}: As with document classification, sarcasm and figurative language cause important issues. Most SA methods falsely label a sentence in these cases, without understanding the non-literal usage of words in their specific context.
		
		\item \textit{Inappropriate sentiment lexicons}: Texts with complicated vocabulary often lack ``simple" key words that are included in most lexicons. In such cases, most lexicon-based algorithms struggle to classify accurately.
		
		\item \textit{Polysemy}: It is challenging to encode semantic and syntactic relations between sentences to the semantic meaning of the document. For example, the words ``contrast" and ``cause" can completely flip the class of a document (see Section \ref{ssec::ClassificationChallenges}).
	\end{enumerate}
	
	Additionally, long document SA faces challenges common to the other NLP tasks, as outlined in Sections \ref{ssec::ClassificationChallenges} \ref{ssec::SummarizationChallenges}: prohibitive computational costs, difficulty to process long input sequences, long-range dependencies that are hard to identify and sentiment variation across different sections or paragraphs.
	
	\subsection{Solutions}
	As expected, long document SA has moved decisively towards DNNs in the past few years. The main body of relevant research approaches the field as simply an instance of document classification, so the content of Section \ref{sec::Classification} will not be repeated here. An interesting avenue would be to exploit related short text semantic detectors of high specificity (e.g., sarcasm detectors) at the sentence-level, given that such tools are mature \cite{dmitry} \cite{shelly} and are indeed being used to aid short text SA \cite{Karamouzas2022}, aggregating their results in a hierarchical fashion to facilitate document-level SA. However, this research direction has not been significantly exploited. Existing deep neural methods for long text SA that offer a particular angle, compared to simple document classification, are reviewed below. The majority of these and other similar methods have not been evaluated on actually long documents, due to the lack of relevant public annotated datasets, but they can in principle support them.
	% One of the earliest studies on Sentiment Analysis \cite{pang2} uses simple machine learning algorithms like Naive Bayes, Maximum entropy and SVMs in a dataset of movie reviews in order to predict the reviewers' sentiments. Through studies like this, the aforementioned "sentiment lexicon" grew as researchers had more and more information about the frequency of certain terms used by people in a certain emotional state.  
	% The same researchers two years later \cite{pang} used the same machine learning methods to explore the question of "Is x sentence objective or subjective?", again using movie reviews as the dataset for both training, validation and testing. Using these simple models, researchers were able to achieve upwards of 87\% accuracy, which at the time, was a huge improvement.  
	% Another problem that was explored early on was sarcasm detection. One of the earliest works on sarcasm detection was \cite{stacey} in 2003, which introduced the "6-tuple representation", and it is considered to be a very important milestone in sarcasm identification in linguistics. They defined sarcasm as a sentence consisting of 6-tuples where: $S$= Speaker, $H$= Listener, $C$= Content, $u$= Utterance, $p$= Literal proposition, $pâ$= Intended proposition.
	% In their attempt to solve the problem of sarcasm detection, most researchers have started using social media sites such as Twitter to train their models \cite{dmitry}, paying special attention to the amount of hyperboles used in sarcastic sentences to aid them. This is done because of the vast amount of easily accessible data and the syntax and form of tweets (short sentences). A 2017 study on sarcasm detection \cite{shelly} using Deep Convolutional Neural Networks was able to reach upwards of 90\% accuracy.
	
	\subsubsection{Hybrid architectures} 
	Initial efforts tackled documents that were a few paragraphs long, instead of book-length. For example, a composite, hierarchical deep neural architecture is proposed in \cite{tang} for classifying long user reviews (e.g., of movies, restaurants, etc.). In order to handle polysemy, the method exploits the ``principle of compostitionality" \cite{francis}, which states that the meaning of a body of text (a sentence, a paragraph or a document) depends on the meanings of its constituents, by using an LSTM to generate sentence-level embeddings for the entire document. These are sequentially fed to a subsequent GRU that encodes their relations and semantics, outputting a document representation that is classified by a final softmax layer. The GRU was explicitly selected due to its virtual lack of an output gate, since it was deemed necessary for all available sentence-level information to be used in the construction of the document representation. The overall architecture is trained end-to-end for classification and receives dense Word2Vec embeddings at its input layer.
	
	A similar method \cite{morocco} combines a CNN and a bidirectional LSTM to extract the sentiment of newspaper articles. It uses Doc2Vec \cite{doc2vec} to pregenerate paragraph-level embeddings that form the input matrix for the CNN. An alternative hierarchical method is presented in \cite{mao2022document}, where an initial bidirectional LSTM sequentially receives word embeddings per sentence and transforms them to sentence embeddings. A second bidirectional LSTM equipped with an attention mechanism receives the latter representations to generate a document-level embedding in the form of a matrix, which is then fed to a final 2D CNN followed by the softmax classifier layer. As a general statement, it can be said that such straightforward hierarchical approaches (e.g., \cite{bhuvaneshwari2022sentiment}) have become a de facto baseline in long text SA, with the recurrence of LSTMs typically exploited to handle the variable and large number of tokens, sentences and paragraphs in long documents.
	
	\subsubsection{Sentiment Analysis with Transformers}
	State-of-the-art models make use of Transformers, with BERT and its variations being very commonly used for SA \cite{toutanova}. The employed pretrained language model is typically finetuned on the desired SA dataset. For instance, this is how RoBERTa is utilized in \cite{xinzhao} for SA of financial texts. However, the document-level sentiment classification task is implemented in an ensemble learning fashion, using weighted joint prediction from multiple independent classifiers, and then combined with a separate pipeline for \textit{key entity detection}, so that specific entities (e.g., companies, countries, etc.) are identified in the text. Each of these entities is individually fed to an instance of an appropriately finetuned RoBERTA model (different than the SA ensemble participants) along with the document, in order for the DNN to predict whether each recognized entity is critical for its meaning.
	
	\section{Public Long Document Datasets}
	\label{sec::Datasets}
	Long text NLP is not a well-explored field, particularly for document-level analysis of book-length texts (e.g., literary fiction). As a result, compared to small text analysis, the amount of relevant public, annotated datasets remains fairly limited. Yet, a few generic NLP datasets do include long documents. One of the most historically important ones is \textbf{OntoNotes} \cite{ontonotes}, which features a variety of texts and contains a number of rather long ones. OntoNotes is mainly annotated for syntax analysis tasks. Many other general-use NLP datasets include a subset of long texts, such as the \textbf{OPUS} dataset for machine translation \cite{tiedemann}.
	
	%An important one is the \textbf{Columbia Quoted Speech Attribution} corpus \cite{elson}, which contains English literature texts from the 19th and 20th centuries that have been annotated for quotation attribution. Although it targets quotation extraction and speaker identification tasks, its data can still be utilised for document-level analysis.
	
	Specifically long document datasets are rarer. An example would be the Project Dialogism Novel Corpus (PDNC dataset) \cite{vishnubhotla2022project}, which comprises 22 English novels.
	
	A modern large-scale dataset for literary fiction texts is \textbf{LitBank} \cite{bamman}, a dataset of 100 English literature works that fully includes word annotations and co-reference relations between words that belong in different groups. Unlike OntoNotes, it focuses particularly on long English fiction literature.
	
	The large-scale \textbf{CSL} dataset \cite{li}, developed in 2022, includes a plethora of Chinese scientific literature books. Modern neural language models trained on this dataset (e.g., BERT) achieved really high accuracy in tasks such as document classification for a language as complicated as Mandarin.
	
	Concerning document summarization, the traditionally used dataset is \textbf{CNN/DailyMail} \cite{nallapati2}, featuring over 330k articles and their respective highlights, as written by journalists at CNN and the Daily Mail. On average, each article has $781$ input tokens and $56$ summary (target) tokens. While the dataset is certainly large, and ships with developed and convenient APIs \cite{tf_cnn_dailymail, kaggle_cnn_dailymail, hugging_face_cnn_dailymail}, its individual articles may not be long enough for modern long document summarization. This has not prevented its continuing use as a benchmark, for measuring the efficiency of contemporary summarizers in short-to-medium length documents \cite{big_bird}.
	
	Because of this concern, two document summarization datasets have risen into prominence the last few years: \textbf{arXiv} \cite{clement} \cite{gong} and \textbf{PubMed} \cite{sumpubmed} \cite{franck}. Both are composed of scientific research articles. They feature a huge number of documents, remarkable document length and user-curated summaries in the form of article abstracts. Their success is evident from their inclusion in many popular machine learning libraries \cite{tf_datasets} \cite{kaggle_arxiv} \cite{hugging_face_arxiv} \cite{hugging_face_pubmed} and from their popularity as benchmarks in current document summarization literature.
	
	Recently, two more challenging datasets have been released: \textbf{BookSum} \cite{poland} and \textbf{SummScreen} \cite{summset}. BookSum covers fiction books from multiple domains in literature, including short stories, novels and plays. SummScreen includes pairs of movie transcripts and respective human-written summaries. They both require the summarizer to combine narrative plot events from implicit subplots, potentially progressing in parallel, forcing it to rely on extremely long context spans and dependencies.
	
	\begin{table}
		\begin{tabular}
			{ |p{2cm}|p{2cm}|p{4cm}|p{2cm}|p{2cm}|p{2cm}| }
			\hline
			\cellcolor{blue!25}\textbf{Name} & \cellcolor{blue!25}\textbf{Domain} & \cellcolor{blue!25}\textbf{Tasks} & \cellcolor{blue!25}\textbf{Documents} & \cellcolor{blue!25}\textbf{Average Document Size} & \cellcolor{blue!25}\textbf{Language} \\
			\hline
			OntoNotes & General & Named Entity Recognition, Coreference Resolution, Semantic Role Labeling & 10k-100k & - & English, Arabic, Chinese \\
			\hline
			Litbank & Literature  & Named Entity Recognition & 100 & 2000 words & English \\
			\hline
			PDNC & Literature (Old) & Quote Attribution, Speaker identification & 35,978 & 79,745 tokens & English \\
			\hline
			CSL & Academic & Text Classification, Document Summarization, Keyword Generation (KG) & 396k & - & Chinese \\
			\hline
			CNN/Daily Mail & News Articles & Document Summarization & 300K &  781 words & English \\
			\hline
			arXiv & Academic & Document Summarization, Text Retrieval & 1.7M & - & English \\
			\hline
			Pubmed & Academic & Language Modeling, Document Classification, Document Summarization & 19,717 & 10M-100M & English \\
			\hline
			BookSum & Literature & Text (Narrative) Summarization & 146.5K paragraphs (12,6K chapters, 436 books) & - & English \\
			\hline
			SummScreen & Screenplays & Document Summarization & 22,503 & - & English \\
			\hline
			LexGLUE & Legal & Multiclass Document Classification & 238K & - & English \\
			\hline
			MIMIC-III & Medical & Multiclass Document Classification & 112K & 709.3 & English \\
			\hline
			ECTHR-ARG & Legal & Multiclass Document Classification & 11K & - & English \\
			\hline
			ContractNLI & Legal & Multiclass Document Classification, Natural Language Inference & 607 & - & English \\
			\hline
			20 Newsgroup & News & Multiclass Document Classification & 20k & - & English \\
			\hline
			Hyperpartisan & News & Document Classification & 750K & - & English \\
			\hline
		\end{tabular}
		\caption{Reviewed long document datasets and their properties. The average document size has been noted wherever this information is publicly available.}
		\label{tab::Dataset}
	\end{table}
	
	Domain-specific datasets are crucial for domain-specific NLP algorithms, either for training or for domain finetuning \cite{dai}. Below is a brief list of such datasets, featuring primarily long documents for classification or summarization tasks:
	\begin{itemize}
		\item \textbf{LexGLUE} \cite{glue_gunner}, a classification dataset containing a collection of smaller datasets of legal texts.
		
		\item \textbf{MIMIC-III} \cite{mimic} (Medical Information Mart for Intensive Care), which contains Intensive Care Unit (ICU) discharge summaries. This dataset has the benefit of being annotated with labels following the ICD-9 (The International Classification of Diseases, Ninth Revision) hierarchy.
		
		\item \textbf{ECtHR-ARG} \cite{habernal}, which contains legal texts (court decisions) from the European Court of Human Rights, along with annotation for \textit{argument mining}. Howev,er this dataset only contains about 300 cases.
		
		\item \textbf{ContractNLI} \cite{koreeda}, a classification dataset for document-level Natural Language Inference (NLI). Each data point is a contract text and a set of hypotheses, with the three classes indicating whether each hypothesis agrees with, contradicts or is not mentioned by the contract.
		
		\item \textbf{20 Newsgroup} \cite{20groups}, a dataset containing approximately 20,000 Usenet newsgroup documents partitioned evenly across 20 different newsgroups. This dataset provides a great amount and variety of data, but its articles tend to be significantly shorter than dedicated long document datasets.
		
		\item \textbf{Hyperpartisan News Detection} \cite{hyperpartisan}, which contains documents for political standpoint classification. This dataset contains both long and short articles, therefore a split must be performed based on the text's length for long document analysis.
	\end{itemize}
	
	The above-mentioned datasets are summarized in Table \ref{tab::Dataset}.
	
	\section{Summary}
	\label{sec::Summary}
	This Section summarizes the issues, the existing solutions and the relevant research presented in this article. Table \ref{tab::text_class_table} concerns document classification and Table \ref{tab::text_sum_table} document summarization. It must be noted that, alternatively, recent instruction-tuned LLMs (see \ref{sssec::AbstractiveSum}) can be leveraged successfully for these tasks, but they are not explicitly designed to support long document analysis. Thus, they typically face issues with lengthy textual inputs.
	
	\begin{table}
		\scriptsize
		\begin{tabular}
			{ |p{3cm}|p{3cm}|p{9cm}|  }
			\hline
			\cellcolor{blue!25}\textbf{Method} & \cellcolor{blue!25}\textbf{Addressed Issues} & \cellcolor{blue!25}\textbf{Details}\\
			\hline \rowcolor{lightgray}
			\multicolumn{3}{|c|}{Metrics \& Preprocessing} \\
			\hline
			Weighted Multi-label Classification \cite{sicong}  & Varying Count of Labels Per Document, Class Imbalance & By using multi-label metrics and loss functions using weighting, effective multi-label multi-class classifiers can be trained and accurately evaluated.\\
			\hline
			Feature Pruning \cite{sicong, worsham} & Dataset Size \break \break Computational Cost & Ruthless pruning and careful selection / sampling can significantly cut down on the computational/memory costs of long documents. On the other hand, even words traditionally considered semantically useless can be significant for a model to understand subtle grammatical and syntactical rules.\\
			\hline
			\rowcolor{lightgray}
			\multicolumn{3}{|c|}{Sparse Attention Transformers} \\
			\hline
			Sparse Transformers \cite{child} & $O(n^2)$ Transformer Cost & By utilizing a sparse attention mechanism, and with various mathematical optimizations, a standard Transformer can become viable at parsing entire long document inputs.\\
			\hline
			Dilated Sliding Window (Longformer) \cite{longformer} & $O(n^2)$ Transformer Cost \break\break Context Fragmentation & Using local self-attention with a dilated sliding window, while also allowing a limited number of global tokens, the Longformer becomes capable of successfully parsing ever-increasing-length documents, while keeping track of local context.\\
			\hline
			Sliding Window + Random Selection + Global Attention (BigBird) \cite{big_bird} & $O(n^2)$ Transformer Cost \break\break Context Fragmentation & Combining Longformer's sliding window, block attention scheme and random token selection can encapsulate local and global context more efficiently while keeping computational costs low.\\
			\hline
			\rowcolor{lightgray}
			\multicolumn{3}{|c|}{Hierarchical Transformers} \\
			\hline
			Hierarchical Transformers combined with traditional networks \cite{ion_han, zichao, khandve, use} & Transformer Input Limit & Instead of using Transformers for the entire document, they can be used to produce segment embeddings, which are then combined by traditional networks (HAN, LSTM, CNN).\\
			\hline
			Hierarchical Transformers with BERT (Hi-Transformer) \cite{qi} &
			Global Context \break\break Context Fragmentation & BERT can be used to produce sentence embeddings, then document embeddings which can be fed to new, context-aware sentence embeddings iteratively.\\
			\hline
			Hierarchical Sparse Transformers (ERNIE-Sparse) \cite{liu} & $O(n^2)$ Transformer Cost \break\break Local/Global Context issues & By using a hierarchical architecture embedded in a Sparse Transformer, ERNIE-SPARSE imparts token representations with additional global context.\\
			\hline
			\rowcolor{lightgray}
			\multicolumn{3}{|c|}{Recurrent Transformers} \\
			\hline
			Recurrent Transformers \cite{dai} & $O(n^2)$ Transformer Cost \break\break Context Fragmentation \break\break Global Context & By keeping the context of previously inputted segments in hidden states and by utilizing relative positional embeddings, the Recurrent Transformer effectively supports much longer input sequences during inference, irrespective of training-stage sequence length.\\
			\hline
			Skimming Recurrent Transformers \cite{dai-etal-2019-transformer} & $O(n^2)$ Transformer Cost \break\break Context Fragmentation \break\break Global Context \break\break Forward context & Forward and global context of greater quality can be obtained by passing through the input text twice, once by peeking small text segments (skimming phase), and then by performing a full pass (retrospective phase), fully utilizing the recurrence mechanism.\\
			\hline
		\end{tabular}
		\caption{Issues and solutions for long document classification.}
		\label{tab::text_class_table}
	\end{table}
	
	Below, Table \ref{tab::ResultsSummarization} presents the published scores achieved by the presented state-of-the-art methods for document summarization, in the 3 most commonly employed corresponding public datasets from Section \ref{sec::Datasets}. The utilized evaluation metric is ROUGE. As it can be seen, modern Sparse Transformer variants exhibit a performance advantage in long document datasets such as arXiv and PubMed, while remaining competitive in small to medium-sized datasets such as CNN/Daily Mail. Although this also seems to hold for document classification as well, it cannot be presented in the same format due to inconsistent metrics and datasets used in the relevant papers.
	
	
	%\begin{table}[]
	%\centering
	%\begin{tabular}{ |p{5cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|  }
	%\hline
	%\cellcolor{blue!25}\textbf{Method} & \cellcolor{blue!25}\textbf{CNN/DailyMail} & %\cellcolor{blue!25}\textbf{CNN/DailyMail} & \cellcolor{blue!25}\textbf{arXiv} & %\cellcolor{blue!25}\textbf{PubMed} \\
	%\hline
	%Sparse Transformers \cite{child} & & &\\
	%\hline
	%Longformer \cite{longformer}  & & &\\
	%\hline
	%Big Bird\cite{big_bird} & & &\\
	%\hline
	%Hierarchical Transformers combined with traditional networks \cite{ion_han, zichao, khandve, use}  & & &\\
	%\hline
	%Hi-Transformer \cite{qi}  & & &\\
	%\hline
	%ERNIE-Sparse \cite{liu}  & & &\\
	%\hline
	%Recurrent Transformers \cite{dai} & & &\\
	%\hline
	%Skimming Recurrent Transformers \cite{dai-etal-2019-transformer} & & &\\
	%\hline 
	%\end{tabular}
	%\caption{Caption}
	%\label{tab:ResultsClassification}
	%\end{table}
	
	% https://paperswithcode.com
	
	% Classification (accuracy)
	% Sparse Transformer is evaluated on bits per byte
	% Longformer accuracy IMDb 52.33 (10 classes), Hyper 94.8 (2 classes), Amazon 65.35 (5 classes), MIND 81.42 (18 classes)
	% BigBird accuracy arxiv 92.31, Hyper 92.2 (2 classes), IMDb 52.87 (10 classes), Amazon 66.05 (5 classes), MIND 81.89 (18 classes)
	% Hi-Transformer IMDb 53.78 (10 classes), Amazon 67.24 (5 classes), MIND 82.51 (18 classes)
	
	% Summarization (ROUGE1)
	% Nallapati 2016: CNN 35.46 
	% SummaRuNNer (Ext): CNN 42.0, arXiv 42.81, PubMed 43.89
	% SummaRunner (Abs): CNN 40.4
	% PGN (abs): CNN 39.53, arXiv 32.06, Pubmed: 35.86
	% BigBird: CNN 43.84, arXiv 46.63, Pubmed 46.32 
	% BERTSUMABS: CNN 41.72
	% BERTSUMEXT (large): CNN 43.85
	% HiStruct (Extr): CNN 43.65, arXiv 45.22
	% HEGEL: arXiv 46.41, PubMed 47.13
	% Longformer: ???
	% LongSpan (Hybrid): arXiv: 48.79, PubMed 48.06
	% LongSpan (Abs): arXiv: 46.59, PubMed 47.47
	
	
	\begin{table}
		\scriptsize
		\begin{tabular}
			{ |p{3cm}|p{3cm}|p{9cm}|  }
			\hline
			\cellcolor{blue!25}\textbf{Method} & \cellcolor{blue!25}\textbf{Addressed Issues} & \cellcolor{blue!25}\textbf{Details}\\
			\hline
			\rowcolor{lightgray} 
			\multicolumn{3}{|c|}{Extractive Document Summarization} \\
			\hline
			Hierarchical Summarization \cite{lapata, nallapati, xiao} & DNN Use for Summarization  \break\break Document Structure Under-utilization & Traditional Encoder-Decoder DNN architectures attempt to explicitly incorporate context and hierarchical elements to the features used by CNN and RNN/LSTM extractive models.\\
			\hline
			BERTSUM-Ext \cite{nallapati2} & Poor Traditional DNN Performance\break\break Weaknesses in Sentence Embeddings & By using BERT as Encoder and attaching stacked Transformer layers, BERT's expressive power leads to accurate extractive summaries.\\  
			\hline
			HiStruct+ \cite{histruct} & Segment/Document Context Loss\break\break Under-utilization of Document Structure & By encoding the hierarchical and topical structure of a document, the Transformer can leverage more information about rigidly structured documents (e.g., research articles).\\
			\hline
			HEGEL \cite{hegel} & Under-utilization of Document Structure\break\break $O(n^2)$ Transformer Cost\break\break Long-span Dependencies & Encoding each document as a hypergraph under multiple different views leads to the model being exceptionally adept at summarizing very long and rigidly hierarchical documents.\\
			\hline
			\rowcolor{lightgray}
			\multicolumn{3}{|c|}{Abstractive Document Summarization} \\
			\hline
			Pointer-Generator Networks \cite{abigail} & Word Repetition\break\break Vocabulary Limitations\break\break  Long Document Computational Cost & By allowing the network to choose whether to copy the next summary word from the source text or predict its own, the model can benefit from a significantly expanded vocabulary, reduce computation costs by keeping a smaller vocabulary and bypass the word repetition problem found in many extractive summarization models. \\
			\hline
			Sparse Attention Transformers \cite{longformer, big_bird, Guo22LongT5} & $O(n^2)$ Transformer Cost \break\break  Vocabulary Limitations & By using a modified Sparse Transformer as the Encoder, the model can leverage the expressive power of Transformers on long documents. \\
			\hline
			Local Attention and Content Selection \cite{gales} & $O(n^2)$ Transformer Cost  \break\break Long Document Computational Cost & Pretrained models can be used for content selection in order for a Sparse Attention model to be fed only useful data, both in training and inference.\\ 
			\hline
		\end{tabular}
		\caption{Issues and solutions for long document summarization.}
		\label{tab::text_sum_table}
	\end{table}
	
	\begin{table}
		\begin{tabular}
			{ |p{5cm}|p{3cm}|p{3cm}|p{3cm}|  }
			\hline
			\cellcolor{blue!25}\textbf{Model} & \cellcolor{blue!25}\textbf{CNN / Daily Mail} & \cellcolor{blue!25}\textbf{ArXiv} &
			\cellcolor{blue!25}\textbf{PubMed}\\
			\hline
			\rowcolor{lightgray} 
			\multicolumn{4}{|c|}{Extractive Document Summarization} \\
			\hline
			SummaRuNNer \cite{nallapati} & 42.0 & 42.81 & 43.89\\
			\hline
			BERTSUM-Ext \cite{nallapati2} & 43.85 & - & - \\  
			\hline
			HiStruct+ \cite{histruct} & 43.65 & 45.22 & - \\
			\hline
			HEGEL \cite{hegel} & - & 46.61 & 43.89\\
			\hline
			\rowcolor{lightgray}
			\multicolumn{4}{|c|}{Abstractive Document Summarization} \\
			\hline
			Pointer-Generator Networks \cite{abigail} & 39.53 & 32.06 & 35.86 \\
			\hline
			BigBird \cite{big_bird} & 43.84 & 46.63 & 46.32 \\
			\hline
			Local Attention and Content Selection (Abstractive) \cite{gales} & - & 46.59 & 47.47\\ 
			\hline
		\end{tabular}
		\caption{The ROUGE-1 scores (higher is better) for state-of-the-art extractive and abstractive summarizers on the three most popular summarization datasets (where available). These results are only indicative, since the ROUGE score's correlation with summarization quality is under debate (see Section \ref{sss::SummarizationEvaluation}).}
		\label{tab::ResultsSummarization}
	\end{table}
	
	
	\section{Conclusions}
	\label{sec::Conclusions}
	Analyzing long documents currently presents a daunting challenge, even with the recent NLP breakthroughs. While traditional machine learning and statistical methods provide limited functionality on these tasks, they ultimately prove less adaptable and less accurate than modern DNNs. However, the latter ones need to face long-standing issues, such as the curse of dimensionality, limited datasets, prohibitively large inputs and difficult writing formats (e.g., in literary fiction).
	
	Most major issues are common to all long document analysis tasks and currently tend to require similar solutions, such as limiting the model's vocabulary, modifying existing Transformer architectures to handle longer input sequences, efficiently exploiting the hierarchical structure of documents and artificially augmenting the training datasets. Task-specific solutions have also been proposed, such as using content preselection and specialized GNNs to capture long-spanning context in summarization, or passing over the input twice and using document-level representations to enrich sentence-level representations in classification. Overall, further investigation of hybrid and recurrent Transformer architectures, as well as of the top-down Transformer, seem to be promising directions for near-future improvements. Incorporation of such advances into generic LLMs is also an avenue worth exploring, along with the development of novel, improved evaluation metrics for automatic text summarization.
	
	While this survey focuses only on document-level analysis for long texts, it becomes apparent how attempts at solving specific tasks can lead to solutions or insights with impact to all NLP fields. However, it is important to note that existing state-of-the-art solutions rely on useful trade-offs and, therefore, cannot be used universally. Finally, there is significant research potential potential for research into literary sentiment analysis, in collaboration with the digital/computational humanities community. The necessary theoretical background already exists, while the mentioned breakthroughs in long document NLP have not been yet applied.
	
	Automated analysis of long documents is essentially a field still in its infancy, with breakthroughs leading to large jumps in computational efficiency and accuracy still frequent. Since 2015 neural network models have evolved from being constrained to (occasionally artificially) small-sized documents, to being able to effectively parse large articles and books. It is not unreasonable to expect that continuing research will help bypass most of the current limitations within the next few years.
	
	% \section*{Acknowledgement}
	% The research leading to these results has received funding from the European Unionâs Internal Security Fund under grant agreement No 101103298 (KLEPTOTRACE). This publication reflects only the authorsâ views. The European Commission is not responsible for any use that may be made of the information it contains.
	
	\bibliographystyle{elsarticle-num-names} 
	\bibliography{refs}
\end{document}
\endinput